{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pendências\n",
    "- [x] Procurar sobre importância de atributos\n",
    "- [x] Calcular acurácia dos modelos\n",
    "- [x] Fazer previsão por liga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports e carregamentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_live_engineer_filtered.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['minute', 'homeTeam', 'awayTeam', 'shotsHome', 'shotsAway',\n",
       "       'blockedShotsHome', 'blockedShotsAway', 'league', 'corners_home',\n",
       "       'corners_away', 'shotsOffgoal_home', 'shotsOffgoal_away',\n",
       "       'shotsOngoal_home', 'shotsOngoal_away', 'yellowcards_home',\n",
       "       'yellowcards_away', 'fouls_home', 'fouls_away', 'offsides_home',\n",
       "       'offsides_away', 'tackles_home', 'tackles_away', 'result', 'match_id',\n",
       "       'possessiontime_away', 'possessiontime_home', 'shotsOnGoalEfficiency',\n",
       "       'attackPressure', 'shotAccuracy_home', 'shotAccuracy_away',\n",
       "       'possessionControl', 'passRisk', 'defensiveDiscipline',\n",
       "       'defensiveEfficacy', 'defensiveAggression'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados separados em características (X) e variável alvo (y).\n",
      "Preprocessador criado com transformações para características numéricas e categóricas.\n",
      "Preprocessador ajustado aos dados.\n",
      "Preprocessador salvo em '../models/preprocessor.pickle'.\n",
      "Dados transformados pelo preprocessador.\n",
      "Dados divididos em conjuntos de treino e teste.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import pickle\n",
    "\n",
    "# Função para separar os dados em características e variável alvo\n",
    "def prepare_data(df):\n",
    "    X = df.drop(columns=['result', 'homeTeam', 'awayTeam', 'match_id'])\n",
    "    y = df['result']\n",
    "    print(\"Dados separados em características (X) e variável alvo (y).\")\n",
    "    return X, y\n",
    "\n",
    "# Função para criar e aplicar o transformador de colunas\n",
    "def create_preprocessor(X):\n",
    "    numeric_features = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "    categorical_features = ['league']\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "            ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "        ])\n",
    "    \n",
    "    print(\"Preprocessador criado com transformações para características numéricas e categóricas.\")\n",
    "    return preprocessor.fit(X)\n",
    "\n",
    "# # Preparando os dados\n",
    "X, y = prepare_data(df)\n",
    "preprocessor = create_preprocessor(X)\n",
    "print(\"Preprocessador ajustado aos dados.\")\n",
    "\n",
    "# Salvando o preprocessador para uso futuro\n",
    "with open('../models/preprocessor.pickle', 'wb') as f:\n",
    "    pickle.dump(preprocessor, f)\n",
    "    print(\"Preprocessador salvo em '../models/preprocessor.pickle'.\")\n",
    "\n",
    "# Dividindo os dados em conjuntos de treino e teste\n",
    "X_transformed = preprocessor.transform(X)\n",
    "print(\"Dados transformados pelo preprocessador.\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.20, random_state=42, stratify=y)\n",
    "print(\"Dados divididos em conjuntos de treino e teste.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Neural no Keras: Treinamento e Avaliação "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrução do modelo, salvamento, visualização treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1203/1203 [==============================] - 9s 6ms/step - loss: 1.0462 - accuracy: 0.5056 - recall: 0.5086 - precision: 0.5056 - auc: 0.5092 - val_loss: 0.8524 - val_accuracy: 0.5277 - val_recall: 0.6117 - val_precision: 0.5237 - val_auc: 0.5352 - lr: 1.0000e-04\n",
      "Epoch 2/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.9457 - accuracy: 0.5112 - recall: 0.5154 - precision: 0.5111 - auc: 0.5161 - val_loss: 0.8457 - val_accuracy: 0.5325 - val_recall: 0.5541 - val_precision: 0.5311 - val_auc: 0.5452 - lr: 1.0000e-04\n",
      "Epoch 3/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.9051 - accuracy: 0.5112 - recall: 0.5118 - precision: 0.5112 - auc: 0.5152 - val_loss: 0.8406 - val_accuracy: 0.5412 - val_recall: 0.5682 - val_precision: 0.5390 - val_auc: 0.5588 - lr: 1.0000e-04\n",
      "Epoch 4/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.8777 - accuracy: 0.5142 - recall: 0.5169 - precision: 0.5141 - auc: 0.5199 - val_loss: 0.8376 - val_accuracy: 0.5401 - val_recall: 0.5624 - val_precision: 0.5384 - val_auc: 0.5602 - lr: 1.0000e-04\n",
      "Epoch 5/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.8596 - accuracy: 0.5159 - recall: 0.5181 - precision: 0.5159 - auc: 0.5235 - val_loss: 0.8345 - val_accuracy: 0.5474 - val_recall: 0.5136 - val_precision: 0.5509 - val_auc: 0.5617 - lr: 1.0000e-04\n",
      "Epoch 6/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.8462 - accuracy: 0.5221 - recall: 0.5172 - precision: 0.5223 - auc: 0.5305 - val_loss: 0.8296 - val_accuracy: 0.5483 - val_recall: 0.5352 - val_precision: 0.5496 - val_auc: 0.5671 - lr: 1.0000e-04\n",
      "Epoch 7/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.8366 - accuracy: 0.5234 - recall: 0.5242 - precision: 0.5234 - auc: 0.5341 - val_loss: 0.8244 - val_accuracy: 0.5525 - val_recall: 0.5367 - val_precision: 0.5543 - val_auc: 0.5668 - lr: 1.0000e-04\n",
      "Epoch 8/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.8277 - accuracy: 0.5271 - recall: 0.5213 - precision: 0.5274 - auc: 0.5382 - val_loss: 0.8179 - val_accuracy: 0.5514 - val_recall: 0.5415 - val_precision: 0.5524 - val_auc: 0.5676 - lr: 1.0000e-04\n",
      "Epoch 9/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.8183 - accuracy: 0.5323 - recall: 0.5321 - precision: 0.5323 - auc: 0.5460 - val_loss: 0.8103 - val_accuracy: 0.5507 - val_recall: 0.5224 - val_precision: 0.5538 - val_auc: 0.5687 - lr: 1.0000e-04\n",
      "Epoch 10/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.8087 - accuracy: 0.5363 - recall: 0.5360 - precision: 0.5364 - auc: 0.5515 - val_loss: 0.8016 - val_accuracy: 0.5497 - val_recall: 0.5157 - val_precision: 0.5534 - val_auc: 0.5678 - lr: 1.0000e-04\n",
      "Epoch 11/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.7990 - accuracy: 0.5401 - recall: 0.5306 - precision: 0.5408 - auc: 0.5556 - val_loss: 0.7922 - val_accuracy: 0.5532 - val_recall: 0.5253 - val_precision: 0.5564 - val_auc: 0.5702 - lr: 1.0000e-04\n",
      "Epoch 12/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.7895 - accuracy: 0.5413 - recall: 0.5341 - precision: 0.5419 - auc: 0.5578 - val_loss: 0.7827 - val_accuracy: 0.5521 - val_recall: 0.5377 - val_precision: 0.5536 - val_auc: 0.5705 - lr: 1.0000e-04\n",
      "Epoch 13/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.7796 - accuracy: 0.5440 - recall: 0.5340 - precision: 0.5449 - auc: 0.5617 - val_loss: 0.7738 - val_accuracy: 0.5545 - val_recall: 0.5819 - val_precision: 0.5517 - val_auc: 0.5717 - lr: 1.0000e-04\n",
      "Epoch 14/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.7708 - accuracy: 0.5467 - recall: 0.5372 - precision: 0.5476 - auc: 0.5637 - val_loss: 0.7650 - val_accuracy: 0.5564 - val_recall: 0.5855 - val_precision: 0.5533 - val_auc: 0.5740 - lr: 1.0000e-04\n",
      "Epoch 15/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.7625 - accuracy: 0.5466 - recall: 0.5448 - precision: 0.5468 - auc: 0.5659 - val_loss: 0.7576 - val_accuracy: 0.5558 - val_recall: 0.5426 - val_precision: 0.5573 - val_auc: 0.5761 - lr: 1.0000e-04\n",
      "Epoch 16/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.7551 - accuracy: 0.5501 - recall: 0.5339 - precision: 0.5518 - auc: 0.5690 - val_loss: 0.7508 - val_accuracy: 0.5578 - val_recall: 0.5816 - val_precision: 0.5552 - val_auc: 0.5768 - lr: 1.0000e-04\n",
      "Epoch 17/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.7489 - accuracy: 0.5513 - recall: 0.5450 - precision: 0.5519 - auc: 0.5712 - val_loss: 0.7447 - val_accuracy: 0.5585 - val_recall: 0.5376 - val_precision: 0.5610 - val_auc: 0.5800 - lr: 1.0000e-04\n",
      "Epoch 18/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.7432 - accuracy: 0.5534 - recall: 0.5427 - precision: 0.5546 - auc: 0.5737 - val_loss: 0.7403 - val_accuracy: 0.5588 - val_recall: 0.5458 - val_precision: 0.5604 - val_auc: 0.5804 - lr: 1.0000e-04\n",
      "Epoch 19/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.7385 - accuracy: 0.5574 - recall: 0.5411 - precision: 0.5593 - auc: 0.5770 - val_loss: 0.7358 - val_accuracy: 0.5639 - val_recall: 0.5504 - val_precision: 0.5656 - val_auc: 0.5820 - lr: 1.0000e-04\n",
      "Epoch 20/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.7342 - accuracy: 0.5569 - recall: 0.5442 - precision: 0.5584 - auc: 0.5790 - val_loss: 0.7315 - val_accuracy: 0.5613 - val_recall: 0.5433 - val_precision: 0.5636 - val_auc: 0.5837 - lr: 1.0000e-04\n",
      "Epoch 21/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.7303 - accuracy: 0.5576 - recall: 0.5520 - precision: 0.5583 - auc: 0.5811 - val_loss: 0.7279 - val_accuracy: 0.5605 - val_recall: 0.5247 - val_precision: 0.5652 - val_auc: 0.5864 - lr: 1.0000e-04\n",
      "Epoch 22/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.7263 - accuracy: 0.5639 - recall: 0.5519 - precision: 0.5654 - auc: 0.5863 - val_loss: 0.7249 - val_accuracy: 0.5670 - val_recall: 0.5403 - val_precision: 0.5708 - val_auc: 0.5895 - lr: 1.0000e-04\n",
      "Epoch 23/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.7237 - accuracy: 0.5646 - recall: 0.5620 - precision: 0.5650 - auc: 0.5885 - val_loss: 0.7223 - val_accuracy: 0.5651 - val_recall: 0.5340 - val_precision: 0.5694 - val_auc: 0.5894 - lr: 1.0000e-04\n",
      "Epoch 24/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.7206 - accuracy: 0.5662 - recall: 0.5570 - precision: 0.5674 - auc: 0.5915 - val_loss: 0.7204 - val_accuracy: 0.5625 - val_recall: 0.5346 - val_precision: 0.5662 - val_auc: 0.5895 - lr: 1.0000e-04\n",
      "Epoch 25/500\n",
      "1203/1203 [==============================] - 7s 6ms/step - loss: 0.7177 - accuracy: 0.5673 - recall: 0.5524 - precision: 0.5694 - auc: 0.5951 - val_loss: 0.7168 - val_accuracy: 0.5665 - val_recall: 0.5509 - val_precision: 0.5686 - val_auc: 0.5978 - lr: 1.0000e-04\n",
      "Epoch 26/500\n",
      "1203/1203 [==============================] - 7s 6ms/step - loss: 0.7167 - accuracy: 0.5700 - recall: 0.5660 - precision: 0.5706 - auc: 0.5947 - val_loss: 0.7152 - val_accuracy: 0.5712 - val_recall: 0.5689 - val_precision: 0.5715 - val_auc: 0.5979 - lr: 1.0000e-04\n",
      "Epoch 27/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.7135 - accuracy: 0.5720 - recall: 0.5664 - precision: 0.5729 - auc: 0.6004 - val_loss: 0.7132 - val_accuracy: 0.5758 - val_recall: 0.5598 - val_precision: 0.5783 - val_auc: 0.6018 - lr: 1.0000e-04\n",
      "Epoch 28/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.7115 - accuracy: 0.5744 - recall: 0.5642 - precision: 0.5759 - auc: 0.6034 - val_loss: 0.7120 - val_accuracy: 0.5729 - val_recall: 0.5753 - val_precision: 0.5725 - val_auc: 0.6013 - lr: 1.0000e-04\n",
      "Epoch 29/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.7106 - accuracy: 0.5737 - recall: 0.5621 - precision: 0.5755 - auc: 0.6035 - val_loss: 0.7093 - val_accuracy: 0.5757 - val_recall: 0.5767 - val_precision: 0.5756 - val_auc: 0.6064 - lr: 1.0000e-04\n",
      "Epoch 30/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.7080 - accuracy: 0.5781 - recall: 0.5743 - precision: 0.5787 - auc: 0.6080 - val_loss: 0.7082 - val_accuracy: 0.5776 - val_recall: 0.5551 - val_precision: 0.5812 - val_auc: 0.6093 - lr: 1.0000e-04\n",
      "Epoch 31/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.7065 - accuracy: 0.5802 - recall: 0.5751 - precision: 0.5811 - auc: 0.6114 - val_loss: 0.7058 - val_accuracy: 0.5861 - val_recall: 0.5917 - val_precision: 0.5851 - val_auc: 0.6155 - lr: 1.0000e-04\n",
      "Epoch 32/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.7040 - accuracy: 0.5848 - recall: 0.5821 - precision: 0.5852 - auc: 0.6176 - val_loss: 0.7041 - val_accuracy: 0.5841 - val_recall: 0.5856 - val_precision: 0.5838 - val_auc: 0.6162 - lr: 1.0000e-04\n",
      "Epoch 33/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.7037 - accuracy: 0.5846 - recall: 0.5818 - precision: 0.5851 - auc: 0.6166 - val_loss: 0.7035 - val_accuracy: 0.5875 - val_recall: 0.5736 - val_precision: 0.5900 - val_auc: 0.6179 - lr: 1.0000e-04\n",
      "Epoch 34/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.7018 - accuracy: 0.5888 - recall: 0.5842 - precision: 0.5896 - auc: 0.6214 - val_loss: 0.7001 - val_accuracy: 0.5894 - val_recall: 0.5908 - val_precision: 0.5891 - val_auc: 0.6256 - lr: 1.0000e-04\n",
      "Epoch 35/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.7000 - accuracy: 0.5905 - recall: 0.5874 - precision: 0.5911 - auc: 0.6246 - val_loss: 0.6989 - val_accuracy: 0.5920 - val_recall: 0.5830 - val_precision: 0.5937 - val_auc: 0.6282 - lr: 1.0000e-04\n",
      "Epoch 36/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6987 - accuracy: 0.5902 - recall: 0.5930 - precision: 0.5897 - auc: 0.6273 - val_loss: 0.6981 - val_accuracy: 0.5944 - val_recall: 0.5930 - val_precision: 0.5946 - val_auc: 0.6310 - lr: 1.0000e-04\n",
      "Epoch 37/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6961 - accuracy: 0.5975 - recall: 0.6041 - precision: 0.5963 - auc: 0.6334 - val_loss: 0.6954 - val_accuracy: 0.5983 - val_recall: 0.6253 - val_precision: 0.5933 - val_auc: 0.6354 - lr: 1.0000e-04\n",
      "Epoch 38/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6946 - accuracy: 0.5968 - recall: 0.6028 - precision: 0.5956 - auc: 0.6365 - val_loss: 0.6934 - val_accuracy: 0.5996 - val_recall: 0.6056 - val_precision: 0.5984 - val_auc: 0.6402 - lr: 1.0000e-04\n",
      "Epoch 39/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6933 - accuracy: 0.5990 - recall: 0.5975 - precision: 0.5993 - auc: 0.6394 - val_loss: 0.6920 - val_accuracy: 0.5989 - val_recall: 0.5725 - val_precision: 0.6045 - val_auc: 0.6422 - lr: 1.0000e-04\n",
      "Epoch 40/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6914 - accuracy: 0.6020 - recall: 0.6055 - precision: 0.6013 - auc: 0.6426 - val_loss: 0.6901 - val_accuracy: 0.6054 - val_recall: 0.5984 - val_precision: 0.6069 - val_auc: 0.6476 - lr: 1.0000e-04\n",
      "Epoch 41/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6909 - accuracy: 0.6032 - recall: 0.6109 - precision: 0.6016 - auc: 0.6446 - val_loss: 0.6885 - val_accuracy: 0.6073 - val_recall: 0.5965 - val_precision: 0.6097 - val_auc: 0.6504 - lr: 1.0000e-04\n",
      "Epoch 42/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6887 - accuracy: 0.6064 - recall: 0.6121 - precision: 0.6052 - auc: 0.6485 - val_loss: 0.6880 - val_accuracy: 0.6088 - val_recall: 0.6361 - val_precision: 0.6031 - val_auc: 0.6508 - lr: 1.0000e-04\n",
      "Epoch 43/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.6876 - accuracy: 0.6081 - recall: 0.6211 - precision: 0.6054 - auc: 0.6513 - val_loss: 0.6853 - val_accuracy: 0.6137 - val_recall: 0.6125 - val_precision: 0.6140 - val_auc: 0.6566 - lr: 1.0000e-04\n",
      "Epoch 44/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6852 - accuracy: 0.6102 - recall: 0.6203 - precision: 0.6081 - auc: 0.6556 - val_loss: 0.6840 - val_accuracy: 0.6140 - val_recall: 0.6116 - val_precision: 0.6145 - val_auc: 0.6592 - lr: 1.0000e-04\n",
      "Epoch 45/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6846 - accuracy: 0.6115 - recall: 0.6210 - precision: 0.6094 - auc: 0.6576 - val_loss: 0.6831 - val_accuracy: 0.6156 - val_recall: 0.6231 - val_precision: 0.6139 - val_auc: 0.6614 - lr: 1.0000e-04\n",
      "Epoch 46/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6834 - accuracy: 0.6146 - recall: 0.6229 - precision: 0.6127 - auc: 0.6601 - val_loss: 0.6811 - val_accuracy: 0.6195 - val_recall: 0.6136 - val_precision: 0.6209 - val_auc: 0.6670 - lr: 1.0000e-04\n",
      "Epoch 47/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6817 - accuracy: 0.6171 - recall: 0.6311 - precision: 0.6139 - auc: 0.6640 - val_loss: 0.6799 - val_accuracy: 0.6202 - val_recall: 0.6312 - val_precision: 0.6176 - val_auc: 0.6687 - lr: 1.0000e-04\n",
      "Epoch 48/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6789 - accuracy: 0.6208 - recall: 0.6270 - precision: 0.6193 - auc: 0.6686 - val_loss: 0.6774 - val_accuracy: 0.6227 - val_recall: 0.6369 - val_precision: 0.6194 - val_auc: 0.6715 - lr: 1.0000e-04\n",
      "Epoch 49/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6780 - accuracy: 0.6218 - recall: 0.6304 - precision: 0.6197 - auc: 0.6705 - val_loss: 0.6743 - val_accuracy: 0.6291 - val_recall: 0.6609 - val_precision: 0.6214 - val_auc: 0.6786 - lr: 1.0000e-04\n",
      "Epoch 50/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6773 - accuracy: 0.6244 - recall: 0.6340 - precision: 0.6220 - auc: 0.6722 - val_loss: 0.6741 - val_accuracy: 0.6300 - val_recall: 0.6375 - val_precision: 0.6280 - val_auc: 0.6790 - lr: 1.0000e-04\n",
      "Epoch 51/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6746 - accuracy: 0.6266 - recall: 0.6400 - precision: 0.6233 - auc: 0.6771 - val_loss: 0.6728 - val_accuracy: 0.6299 - val_recall: 0.6160 - val_precision: 0.6336 - val_auc: 0.6809 - lr: 1.0000e-04\n",
      "Epoch 52/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6742 - accuracy: 0.6260 - recall: 0.6406 - precision: 0.6224 - auc: 0.6778 - val_loss: 0.6698 - val_accuracy: 0.6342 - val_recall: 0.6382 - val_precision: 0.6332 - val_auc: 0.6865 - lr: 1.0000e-04\n",
      "Epoch 53/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6714 - accuracy: 0.6313 - recall: 0.6456 - precision: 0.6277 - auc: 0.6831 - val_loss: 0.6678 - val_accuracy: 0.6341 - val_recall: 0.6379 - val_precision: 0.6330 - val_auc: 0.6904 - lr: 1.0000e-04\n",
      "Epoch 54/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6706 - accuracy: 0.6343 - recall: 0.6498 - precision: 0.6302 - auc: 0.6856 - val_loss: 0.6656 - val_accuracy: 0.6393 - val_recall: 0.6536 - val_precision: 0.6354 - val_auc: 0.6944 - lr: 1.0000e-04\n",
      "Epoch 55/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6696 - accuracy: 0.6329 - recall: 0.6455 - precision: 0.6296 - auc: 0.6871 - val_loss: 0.6636 - val_accuracy: 0.6398 - val_recall: 0.6686 - val_precision: 0.6322 - val_auc: 0.6978 - lr: 1.0000e-04\n",
      "Epoch 56/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6670 - accuracy: 0.6366 - recall: 0.6574 - precision: 0.6311 - auc: 0.6910 - val_loss: 0.6608 - val_accuracy: 0.6464 - val_recall: 0.6738 - val_precision: 0.6388 - val_auc: 0.7024 - lr: 1.0000e-04\n",
      "Epoch 57/500\n",
      "1203/1203 [==============================] - 7s 6ms/step - loss: 0.6664 - accuracy: 0.6381 - recall: 0.6578 - precision: 0.6329 - auc: 0.6932 - val_loss: 0.6588 - val_accuracy: 0.6505 - val_recall: 0.6445 - val_precision: 0.6523 - val_auc: 0.7059 - lr: 1.0000e-04\n",
      "Epoch 58/500\n",
      "1203/1203 [==============================] - 7s 6ms/step - loss: 0.6654 - accuracy: 0.6405 - recall: 0.6560 - precision: 0.6362 - auc: 0.6950 - val_loss: 0.6589 - val_accuracy: 0.6467 - val_recall: 0.6562 - val_precision: 0.6440 - val_auc: 0.7054 - lr: 1.0000e-04\n",
      "Epoch 59/500\n",
      "1203/1203 [==============================] - 7s 5ms/step - loss: 0.6613 - accuracy: 0.6446 - recall: 0.6623 - precision: 0.6397 - auc: 0.7013 - val_loss: 0.6554 - val_accuracy: 0.6521 - val_recall: 0.6648 - val_precision: 0.6483 - val_auc: 0.7114 - lr: 1.0000e-04\n",
      "Epoch 60/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6616 - accuracy: 0.6422 - recall: 0.6610 - precision: 0.6370 - auc: 0.7006 - val_loss: 0.6543 - val_accuracy: 0.6566 - val_recall: 0.6719 - val_precision: 0.6519 - val_auc: 0.7139 - lr: 1.0000e-04\n",
      "Epoch 61/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6597 - accuracy: 0.6494 - recall: 0.6676 - precision: 0.6442 - auc: 0.7043 - val_loss: 0.6518 - val_accuracy: 0.6576 - val_recall: 0.6838 - val_precision: 0.6497 - val_auc: 0.7161 - lr: 1.0000e-04\n",
      "Epoch 62/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6590 - accuracy: 0.6468 - recall: 0.6664 - precision: 0.6413 - auc: 0.7048 - val_loss: 0.6523 - val_accuracy: 0.6559 - val_recall: 0.6795 - val_precision: 0.6489 - val_auc: 0.7165 - lr: 1.0000e-04\n",
      "Epoch 63/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6571 - accuracy: 0.6503 - recall: 0.6702 - precision: 0.6445 - auc: 0.7087 - val_loss: 0.6508 - val_accuracy: 0.6585 - val_recall: 0.6792 - val_precision: 0.6522 - val_auc: 0.7183 - lr: 1.0000e-04\n",
      "Epoch 64/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.6570 - accuracy: 0.6513 - recall: 0.6680 - precision: 0.6465 - auc: 0.7096 - val_loss: 0.6500 - val_accuracy: 0.6608 - val_recall: 0.6768 - val_precision: 0.6559 - val_auc: 0.7196 - lr: 1.0000e-04\n",
      "Epoch 65/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6571 - accuracy: 0.6495 - recall: 0.6663 - precision: 0.6447 - auc: 0.7095 - val_loss: 0.6496 - val_accuracy: 0.6616 - val_recall: 0.6660 - val_precision: 0.6602 - val_auc: 0.7218 - lr: 1.0000e-04\n",
      "Epoch 66/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.6549 - accuracy: 0.6524 - recall: 0.6677 - precision: 0.6479 - auc: 0.7126 - val_loss: 0.6461 - val_accuracy: 0.6640 - val_recall: 0.6994 - val_precision: 0.6531 - val_auc: 0.7244 - lr: 1.0000e-04\n",
      "Epoch 67/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6511 - accuracy: 0.6562 - recall: 0.6801 - precision: 0.6491 - auc: 0.7181 - val_loss: 0.6434 - val_accuracy: 0.6659 - val_recall: 0.6838 - val_precision: 0.6601 - val_auc: 0.7284 - lr: 1.0000e-04\n",
      "Epoch 68/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6522 - accuracy: 0.6557 - recall: 0.6739 - precision: 0.6503 - auc: 0.7174 - val_loss: 0.6422 - val_accuracy: 0.6676 - val_recall: 0.7039 - val_precision: 0.6563 - val_auc: 0.7311 - lr: 1.0000e-04\n",
      "Epoch 69/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6508 - accuracy: 0.6589 - recall: 0.6766 - precision: 0.6535 - auc: 0.7193 - val_loss: 0.6403 - val_accuracy: 0.6705 - val_recall: 0.7005 - val_precision: 0.6609 - val_auc: 0.7334 - lr: 1.0000e-04\n",
      "Epoch 70/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.6488 - accuracy: 0.6616 - recall: 0.6862 - precision: 0.6540 - auc: 0.7222 - val_loss: 0.6398 - val_accuracy: 0.6707 - val_recall: 0.7122 - val_precision: 0.6576 - val_auc: 0.7361 - lr: 1.0000e-04\n",
      "Epoch 71/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6460 - accuracy: 0.6611 - recall: 0.6804 - precision: 0.6551 - auc: 0.7257 - val_loss: 0.6379 - val_accuracy: 0.6686 - val_recall: 0.6960 - val_precision: 0.6598 - val_auc: 0.7362 - lr: 1.0000e-04\n",
      "Epoch 72/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6474 - accuracy: 0.6624 - recall: 0.6848 - precision: 0.6555 - auc: 0.7247 - val_loss: 0.6351 - val_accuracy: 0.6725 - val_recall: 0.6899 - val_precision: 0.6667 - val_auc: 0.7397 - lr: 1.0000e-04\n",
      "Epoch 73/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6465 - accuracy: 0.6649 - recall: 0.6831 - precision: 0.6591 - auc: 0.7270 - val_loss: 0.6348 - val_accuracy: 0.6762 - val_recall: 0.7011 - val_precision: 0.6679 - val_auc: 0.7425 - lr: 1.0000e-04\n",
      "Epoch 74/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6440 - accuracy: 0.6672 - recall: 0.6902 - precision: 0.6598 - auc: 0.7301 - val_loss: 0.6344 - val_accuracy: 0.6773 - val_recall: 0.6935 - val_precision: 0.6717 - val_auc: 0.7433 - lr: 1.0000e-04\n",
      "Epoch 75/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6442 - accuracy: 0.6656 - recall: 0.6885 - precision: 0.6583 - auc: 0.7293 - val_loss: 0.6342 - val_accuracy: 0.6773 - val_recall: 0.7061 - val_precision: 0.6676 - val_auc: 0.7439 - lr: 1.0000e-04\n",
      "Epoch 76/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.6428 - accuracy: 0.6670 - recall: 0.6837 - precision: 0.6616 - auc: 0.7318 - val_loss: 0.6291 - val_accuracy: 0.6856 - val_recall: 0.7056 - val_precision: 0.6784 - val_auc: 0.7511 - lr: 1.0000e-04\n",
      "Epoch 77/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6415 - accuracy: 0.6688 - recall: 0.6871 - precision: 0.6629 - auc: 0.7335 - val_loss: 0.6294 - val_accuracy: 0.6791 - val_recall: 0.6896 - val_precision: 0.6755 - val_auc: 0.7496 - lr: 1.0000e-04\n",
      "Epoch 78/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6416 - accuracy: 0.6701 - recall: 0.6925 - precision: 0.6628 - auc: 0.7344 - val_loss: 0.6296 - val_accuracy: 0.6829 - val_recall: 0.7021 - val_precision: 0.6761 - val_auc: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 79/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6383 - accuracy: 0.6711 - recall: 0.6897 - precision: 0.6649 - auc: 0.7378 - val_loss: 0.6284 - val_accuracy: 0.6826 - val_recall: 0.6916 - val_precision: 0.6793 - val_auc: 0.7523 - lr: 1.0000e-04\n",
      "Epoch 80/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6396 - accuracy: 0.6707 - recall: 0.6863 - precision: 0.6655 - auc: 0.7371 - val_loss: 0.6258 - val_accuracy: 0.6878 - val_recall: 0.7137 - val_precision: 0.6786 - val_auc: 0.7541 - lr: 1.0000e-04\n",
      "Epoch 81/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6402 - accuracy: 0.6699 - recall: 0.6905 - precision: 0.6631 - auc: 0.7364 - val_loss: 0.6253 - val_accuracy: 0.6887 - val_recall: 0.7195 - val_precision: 0.6777 - val_auc: 0.7562 - lr: 1.0000e-04\n",
      "Epoch 82/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.6372 - accuracy: 0.6743 - recall: 0.6958 - precision: 0.6671 - auc: 0.7407 - val_loss: 0.6230 - val_accuracy: 0.6909 - val_recall: 0.7112 - val_precision: 0.6835 - val_auc: 0.7598 - lr: 1.0000e-04\n",
      "Epoch 83/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6367 - accuracy: 0.6760 - recall: 0.6982 - precision: 0.6685 - auc: 0.7420 - val_loss: 0.6241 - val_accuracy: 0.6886 - val_recall: 0.7030 - val_precision: 0.6834 - val_auc: 0.7589 - lr: 1.0000e-04\n",
      "Epoch 84/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6348 - accuracy: 0.6757 - recall: 0.6938 - precision: 0.6696 - auc: 0.7437 - val_loss: 0.6216 - val_accuracy: 0.6891 - val_recall: 0.7187 - val_precision: 0.6786 - val_auc: 0.7602 - lr: 1.0000e-04\n",
      "Epoch 85/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6345 - accuracy: 0.6770 - recall: 0.7000 - precision: 0.6693 - auc: 0.7445 - val_loss: 0.6223 - val_accuracy: 0.6915 - val_recall: 0.7303 - val_precision: 0.6778 - val_auc: 0.7596 - lr: 1.0000e-04\n",
      "Epoch 86/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.6362 - accuracy: 0.6759 - recall: 0.6986 - precision: 0.6682 - auc: 0.7433 - val_loss: 0.6189 - val_accuracy: 0.6912 - val_recall: 0.7260 - val_precision: 0.6787 - val_auc: 0.7637 - lr: 1.0000e-04\n",
      "Epoch 87/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6326 - accuracy: 0.6798 - recall: 0.7046 - precision: 0.6713 - auc: 0.7477 - val_loss: 0.6183 - val_accuracy: 0.6923 - val_recall: 0.7264 - val_precision: 0.6801 - val_auc: 0.7652 - lr: 1.0000e-04\n",
      "Epoch 88/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6325 - accuracy: 0.6805 - recall: 0.7042 - precision: 0.6723 - auc: 0.7479 - val_loss: 0.6179 - val_accuracy: 0.6959 - val_recall: 0.7253 - val_precision: 0.6850 - val_auc: 0.7664 - lr: 1.0000e-04\n",
      "Epoch 89/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6326 - accuracy: 0.6791 - recall: 0.7025 - precision: 0.6712 - auc: 0.7477 - val_loss: 0.6167 - val_accuracy: 0.6949 - val_recall: 0.7298 - val_precision: 0.6822 - val_auc: 0.7680 - lr: 1.0000e-04\n",
      "Epoch 90/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6306 - accuracy: 0.6822 - recall: 0.7019 - precision: 0.6753 - auc: 0.7505 - val_loss: 0.6148 - val_accuracy: 0.6994 - val_recall: 0.7286 - val_precision: 0.6884 - val_auc: 0.7702 - lr: 1.0000e-04\n",
      "Epoch 91/500\n",
      "1203/1203 [==============================] - 7s 5ms/step - loss: 0.6293 - accuracy: 0.6818 - recall: 0.7027 - precision: 0.6746 - auc: 0.7516 - val_loss: 0.6153 - val_accuracy: 0.6972 - val_recall: 0.7245 - val_precision: 0.6869 - val_auc: 0.7690 - lr: 1.0000e-04\n",
      "Epoch 92/500\n",
      "1203/1203 [==============================] - 7s 5ms/step - loss: 0.6298 - accuracy: 0.6812 - recall: 0.7006 - precision: 0.6745 - auc: 0.7514 - val_loss: 0.6132 - val_accuracy: 0.7014 - val_recall: 0.7566 - val_precision: 0.6814 - val_auc: 0.7717 - lr: 1.0000e-04\n",
      "Epoch 93/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6288 - accuracy: 0.6847 - recall: 0.7124 - precision: 0.6750 - auc: 0.7535 - val_loss: 0.6137 - val_accuracy: 0.7004 - val_recall: 0.7365 - val_precision: 0.6869 - val_auc: 0.7721 - lr: 1.0000e-04\n",
      "Epoch 94/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6278 - accuracy: 0.6848 - recall: 0.7108 - precision: 0.6757 - auc: 0.7546 - val_loss: 0.6126 - val_accuracy: 0.6995 - val_recall: 0.7324 - val_precision: 0.6872 - val_auc: 0.7719 - lr: 1.0000e-04\n",
      "Epoch 95/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6274 - accuracy: 0.6839 - recall: 0.7078 - precision: 0.6756 - auc: 0.7550 - val_loss: 0.6111 - val_accuracy: 0.7032 - val_recall: 0.7419 - val_precision: 0.6886 - val_auc: 0.7748 - lr: 1.0000e-04\n",
      "Epoch 96/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6282 - accuracy: 0.6864 - recall: 0.7176 - precision: 0.6754 - auc: 0.7542 - val_loss: 0.6116 - val_accuracy: 0.7054 - val_recall: 0.7358 - val_precision: 0.6936 - val_auc: 0.7747 - lr: 1.0000e-04\n",
      "Epoch 97/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6258 - accuracy: 0.6885 - recall: 0.7087 - precision: 0.6812 - auc: 0.7580 - val_loss: 0.6113 - val_accuracy: 0.7039 - val_recall: 0.7384 - val_precision: 0.6907 - val_auc: 0.7757 - lr: 1.0000e-04\n",
      "Epoch 98/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6251 - accuracy: 0.6863 - recall: 0.7147 - precision: 0.6762 - auc: 0.7580 - val_loss: 0.6092 - val_accuracy: 0.7053 - val_recall: 0.7173 - val_precision: 0.7005 - val_auc: 0.7781 - lr: 1.0000e-04\n",
      "Epoch 99/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6272 - accuracy: 0.6874 - recall: 0.7075 - precision: 0.6802 - auc: 0.7566 - val_loss: 0.6087 - val_accuracy: 0.7061 - val_recall: 0.7306 - val_precision: 0.6965 - val_auc: 0.7787 - lr: 1.0000e-04\n",
      "Epoch 100/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6274 - accuracy: 0.6853 - recall: 0.7066 - precision: 0.6777 - auc: 0.7562 - val_loss: 0.6102 - val_accuracy: 0.7054 - val_recall: 0.7358 - val_precision: 0.6936 - val_auc: 0.7772 - lr: 1.0000e-04\n",
      "Epoch 101/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6266 - accuracy: 0.6887 - recall: 0.7149 - precision: 0.6793 - auc: 0.7578 - val_loss: 0.6086 - val_accuracy: 0.7085 - val_recall: 0.7556 - val_precision: 0.6905 - val_auc: 0.7789 - lr: 1.0000e-04\n",
      "Epoch 102/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6257 - accuracy: 0.6886 - recall: 0.7139 - precision: 0.6795 - auc: 0.7595 - val_loss: 0.6084 - val_accuracy: 0.7064 - val_recall: 0.7513 - val_precision: 0.6894 - val_auc: 0.7805 - lr: 1.0000e-04\n",
      "Epoch 103/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6240 - accuracy: 0.6886 - recall: 0.7112 - precision: 0.6805 - auc: 0.7607 - val_loss: 0.6076 - val_accuracy: 0.7072 - val_recall: 0.7276 - val_precision: 0.6991 - val_auc: 0.7814 - lr: 1.0000e-04\n",
      "Epoch 104/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.6235 - accuracy: 0.6913 - recall: 0.7154 - precision: 0.6825 - auc: 0.7618 - val_loss: 0.6102 - val_accuracy: 0.7036 - val_recall: 0.7270 - val_precision: 0.6945 - val_auc: 0.7773 - lr: 1.0000e-04\n",
      "Epoch 105/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6241 - accuracy: 0.6903 - recall: 0.7158 - precision: 0.6810 - auc: 0.7612 - val_loss: 0.6077 - val_accuracy: 0.7055 - val_recall: 0.7363 - val_precision: 0.6936 - val_auc: 0.7805 - lr: 1.0000e-04\n",
      "Epoch 106/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.6195 - accuracy: 0.6914 - recall: 0.7137 - precision: 0.6832 - auc: 0.7659 - val_loss: 0.6036 - val_accuracy: 0.7126 - val_recall: 0.7572 - val_precision: 0.6952 - val_auc: 0.7848 - lr: 1.0000e-04\n",
      "Epoch 107/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6204 - accuracy: 0.6947 - recall: 0.7172 - precision: 0.6864 - auc: 0.7657 - val_loss: 0.6033 - val_accuracy: 0.7100 - val_recall: 0.7483 - val_precision: 0.6951 - val_auc: 0.7848 - lr: 1.0000e-04\n",
      "Epoch 108/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6211 - accuracy: 0.6933 - recall: 0.7208 - precision: 0.6832 - auc: 0.7649 - val_loss: 0.6042 - val_accuracy: 0.7076 - val_recall: 0.7369 - val_precision: 0.6961 - val_auc: 0.7838 - lr: 1.0000e-04\n",
      "Epoch 109/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.6208 - accuracy: 0.6926 - recall: 0.7173 - precision: 0.6836 - auc: 0.7657 - val_loss: 0.6012 - val_accuracy: 0.7147 - val_recall: 0.7639 - val_precision: 0.6955 - val_auc: 0.7886 - lr: 1.0000e-04\n",
      "Epoch 110/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6181 - accuracy: 0.6963 - recall: 0.7194 - precision: 0.6876 - auc: 0.7686 - val_loss: 0.6031 - val_accuracy: 0.7119 - val_recall: 0.7486 - val_precision: 0.6974 - val_auc: 0.7862 - lr: 1.0000e-04\n",
      "Epoch 111/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6196 - accuracy: 0.6945 - recall: 0.7198 - precision: 0.6851 - auc: 0.7668 - val_loss: 0.6038 - val_accuracy: 0.7122 - val_recall: 0.7674 - val_precision: 0.6911 - val_auc: 0.7856 - lr: 1.0000e-04\n",
      "Epoch 112/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6197 - accuracy: 0.6959 - recall: 0.7205 - precision: 0.6867 - auc: 0.7677 - val_loss: 0.6023 - val_accuracy: 0.7142 - val_recall: 0.7503 - val_precision: 0.6998 - val_auc: 0.7873 - lr: 1.0000e-04\n",
      "Epoch 113/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6188 - accuracy: 0.6972 - recall: 0.7191 - precision: 0.6889 - auc: 0.7687 - val_loss: 0.6032 - val_accuracy: 0.7124 - val_recall: 0.7434 - val_precision: 0.7000 - val_auc: 0.7861 - lr: 1.0000e-04\n",
      "Epoch 114/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6148 - accuracy: 0.6993 - recall: 0.7238 - precision: 0.6899 - auc: 0.7729 - val_loss: 0.6005 - val_accuracy: 0.7149 - val_recall: 0.7590 - val_precision: 0.6975 - val_auc: 0.7897 - lr: 1.0000e-04\n",
      "Epoch 115/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6175 - accuracy: 0.6970 - recall: 0.7195 - precision: 0.6886 - auc: 0.7698 - val_loss: 0.6004 - val_accuracy: 0.7132 - val_recall: 0.7584 - val_precision: 0.6955 - val_auc: 0.7880 - lr: 1.0000e-04\n",
      "Epoch 116/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6171 - accuracy: 0.6973 - recall: 0.7216 - precision: 0.6882 - auc: 0.7703 - val_loss: 0.5982 - val_accuracy: 0.7190 - val_recall: 0.7730 - val_precision: 0.6976 - val_auc: 0.7922 - lr: 1.0000e-04\n",
      "Epoch 117/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6164 - accuracy: 0.6995 - recall: 0.7238 - precision: 0.6902 - auc: 0.7716 - val_loss: 0.5989 - val_accuracy: 0.7158 - val_recall: 0.7481 - val_precision: 0.7027 - val_auc: 0.7913 - lr: 1.0000e-04\n",
      "Epoch 118/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6152 - accuracy: 0.7012 - recall: 0.7231 - precision: 0.6927 - auc: 0.7732 - val_loss: 0.5997 - val_accuracy: 0.7138 - val_recall: 0.7532 - val_precision: 0.6982 - val_auc: 0.7898 - lr: 1.0000e-04\n",
      "Epoch 119/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6153 - accuracy: 0.7003 - recall: 0.7235 - precision: 0.6914 - auc: 0.7732 - val_loss: 0.5976 - val_accuracy: 0.7139 - val_recall: 0.7568 - val_precision: 0.6970 - val_auc: 0.7909 - lr: 1.0000e-04\n",
      "Epoch 120/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.6153 - accuracy: 0.7005 - recall: 0.7214 - precision: 0.6925 - auc: 0.7733 - val_loss: 0.5988 - val_accuracy: 0.7163 - val_recall: 0.7704 - val_precision: 0.6952 - val_auc: 0.7926 - lr: 1.0000e-04\n",
      "Epoch 121/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6168 - accuracy: 0.6968 - recall: 0.7206 - precision: 0.6879 - auc: 0.7715 - val_loss: 0.5977 - val_accuracy: 0.7160 - val_recall: 0.7735 - val_precision: 0.6937 - val_auc: 0.7933 - lr: 1.0000e-04\n",
      "Epoch 122/500\n",
      "1203/1203 [==============================] - 7s 6ms/step - loss: 0.6142 - accuracy: 0.7020 - recall: 0.7252 - precision: 0.6930 - auc: 0.7748 - val_loss: 0.5970 - val_accuracy: 0.7170 - val_recall: 0.7689 - val_precision: 0.6965 - val_auc: 0.7938 - lr: 1.0000e-04\n",
      "Epoch 123/500\n",
      "1203/1203 [==============================] - 7s 6ms/step - loss: 0.6126 - accuracy: 0.7016 - recall: 0.7255 - precision: 0.6924 - auc: 0.7762 - val_loss: 0.5948 - val_accuracy: 0.7229 - val_recall: 0.7626 - val_precision: 0.7065 - val_auc: 0.7971 - lr: 1.0000e-04\n",
      "Epoch 124/500\n",
      "1203/1203 [==============================] - 7s 6ms/step - loss: 0.6157 - accuracy: 0.6994 - recall: 0.7265 - precision: 0.6892 - auc: 0.7735 - val_loss: 0.5953 - val_accuracy: 0.7228 - val_recall: 0.7773 - val_precision: 0.7009 - val_auc: 0.7966 - lr: 1.0000e-04\n",
      "Epoch 125/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.6143 - accuracy: 0.7023 - recall: 0.7319 - precision: 0.6910 - auc: 0.7748 - val_loss: 0.5960 - val_accuracy: 0.7214 - val_recall: 0.7855 - val_precision: 0.6962 - val_auc: 0.7960 - lr: 1.0000e-04\n",
      "Epoch 126/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.6133 - accuracy: 0.7025 - recall: 0.7267 - precision: 0.6932 - auc: 0.7767 - val_loss: 0.5932 - val_accuracy: 0.7218 - val_recall: 0.7643 - val_precision: 0.7044 - val_auc: 0.7971 - lr: 1.0000e-04\n",
      "Epoch 127/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6114 - accuracy: 0.7037 - recall: 0.7272 - precision: 0.6946 - auc: 0.7779 - val_loss: 0.5929 - val_accuracy: 0.7258 - val_recall: 0.7701 - val_precision: 0.7075 - val_auc: 0.7983 - lr: 1.0000e-04\n",
      "Epoch 128/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6130 - accuracy: 0.7048 - recall: 0.7250 - precision: 0.6969 - auc: 0.7772 - val_loss: 0.5938 - val_accuracy: 0.7232 - val_recall: 0.7661 - val_precision: 0.7056 - val_auc: 0.7987 - lr: 1.0000e-04\n",
      "Epoch 129/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6132 - accuracy: 0.7033 - recall: 0.7223 - precision: 0.6959 - auc: 0.7770 - val_loss: 0.5932 - val_accuracy: 0.7224 - val_recall: 0.7691 - val_precision: 0.7034 - val_auc: 0.7988 - lr: 1.0000e-04\n",
      "Epoch 130/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6118 - accuracy: 0.7036 - recall: 0.7245 - precision: 0.6955 - auc: 0.7788 - val_loss: 0.5901 - val_accuracy: 0.7246 - val_recall: 0.7616 - val_precision: 0.7091 - val_auc: 0.8013 - lr: 1.0000e-04\n",
      "Epoch 131/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.6113 - accuracy: 0.7049 - recall: 0.7281 - precision: 0.6958 - auc: 0.7791 - val_loss: 0.5935 - val_accuracy: 0.7227 - val_recall: 0.7801 - val_precision: 0.6997 - val_auc: 0.7984 - lr: 1.0000e-04\n",
      "Epoch 132/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6116 - accuracy: 0.7062 - recall: 0.7293 - precision: 0.6971 - auc: 0.7789 - val_loss: 0.5898 - val_accuracy: 0.7260 - val_recall: 0.7426 - val_precision: 0.7188 - val_auc: 0.8027 - lr: 1.0000e-04\n",
      "Epoch 133/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6096 - accuracy: 0.7063 - recall: 0.7275 - precision: 0.6980 - auc: 0.7809 - val_loss: 0.5918 - val_accuracy: 0.7245 - val_recall: 0.7643 - val_precision: 0.7080 - val_auc: 0.8003 - lr: 1.0000e-04\n",
      "Epoch 134/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6119 - accuracy: 0.7040 - recall: 0.7243 - precision: 0.6961 - auc: 0.7785 - val_loss: 0.5905 - val_accuracy: 0.7265 - val_recall: 0.7724 - val_precision: 0.7075 - val_auc: 0.8024 - lr: 1.0000e-04\n",
      "Epoch 135/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6096 - accuracy: 0.7059 - recall: 0.7284 - precision: 0.6971 - auc: 0.7810 - val_loss: 0.5923 - val_accuracy: 0.7245 - val_recall: 0.7620 - val_precision: 0.7088 - val_auc: 0.8009 - lr: 1.0000e-04\n",
      "Epoch 136/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.6105 - accuracy: 0.7058 - recall: 0.7259 - precision: 0.6978 - auc: 0.7803 - val_loss: 0.5882 - val_accuracy: 0.7265 - val_recall: 0.7607 - val_precision: 0.7120 - val_auc: 0.8050 - lr: 1.0000e-04\n",
      "Epoch 137/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.6089 - accuracy: 0.7073 - recall: 0.7313 - precision: 0.6978 - auc: 0.7821 - val_loss: 0.5876 - val_accuracy: 0.7295 - val_recall: 0.7755 - val_precision: 0.7102 - val_auc: 0.8059 - lr: 1.0000e-04\n",
      "Epoch 138/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6123 - accuracy: 0.7036 - recall: 0.7258 - precision: 0.6949 - auc: 0.7788 - val_loss: 0.5892 - val_accuracy: 0.7240 - val_recall: 0.7744 - val_precision: 0.7035 - val_auc: 0.8046 - lr: 1.0000e-04\n",
      "Epoch 139/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6083 - accuracy: 0.7064 - recall: 0.7285 - precision: 0.6977 - auc: 0.7831 - val_loss: 0.5904 - val_accuracy: 0.7241 - val_recall: 0.7564 - val_precision: 0.7105 - val_auc: 0.8021 - lr: 1.0000e-04\n",
      "Epoch 140/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6078 - accuracy: 0.7077 - recall: 0.7282 - precision: 0.6995 - auc: 0.7832 - val_loss: 0.5908 - val_accuracy: 0.7267 - val_recall: 0.7652 - val_precision: 0.7105 - val_auc: 0.8027 - lr: 1.0000e-04\n",
      "Epoch 141/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6093 - accuracy: 0.7079 - recall: 0.7301 - precision: 0.6990 - auc: 0.7823 - val_loss: 0.5906 - val_accuracy: 0.7254 - val_recall: 0.7766 - val_precision: 0.7045 - val_auc: 0.8032 - lr: 1.0000e-04\n",
      "Epoch 142/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6099 - accuracy: 0.7066 - recall: 0.7280 - precision: 0.6981 - auc: 0.7815 - val_loss: 0.5896 - val_accuracy: 0.7265 - val_recall: 0.7776 - val_precision: 0.7054 - val_auc: 0.8027 - lr: 1.0000e-04\n",
      "Epoch 143/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.6008 - accuracy: 0.7117 - recall: 0.7336 - precision: 0.7028 - auc: 0.7898 - val_loss: 0.5830 - val_accuracy: 0.7318 - val_recall: 0.7857 - val_precision: 0.7093 - val_auc: 0.8093 - lr: 5.0000e-05\n",
      "Epoch 144/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5959 - accuracy: 0.7157 - recall: 0.7373 - precision: 0.7068 - auc: 0.7944 - val_loss: 0.5797 - val_accuracy: 0.7336 - val_recall: 0.7961 - val_precision: 0.7077 - val_auc: 0.8116 - lr: 5.0000e-05\n",
      "Epoch 145/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5950 - accuracy: 0.7191 - recall: 0.7447 - precision: 0.7085 - auc: 0.7958 - val_loss: 0.5776 - val_accuracy: 0.7376 - val_recall: 0.7904 - val_precision: 0.7150 - val_auc: 0.8133 - lr: 5.0000e-05\n",
      "Epoch 146/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5894 - accuracy: 0.7202 - recall: 0.7451 - precision: 0.7097 - auc: 0.8004 - val_loss: 0.5749 - val_accuracy: 0.7409 - val_recall: 0.7881 - val_precision: 0.7201 - val_auc: 0.8161 - lr: 5.0000e-05\n",
      "Epoch 147/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5930 - accuracy: 0.7201 - recall: 0.7390 - precision: 0.7121 - auc: 0.7977 - val_loss: 0.5739 - val_accuracy: 0.7410 - val_recall: 0.7828 - val_precision: 0.7224 - val_auc: 0.8174 - lr: 5.0000e-05\n",
      "Epoch 148/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5903 - accuracy: 0.7214 - recall: 0.7426 - precision: 0.7124 - auc: 0.7995 - val_loss: 0.5719 - val_accuracy: 0.7422 - val_recall: 0.7842 - val_precision: 0.7234 - val_auc: 0.8189 - lr: 5.0000e-05\n",
      "Epoch 149/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5885 - accuracy: 0.7220 - recall: 0.7433 - precision: 0.7129 - auc: 0.8012 - val_loss: 0.5728 - val_accuracy: 0.7417 - val_recall: 0.7927 - val_precision: 0.7194 - val_auc: 0.8181 - lr: 5.0000e-05\n",
      "Epoch 150/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5885 - accuracy: 0.7218 - recall: 0.7427 - precision: 0.7129 - auc: 0.8012 - val_loss: 0.5708 - val_accuracy: 0.7430 - val_recall: 0.7855 - val_precision: 0.7240 - val_auc: 0.8193 - lr: 5.0000e-05\n",
      "Epoch 151/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5857 - accuracy: 0.7262 - recall: 0.7474 - precision: 0.7170 - auc: 0.8040 - val_loss: 0.5693 - val_accuracy: 0.7437 - val_recall: 0.7965 - val_precision: 0.7204 - val_auc: 0.8202 - lr: 5.0000e-05\n",
      "Epoch 152/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5843 - accuracy: 0.7272 - recall: 0.7497 - precision: 0.7175 - auc: 0.8049 - val_loss: 0.5684 - val_accuracy: 0.7446 - val_recall: 0.7874 - val_precision: 0.7253 - val_auc: 0.8206 - lr: 5.0000e-05\n",
      "Epoch 153/500\n",
      "1203/1203 [==============================] - 7s 6ms/step - loss: 0.5857 - accuracy: 0.7258 - recall: 0.7507 - precision: 0.7151 - auc: 0.8037 - val_loss: 0.5675 - val_accuracy: 0.7440 - val_recall: 0.7804 - val_precision: 0.7275 - val_auc: 0.8209 - lr: 5.0000e-05\n",
      "Epoch 154/500\n",
      "1203/1203 [==============================] - 7s 6ms/step - loss: 0.5841 - accuracy: 0.7278 - recall: 0.7519 - precision: 0.7174 - auc: 0.8051 - val_loss: 0.5684 - val_accuracy: 0.7441 - val_recall: 0.7862 - val_precision: 0.7252 - val_auc: 0.8201 - lr: 5.0000e-05\n",
      "Epoch 155/500\n",
      "1203/1203 [==============================] - 7s 6ms/step - loss: 0.5825 - accuracy: 0.7263 - recall: 0.7453 - precision: 0.7181 - auc: 0.8062 - val_loss: 0.5666 - val_accuracy: 0.7446 - val_recall: 0.7912 - val_precision: 0.7237 - val_auc: 0.8221 - lr: 5.0000e-05\n",
      "Epoch 156/500\n",
      "1203/1203 [==============================] - 7s 6ms/step - loss: 0.5822 - accuracy: 0.7248 - recall: 0.7488 - precision: 0.7145 - auc: 0.8057 - val_loss: 0.5665 - val_accuracy: 0.7448 - val_recall: 0.7888 - val_precision: 0.7250 - val_auc: 0.8220 - lr: 5.0000e-05\n",
      "Epoch 157/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.5868 - accuracy: 0.7241 - recall: 0.7461 - precision: 0.7147 - auc: 0.8024 - val_loss: 0.5658 - val_accuracy: 0.7479 - val_recall: 0.8023 - val_precision: 0.7235 - val_auc: 0.8234 - lr: 5.0000e-05\n",
      "Epoch 158/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5821 - accuracy: 0.7262 - recall: 0.7488 - precision: 0.7164 - auc: 0.8064 - val_loss: 0.5651 - val_accuracy: 0.7465 - val_recall: 0.7945 - val_precision: 0.7249 - val_auc: 0.8236 - lr: 5.0000e-05\n",
      "Epoch 159/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5808 - accuracy: 0.7284 - recall: 0.7509 - precision: 0.7186 - auc: 0.8074 - val_loss: 0.5643 - val_accuracy: 0.7441 - val_recall: 0.7881 - val_precision: 0.7243 - val_auc: 0.8239 - lr: 5.0000e-05\n",
      "Epoch 160/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5818 - accuracy: 0.7270 - recall: 0.7520 - precision: 0.7162 - auc: 0.8065 - val_loss: 0.5643 - val_accuracy: 0.7475 - val_recall: 0.7873 - val_precision: 0.7293 - val_auc: 0.8249 - lr: 5.0000e-05\n",
      "Epoch 161/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5807 - accuracy: 0.7285 - recall: 0.7483 - precision: 0.7198 - auc: 0.8074 - val_loss: 0.5625 - val_accuracy: 0.7488 - val_recall: 0.8024 - val_precision: 0.7247 - val_auc: 0.8262 - lr: 5.0000e-05\n",
      "Epoch 162/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5789 - accuracy: 0.7299 - recall: 0.7499 - precision: 0.7210 - auc: 0.8088 - val_loss: 0.5615 - val_accuracy: 0.7499 - val_recall: 0.7990 - val_precision: 0.7276 - val_auc: 0.8264 - lr: 5.0000e-05\n",
      "Epoch 163/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.5795 - accuracy: 0.7286 - recall: 0.7535 - precision: 0.7178 - auc: 0.8081 - val_loss: 0.5624 - val_accuracy: 0.7491 - val_recall: 0.7957 - val_precision: 0.7279 - val_auc: 0.8256 - lr: 5.0000e-05\n",
      "Epoch 164/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.5786 - accuracy: 0.7293 - recall: 0.7519 - precision: 0.7193 - auc: 0.8087 - val_loss: 0.5610 - val_accuracy: 0.7532 - val_recall: 0.8068 - val_precision: 0.7287 - val_auc: 0.8278 - lr: 5.0000e-05\n",
      "Epoch 165/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5770 - accuracy: 0.7308 - recall: 0.7508 - precision: 0.7219 - auc: 0.8107 - val_loss: 0.5600 - val_accuracy: 0.7509 - val_recall: 0.7904 - val_precision: 0.7325 - val_auc: 0.8284 - lr: 5.0000e-05\n",
      "Epoch 166/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.5766 - accuracy: 0.7292 - recall: 0.7497 - precision: 0.7202 - auc: 0.8103 - val_loss: 0.5596 - val_accuracy: 0.7499 - val_recall: 0.7874 - val_precision: 0.7325 - val_auc: 0.8281 - lr: 5.0000e-05\n",
      "Epoch 167/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5760 - accuracy: 0.7312 - recall: 0.7523 - precision: 0.7218 - auc: 0.8114 - val_loss: 0.5592 - val_accuracy: 0.7503 - val_recall: 0.7874 - val_precision: 0.7330 - val_auc: 0.8273 - lr: 5.0000e-05\n",
      "Epoch 168/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.5768 - accuracy: 0.7309 - recall: 0.7509 - precision: 0.7220 - auc: 0.8109 - val_loss: 0.5591 - val_accuracy: 0.7491 - val_recall: 0.7934 - val_precision: 0.7289 - val_auc: 0.8278 - lr: 5.0000e-05\n",
      "Epoch 169/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5770 - accuracy: 0.7309 - recall: 0.7504 - precision: 0.7222 - auc: 0.8106 - val_loss: 0.5600 - val_accuracy: 0.7513 - val_recall: 0.8101 - val_precision: 0.7248 - val_auc: 0.8286 - lr: 5.0000e-05\n",
      "Epoch 170/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5762 - accuracy: 0.7307 - recall: 0.7529 - precision: 0.7208 - auc: 0.8110 - val_loss: 0.5588 - val_accuracy: 0.7512 - val_recall: 0.8030 - val_precision: 0.7277 - val_auc: 0.8281 - lr: 5.0000e-05\n",
      "Epoch 171/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5724 - accuracy: 0.7334 - recall: 0.7588 - precision: 0.7222 - auc: 0.8139 - val_loss: 0.5567 - val_accuracy: 0.7534 - val_recall: 0.8014 - val_precision: 0.7312 - val_auc: 0.8296 - lr: 5.0000e-05\n",
      "Epoch 172/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5740 - accuracy: 0.7337 - recall: 0.7582 - precision: 0.7228 - auc: 0.8129 - val_loss: 0.5570 - val_accuracy: 0.7526 - val_recall: 0.7994 - val_precision: 0.7310 - val_auc: 0.8300 - lr: 5.0000e-05\n",
      "Epoch 173/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5764 - accuracy: 0.7311 - recall: 0.7525 - precision: 0.7217 - auc: 0.8105 - val_loss: 0.5562 - val_accuracy: 0.7513 - val_recall: 0.7992 - val_precision: 0.7293 - val_auc: 0.8300 - lr: 5.0000e-05\n",
      "Epoch 174/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5737 - accuracy: 0.7327 - recall: 0.7523 - precision: 0.7239 - auc: 0.8130 - val_loss: 0.5566 - val_accuracy: 0.7520 - val_recall: 0.8051 - val_precision: 0.7278 - val_auc: 0.8302 - lr: 5.0000e-05\n",
      "Epoch 175/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5754 - accuracy: 0.7330 - recall: 0.7527 - precision: 0.7241 - auc: 0.8121 - val_loss: 0.5557 - val_accuracy: 0.7521 - val_recall: 0.8047 - val_precision: 0.7281 - val_auc: 0.8311 - lr: 5.0000e-05\n",
      "Epoch 176/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5734 - accuracy: 0.7338 - recall: 0.7543 - precision: 0.7246 - auc: 0.8137 - val_loss: 0.5547 - val_accuracy: 0.7530 - val_recall: 0.8073 - val_precision: 0.7282 - val_auc: 0.8311 - lr: 5.0000e-05\n",
      "Epoch 177/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5729 - accuracy: 0.7357 - recall: 0.7590 - precision: 0.7253 - auc: 0.8139 - val_loss: 0.5544 - val_accuracy: 0.7539 - val_recall: 0.8015 - val_precision: 0.7318 - val_auc: 0.8313 - lr: 5.0000e-05\n",
      "Epoch 178/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5762 - accuracy: 0.7327 - recall: 0.7526 - precision: 0.7238 - auc: 0.8109 - val_loss: 0.5540 - val_accuracy: 0.7540 - val_recall: 0.8070 - val_precision: 0.7297 - val_auc: 0.8319 - lr: 5.0000e-05\n",
      "Epoch 179/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.5737 - accuracy: 0.7324 - recall: 0.7530 - precision: 0.7233 - auc: 0.8126 - val_loss: 0.5531 - val_accuracy: 0.7567 - val_recall: 0.8045 - val_precision: 0.7342 - val_auc: 0.8328 - lr: 5.0000e-05\n",
      "Epoch 180/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5712 - accuracy: 0.7361 - recall: 0.7578 - precision: 0.7262 - auc: 0.8150 - val_loss: 0.5535 - val_accuracy: 0.7562 - val_recall: 0.8119 - val_precision: 0.7305 - val_auc: 0.8319 - lr: 5.0000e-05\n",
      "Epoch 181/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5727 - accuracy: 0.7328 - recall: 0.7541 - precision: 0.7233 - auc: 0.8136 - val_loss: 0.5530 - val_accuracy: 0.7581 - val_recall: 0.8097 - val_precision: 0.7339 - val_auc: 0.8336 - lr: 5.0000e-05\n",
      "Epoch 182/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5747 - accuracy: 0.7318 - recall: 0.7527 - precision: 0.7225 - auc: 0.8117 - val_loss: 0.5517 - val_accuracy: 0.7585 - val_recall: 0.8024 - val_precision: 0.7377 - val_auc: 0.8339 - lr: 5.0000e-05\n",
      "Epoch 183/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5717 - accuracy: 0.7348 - recall: 0.7556 - precision: 0.7254 - auc: 0.8146 - val_loss: 0.5524 - val_accuracy: 0.7571 - val_recall: 0.8064 - val_precision: 0.7340 - val_auc: 0.8340 - lr: 5.0000e-05\n",
      "Epoch 184/500\n",
      "1203/1203 [==============================] - 7s 6ms/step - loss: 0.5696 - accuracy: 0.7360 - recall: 0.7559 - precision: 0.7270 - auc: 0.8160 - val_loss: 0.5524 - val_accuracy: 0.7562 - val_recall: 0.8069 - val_precision: 0.7327 - val_auc: 0.8337 - lr: 5.0000e-05\n",
      "Epoch 185/500\n",
      "1203/1203 [==============================] - 7s 6ms/step - loss: 0.5711 - accuracy: 0.7344 - recall: 0.7548 - precision: 0.7252 - auc: 0.8149 - val_loss: 0.5505 - val_accuracy: 0.7575 - val_recall: 0.8063 - val_precision: 0.7346 - val_auc: 0.8357 - lr: 5.0000e-05\n",
      "Epoch 186/500\n",
      "1203/1203 [==============================] - 7s 6ms/step - loss: 0.5700 - accuracy: 0.7356 - recall: 0.7544 - precision: 0.7270 - auc: 0.8159 - val_loss: 0.5508 - val_accuracy: 0.7578 - val_recall: 0.8117 - val_precision: 0.7328 - val_auc: 0.8350 - lr: 5.0000e-05\n",
      "Epoch 187/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5706 - accuracy: 0.7338 - recall: 0.7548 - precision: 0.7244 - auc: 0.8154 - val_loss: 0.5508 - val_accuracy: 0.7577 - val_recall: 0.8058 - val_precision: 0.7352 - val_auc: 0.8351 - lr: 5.0000e-05\n",
      "Epoch 188/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5724 - accuracy: 0.7344 - recall: 0.7575 - precision: 0.7241 - auc: 0.8135 - val_loss: 0.5511 - val_accuracy: 0.7585 - val_recall: 0.8060 - val_precision: 0.7361 - val_auc: 0.8346 - lr: 5.0000e-05\n",
      "Epoch 189/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5710 - accuracy: 0.7337 - recall: 0.7543 - precision: 0.7244 - auc: 0.8150 - val_loss: 0.5516 - val_accuracy: 0.7559 - val_recall: 0.8062 - val_precision: 0.7325 - val_auc: 0.8346 - lr: 5.0000e-05\n",
      "Epoch 190/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5696 - accuracy: 0.7356 - recall: 0.7543 - precision: 0.7271 - auc: 0.8161 - val_loss: 0.5499 - val_accuracy: 0.7578 - val_recall: 0.8038 - val_precision: 0.7362 - val_auc: 0.8352 - lr: 5.0000e-05\n",
      "Epoch 191/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5689 - accuracy: 0.7351 - recall: 0.7528 - precision: 0.7270 - auc: 0.8164 - val_loss: 0.5505 - val_accuracy: 0.7573 - val_recall: 0.8068 - val_precision: 0.7341 - val_auc: 0.8346 - lr: 5.0000e-05\n",
      "Epoch 192/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5677 - accuracy: 0.7367 - recall: 0.7601 - precision: 0.7261 - auc: 0.8174 - val_loss: 0.5502 - val_accuracy: 0.7569 - val_recall: 0.8131 - val_precision: 0.7309 - val_auc: 0.8352 - lr: 5.0000e-05\n",
      "Epoch 193/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.5709 - accuracy: 0.7362 - recall: 0.7550 - precision: 0.7276 - auc: 0.8152 - val_loss: 0.5505 - val_accuracy: 0.7601 - val_recall: 0.8179 - val_precision: 0.7332 - val_auc: 0.8356 - lr: 5.0000e-05\n",
      "Epoch 194/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5679 - accuracy: 0.7368 - recall: 0.7584 - precision: 0.7270 - auc: 0.8176 - val_loss: 0.5499 - val_accuracy: 0.7610 - val_recall: 0.8155 - val_precision: 0.7353 - val_auc: 0.8351 - lr: 5.0000e-05\n",
      "Epoch 195/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5681 - accuracy: 0.7362 - recall: 0.7550 - precision: 0.7277 - auc: 0.8168 - val_loss: 0.5501 - val_accuracy: 0.7566 - val_recall: 0.8125 - val_precision: 0.7308 - val_auc: 0.8347 - lr: 5.0000e-05\n",
      "Epoch 196/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5647 - accuracy: 0.7371 - recall: 0.7571 - precision: 0.7280 - auc: 0.8199 - val_loss: 0.5469 - val_accuracy: 0.7586 - val_recall: 0.8078 - val_precision: 0.7355 - val_auc: 0.8370 - lr: 2.5000e-05\n",
      "Epoch 197/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5630 - accuracy: 0.7402 - recall: 0.7598 - precision: 0.7311 - auc: 0.8216 - val_loss: 0.5450 - val_accuracy: 0.7585 - val_recall: 0.7998 - val_precision: 0.7387 - val_auc: 0.8378 - lr: 2.5000e-05\n",
      "Epoch 198/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5607 - accuracy: 0.7426 - recall: 0.7618 - precision: 0.7336 - auc: 0.8232 - val_loss: 0.5433 - val_accuracy: 0.7614 - val_recall: 0.8124 - val_precision: 0.7372 - val_auc: 0.8391 - lr: 2.5000e-05\n",
      "Epoch 199/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5596 - accuracy: 0.7429 - recall: 0.7645 - precision: 0.7328 - auc: 0.8240 - val_loss: 0.5424 - val_accuracy: 0.7622 - val_recall: 0.8071 - val_precision: 0.7406 - val_auc: 0.8395 - lr: 2.5000e-05\n",
      "Epoch 200/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5566 - accuracy: 0.7440 - recall: 0.7628 - precision: 0.7351 - auc: 0.8265 - val_loss: 0.5420 - val_accuracy: 0.7634 - val_recall: 0.8126 - val_precision: 0.7397 - val_auc: 0.8402 - lr: 2.5000e-05\n",
      "Epoch 201/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5597 - accuracy: 0.7429 - recall: 0.7624 - precision: 0.7338 - auc: 0.8236 - val_loss: 0.5420 - val_accuracy: 0.7639 - val_recall: 0.8119 - val_precision: 0.7408 - val_auc: 0.8404 - lr: 2.5000e-05\n",
      "Epoch 202/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5600 - accuracy: 0.7406 - recall: 0.7626 - precision: 0.7304 - auc: 0.8229 - val_loss: 0.5418 - val_accuracy: 0.7620 - val_recall: 0.8185 - val_precision: 0.7354 - val_auc: 0.8412 - lr: 2.5000e-05\n",
      "Epoch 203/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5563 - accuracy: 0.7448 - recall: 0.7637 - precision: 0.7358 - auc: 0.8266 - val_loss: 0.5395 - val_accuracy: 0.7648 - val_recall: 0.8138 - val_precision: 0.7412 - val_auc: 0.8424 - lr: 2.5000e-05\n",
      "Epoch 204/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5557 - accuracy: 0.7468 - recall: 0.7662 - precision: 0.7375 - auc: 0.8270 - val_loss: 0.5383 - val_accuracy: 0.7667 - val_recall: 0.8181 - val_precision: 0.7419 - val_auc: 0.8433 - lr: 2.5000e-05\n",
      "Epoch 205/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5569 - accuracy: 0.7438 - recall: 0.7622 - precision: 0.7352 - auc: 0.8255 - val_loss: 0.5386 - val_accuracy: 0.7644 - val_recall: 0.8179 - val_precision: 0.7388 - val_auc: 0.8433 - lr: 2.5000e-05\n",
      "Epoch 206/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5544 - accuracy: 0.7473 - recall: 0.7670 - precision: 0.7379 - auc: 0.8279 - val_loss: 0.5384 - val_accuracy: 0.7652 - val_recall: 0.8165 - val_precision: 0.7405 - val_auc: 0.8428 - lr: 2.5000e-05\n",
      "Epoch 207/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5574 - accuracy: 0.7432 - recall: 0.7617 - precision: 0.7346 - auc: 0.8252 - val_loss: 0.5373 - val_accuracy: 0.7665 - val_recall: 0.8134 - val_precision: 0.7437 - val_auc: 0.8433 - lr: 2.5000e-05\n",
      "Epoch 208/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5514 - accuracy: 0.7480 - recall: 0.7689 - precision: 0.7380 - auc: 0.8300 - val_loss: 0.5364 - val_accuracy: 0.7661 - val_recall: 0.8164 - val_precision: 0.7418 - val_auc: 0.8441 - lr: 2.5000e-05\n",
      "Epoch 209/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5571 - accuracy: 0.7446 - recall: 0.7658 - precision: 0.7347 - auc: 0.8258 - val_loss: 0.5372 - val_accuracy: 0.7664 - val_recall: 0.8198 - val_precision: 0.7407 - val_auc: 0.8446 - lr: 2.5000e-05\n",
      "Epoch 210/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5538 - accuracy: 0.7468 - recall: 0.7674 - precision: 0.7370 - auc: 0.8281 - val_loss: 0.5376 - val_accuracy: 0.7666 - val_recall: 0.8252 - val_precision: 0.7386 - val_auc: 0.8441 - lr: 2.5000e-05\n",
      "Epoch 211/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5528 - accuracy: 0.7481 - recall: 0.7662 - precision: 0.7394 - auc: 0.8290 - val_loss: 0.5361 - val_accuracy: 0.7668 - val_recall: 0.8270 - val_precision: 0.7382 - val_auc: 0.8451 - lr: 2.5000e-05\n",
      "Epoch 212/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5534 - accuracy: 0.7467 - recall: 0.7682 - precision: 0.7366 - auc: 0.8281 - val_loss: 0.5367 - val_accuracy: 0.7686 - val_recall: 0.8240 - val_precision: 0.7417 - val_auc: 0.8448 - lr: 2.5000e-05\n",
      "Epoch 213/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5512 - accuracy: 0.7484 - recall: 0.7676 - precision: 0.7392 - auc: 0.8301 - val_loss: 0.5358 - val_accuracy: 0.7668 - val_recall: 0.8302 - val_precision: 0.7368 - val_auc: 0.8454 - lr: 2.5000e-05\n",
      "Epoch 214/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5527 - accuracy: 0.7456 - recall: 0.7682 - precision: 0.7350 - auc: 0.8288 - val_loss: 0.5337 - val_accuracy: 0.7672 - val_recall: 0.8207 - val_precision: 0.7414 - val_auc: 0.8459 - lr: 2.5000e-05\n",
      "Epoch 215/500\n",
      "1203/1203 [==============================] - 7s 5ms/step - loss: 0.5528 - accuracy: 0.7466 - recall: 0.7689 - precision: 0.7361 - auc: 0.8288 - val_loss: 0.5354 - val_accuracy: 0.7675 - val_recall: 0.8177 - val_precision: 0.7431 - val_auc: 0.8451 - lr: 2.5000e-05\n",
      "Epoch 216/500\n",
      "1203/1203 [==============================] - 7s 6ms/step - loss: 0.5524 - accuracy: 0.7477 - recall: 0.7679 - precision: 0.7381 - auc: 0.8292 - val_loss: 0.5346 - val_accuracy: 0.7665 - val_recall: 0.8250 - val_precision: 0.7386 - val_auc: 0.8455 - lr: 2.5000e-05\n",
      "Epoch 217/500\n",
      "1203/1203 [==============================] - 7s 6ms/step - loss: 0.5497 - accuracy: 0.7488 - recall: 0.7680 - precision: 0.7396 - auc: 0.8311 - val_loss: 0.5335 - val_accuracy: 0.7677 - val_recall: 0.8280 - val_precision: 0.7389 - val_auc: 0.8464 - lr: 2.5000e-05\n",
      "Epoch 218/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5508 - accuracy: 0.7487 - recall: 0.7696 - precision: 0.7387 - auc: 0.8303 - val_loss: 0.5329 - val_accuracy: 0.7675 - val_recall: 0.8149 - val_precision: 0.7444 - val_auc: 0.8462 - lr: 2.5000e-05\n",
      "Epoch 219/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5496 - accuracy: 0.7481 - recall: 0.7663 - precision: 0.7394 - auc: 0.8313 - val_loss: 0.5331 - val_accuracy: 0.7675 - val_recall: 0.8201 - val_precision: 0.7420 - val_auc: 0.8465 - lr: 2.5000e-05\n",
      "Epoch 220/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5528 - accuracy: 0.7464 - recall: 0.7643 - precision: 0.7379 - auc: 0.8287 - val_loss: 0.5331 - val_accuracy: 0.7667 - val_recall: 0.8227 - val_precision: 0.7398 - val_auc: 0.8466 - lr: 2.5000e-05\n",
      "Epoch 221/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5515 - accuracy: 0.7488 - recall: 0.7682 - precision: 0.7395 - auc: 0.8299 - val_loss: 0.5338 - val_accuracy: 0.7675 - val_recall: 0.8242 - val_precision: 0.7403 - val_auc: 0.8462 - lr: 2.5000e-05\n",
      "Epoch 222/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5486 - accuracy: 0.7505 - recall: 0.7705 - precision: 0.7409 - auc: 0.8318 - val_loss: 0.5321 - val_accuracy: 0.7692 - val_recall: 0.8261 - val_precision: 0.7416 - val_auc: 0.8472 - lr: 2.5000e-05\n",
      "Epoch 223/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5492 - accuracy: 0.7484 - recall: 0.7673 - precision: 0.7394 - auc: 0.8313 - val_loss: 0.5328 - val_accuracy: 0.7678 - val_recall: 0.8322 - val_precision: 0.7372 - val_auc: 0.8468 - lr: 2.5000e-05\n",
      "Epoch 224/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5519 - accuracy: 0.7477 - recall: 0.7666 - precision: 0.7386 - auc: 0.8290 - val_loss: 0.5324 - val_accuracy: 0.7693 - val_recall: 0.8252 - val_precision: 0.7423 - val_auc: 0.8473 - lr: 2.5000e-05\n",
      "Epoch 225/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5487 - accuracy: 0.7485 - recall: 0.7683 - precision: 0.7391 - auc: 0.8313 - val_loss: 0.5309 - val_accuracy: 0.7686 - val_recall: 0.8178 - val_precision: 0.7445 - val_auc: 0.8478 - lr: 2.5000e-05\n",
      "Epoch 226/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5500 - accuracy: 0.7482 - recall: 0.7677 - precision: 0.7388 - auc: 0.8306 - val_loss: 0.5327 - val_accuracy: 0.7670 - val_recall: 0.8272 - val_precision: 0.7384 - val_auc: 0.8466 - lr: 2.5000e-05\n",
      "Epoch 227/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5508 - accuracy: 0.7485 - recall: 0.7663 - precision: 0.7400 - auc: 0.8301 - val_loss: 0.5313 - val_accuracy: 0.7686 - val_recall: 0.8234 - val_precision: 0.7420 - val_auc: 0.8475 - lr: 2.5000e-05\n",
      "Epoch 228/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5485 - accuracy: 0.7501 - recall: 0.7679 - precision: 0.7414 - auc: 0.8318 - val_loss: 0.5309 - val_accuracy: 0.7684 - val_recall: 0.8273 - val_precision: 0.7401 - val_auc: 0.8475 - lr: 2.5000e-05\n",
      "Epoch 229/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5481 - accuracy: 0.7504 - recall: 0.7725 - precision: 0.7397 - auc: 0.8322 - val_loss: 0.5307 - val_accuracy: 0.7677 - val_recall: 0.8318 - val_precision: 0.7372 - val_auc: 0.8475 - lr: 2.5000e-05\n",
      "Epoch 230/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5473 - accuracy: 0.7497 - recall: 0.7706 - precision: 0.7397 - auc: 0.8326 - val_loss: 0.5304 - val_accuracy: 0.7684 - val_recall: 0.8291 - val_precision: 0.7394 - val_auc: 0.8478 - lr: 2.5000e-05\n",
      "Epoch 231/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5493 - accuracy: 0.7492 - recall: 0.7700 - precision: 0.7392 - auc: 0.8311 - val_loss: 0.5310 - val_accuracy: 0.7676 - val_recall: 0.8230 - val_precision: 0.7409 - val_auc: 0.8478 - lr: 2.5000e-05\n",
      "Epoch 232/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5467 - accuracy: 0.7498 - recall: 0.7704 - precision: 0.7399 - auc: 0.8330 - val_loss: 0.5305 - val_accuracy: 0.7693 - val_recall: 0.8210 - val_precision: 0.7440 - val_auc: 0.8477 - lr: 2.5000e-05\n",
      "Epoch 233/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5477 - accuracy: 0.7498 - recall: 0.7689 - precision: 0.7405 - auc: 0.8323 - val_loss: 0.5298 - val_accuracy: 0.7694 - val_recall: 0.8197 - val_precision: 0.7448 - val_auc: 0.8486 - lr: 2.5000e-05\n",
      "Epoch 234/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5465 - accuracy: 0.7515 - recall: 0.7691 - precision: 0.7430 - auc: 0.8331 - val_loss: 0.5303 - val_accuracy: 0.7691 - val_recall: 0.8223 - val_precision: 0.7432 - val_auc: 0.8475 - lr: 2.5000e-05\n",
      "Epoch 235/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5462 - accuracy: 0.7498 - recall: 0.7688 - precision: 0.7406 - auc: 0.8331 - val_loss: 0.5303 - val_accuracy: 0.7677 - val_recall: 0.8205 - val_precision: 0.7422 - val_auc: 0.8479 - lr: 2.5000e-05\n",
      "Epoch 236/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5448 - accuracy: 0.7523 - recall: 0.7695 - precision: 0.7439 - auc: 0.8344 - val_loss: 0.5293 - val_accuracy: 0.7693 - val_recall: 0.8299 - val_precision: 0.7402 - val_auc: 0.8492 - lr: 2.5000e-05\n",
      "Epoch 237/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5448 - accuracy: 0.7516 - recall: 0.7707 - precision: 0.7423 - auc: 0.8342 - val_loss: 0.5290 - val_accuracy: 0.7710 - val_recall: 0.8227 - val_precision: 0.7457 - val_auc: 0.8489 - lr: 2.5000e-05\n",
      "Epoch 238/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5459 - accuracy: 0.7513 - recall: 0.7697 - precision: 0.7424 - auc: 0.8334 - val_loss: 0.5307 - val_accuracy: 0.7680 - val_recall: 0.8222 - val_precision: 0.7418 - val_auc: 0.8481 - lr: 2.5000e-05\n",
      "Epoch 239/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5482 - accuracy: 0.7487 - recall: 0.7673 - precision: 0.7398 - auc: 0.8313 - val_loss: 0.5302 - val_accuracy: 0.7699 - val_recall: 0.8315 - val_precision: 0.7402 - val_auc: 0.8484 - lr: 2.5000e-05\n",
      "Epoch 240/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5455 - accuracy: 0.7517 - recall: 0.7723 - precision: 0.7417 - auc: 0.8336 - val_loss: 0.5295 - val_accuracy: 0.7710 - val_recall: 0.8208 - val_precision: 0.7464 - val_auc: 0.8483 - lr: 2.5000e-05\n",
      "Epoch 241/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5452 - accuracy: 0.7517 - recall: 0.7701 - precision: 0.7428 - auc: 0.8340 - val_loss: 0.5288 - val_accuracy: 0.7700 - val_recall: 0.8128 - val_precision: 0.7486 - val_auc: 0.8487 - lr: 2.5000e-05\n",
      "Epoch 242/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5454 - accuracy: 0.7522 - recall: 0.7683 - precision: 0.7444 - auc: 0.8339 - val_loss: 0.5286 - val_accuracy: 0.7708 - val_recall: 0.8301 - val_precision: 0.7421 - val_auc: 0.8491 - lr: 2.5000e-05\n",
      "Epoch 243/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5474 - accuracy: 0.7504 - recall: 0.7689 - precision: 0.7415 - auc: 0.8323 - val_loss: 0.5298 - val_accuracy: 0.7689 - val_recall: 0.8289 - val_precision: 0.7400 - val_auc: 0.8487 - lr: 2.5000e-05\n",
      "Epoch 244/500\n",
      "1203/1203 [==============================] - 7s 5ms/step - loss: 0.5438 - accuracy: 0.7535 - recall: 0.7702 - precision: 0.7453 - auc: 0.8350 - val_loss: 0.5274 - val_accuracy: 0.7702 - val_recall: 0.8220 - val_precision: 0.7448 - val_auc: 0.8496 - lr: 2.5000e-05\n",
      "Epoch 245/500\n",
      "1203/1203 [==============================] - 7s 6ms/step - loss: 0.5472 - accuracy: 0.7506 - recall: 0.7676 - precision: 0.7424 - auc: 0.8322 - val_loss: 0.5288 - val_accuracy: 0.7709 - val_recall: 0.8244 - val_precision: 0.7448 - val_auc: 0.8488 - lr: 2.5000e-05\n",
      "Epoch 246/500\n",
      "1203/1203 [==============================] - 8s 6ms/step - loss: 0.5454 - accuracy: 0.7526 - recall: 0.7733 - precision: 0.7426 - auc: 0.8337 - val_loss: 0.5288 - val_accuracy: 0.7703 - val_recall: 0.8272 - val_precision: 0.7427 - val_auc: 0.8488 - lr: 2.5000e-05\n",
      "Epoch 247/500\n",
      "1203/1203 [==============================] - 7s 6ms/step - loss: 0.5450 - accuracy: 0.7513 - recall: 0.7749 - precision: 0.7400 - auc: 0.8336 - val_loss: 0.5277 - val_accuracy: 0.7716 - val_recall: 0.8255 - val_precision: 0.7452 - val_auc: 0.8499 - lr: 2.5000e-05\n",
      "Epoch 248/500\n",
      "1203/1203 [==============================] - 7s 6ms/step - loss: 0.5460 - accuracy: 0.7508 - recall: 0.7702 - precision: 0.7414 - auc: 0.8331 - val_loss: 0.5278 - val_accuracy: 0.7725 - val_recall: 0.8265 - val_precision: 0.7459 - val_auc: 0.8501 - lr: 2.5000e-05\n",
      "Epoch 249/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5458 - accuracy: 0.7504 - recall: 0.7699 - precision: 0.7411 - auc: 0.8333 - val_loss: 0.5282 - val_accuracy: 0.7708 - val_recall: 0.8313 - val_precision: 0.7416 - val_auc: 0.8501 - lr: 2.5000e-05\n",
      "Epoch 250/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5406 - accuracy: 0.7542 - recall: 0.7753 - precision: 0.7439 - auc: 0.8371 - val_loss: 0.5255 - val_accuracy: 0.7701 - val_recall: 0.8193 - val_precision: 0.7459 - val_auc: 0.8505 - lr: 1.2500e-05\n",
      "Epoch 251/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5399 - accuracy: 0.7542 - recall: 0.7743 - precision: 0.7444 - auc: 0.8378 - val_loss: 0.5247 - val_accuracy: 0.7732 - val_recall: 0.8269 - val_precision: 0.7467 - val_auc: 0.8512 - lr: 1.2500e-05\n",
      "Epoch 252/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5398 - accuracy: 0.7538 - recall: 0.7725 - precision: 0.7446 - auc: 0.8374 - val_loss: 0.5249 - val_accuracy: 0.7732 - val_recall: 0.8344 - val_precision: 0.7434 - val_auc: 0.8517 - lr: 1.2500e-05\n",
      "Epoch 253/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.5415 - accuracy: 0.7529 - recall: 0.7726 - precision: 0.7434 - auc: 0.8360 - val_loss: 0.5246 - val_accuracy: 0.7728 - val_recall: 0.8310 - val_precision: 0.7443 - val_auc: 0.8516 - lr: 1.2500e-05\n",
      "Epoch 254/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5398 - accuracy: 0.7557 - recall: 0.7752 - precision: 0.7461 - auc: 0.8379 - val_loss: 0.5236 - val_accuracy: 0.7735 - val_recall: 0.8235 - val_precision: 0.7487 - val_auc: 0.8518 - lr: 1.2500e-05\n",
      "Epoch 255/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5405 - accuracy: 0.7545 - recall: 0.7756 - precision: 0.7442 - auc: 0.8374 - val_loss: 0.5237 - val_accuracy: 0.7727 - val_recall: 0.8252 - val_precision: 0.7468 - val_auc: 0.8514 - lr: 1.2500e-05\n",
      "Epoch 256/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5382 - accuracy: 0.7545 - recall: 0.7756 - precision: 0.7441 - auc: 0.8384 - val_loss: 0.5236 - val_accuracy: 0.7735 - val_recall: 0.8325 - val_precision: 0.7446 - val_auc: 0.8518 - lr: 1.2500e-05\n",
      "Epoch 257/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5385 - accuracy: 0.7549 - recall: 0.7751 - precision: 0.7450 - auc: 0.8386 - val_loss: 0.5242 - val_accuracy: 0.7738 - val_recall: 0.8333 - val_precision: 0.7447 - val_auc: 0.8519 - lr: 1.2500e-05\n",
      "Epoch 258/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5381 - accuracy: 0.7564 - recall: 0.7749 - precision: 0.7472 - auc: 0.8386 - val_loss: 0.5234 - val_accuracy: 0.7742 - val_recall: 0.8230 - val_precision: 0.7498 - val_auc: 0.8516 - lr: 1.2500e-05\n",
      "Epoch 259/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5366 - accuracy: 0.7570 - recall: 0.7738 - precision: 0.7487 - auc: 0.8397 - val_loss: 0.5231 - val_accuracy: 0.7750 - val_recall: 0.8260 - val_precision: 0.7495 - val_auc: 0.8518 - lr: 1.2500e-05\n",
      "Epoch 260/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5375 - accuracy: 0.7557 - recall: 0.7745 - precision: 0.7464 - auc: 0.8389 - val_loss: 0.5233 - val_accuracy: 0.7751 - val_recall: 0.8246 - val_precision: 0.7503 - val_auc: 0.8519 - lr: 1.2500e-05\n",
      "Epoch 261/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5378 - accuracy: 0.7558 - recall: 0.7739 - precision: 0.7469 - auc: 0.8389 - val_loss: 0.5229 - val_accuracy: 0.7748 - val_recall: 0.8274 - val_precision: 0.7486 - val_auc: 0.8526 - lr: 1.2500e-05\n",
      "Epoch 262/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5374 - accuracy: 0.7564 - recall: 0.7760 - precision: 0.7467 - auc: 0.8390 - val_loss: 0.5222 - val_accuracy: 0.7742 - val_recall: 0.8243 - val_precision: 0.7493 - val_auc: 0.8530 - lr: 1.2500e-05\n",
      "Epoch 263/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5375 - accuracy: 0.7556 - recall: 0.7751 - precision: 0.7460 - auc: 0.8389 - val_loss: 0.5221 - val_accuracy: 0.7750 - val_recall: 0.8245 - val_precision: 0.7502 - val_auc: 0.8528 - lr: 1.2500e-05\n",
      "Epoch 264/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5368 - accuracy: 0.7577 - recall: 0.7765 - precision: 0.7484 - auc: 0.8399 - val_loss: 0.5222 - val_accuracy: 0.7739 - val_recall: 0.8335 - val_precision: 0.7447 - val_auc: 0.8533 - lr: 1.2500e-05\n",
      "Epoch 265/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5348 - accuracy: 0.7585 - recall: 0.7770 - precision: 0.7492 - auc: 0.8412 - val_loss: 0.5211 - val_accuracy: 0.7749 - val_recall: 0.8194 - val_precision: 0.7524 - val_auc: 0.8533 - lr: 1.2500e-05\n",
      "Epoch 266/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5362 - accuracy: 0.7579 - recall: 0.7748 - precision: 0.7494 - auc: 0.8404 - val_loss: 0.5219 - val_accuracy: 0.7745 - val_recall: 0.8316 - val_precision: 0.7464 - val_auc: 0.8533 - lr: 1.2500e-05\n",
      "Epoch 267/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5373 - accuracy: 0.7578 - recall: 0.7751 - precision: 0.7491 - auc: 0.8392 - val_loss: 0.5217 - val_accuracy: 0.7745 - val_recall: 0.8269 - val_precision: 0.7485 - val_auc: 0.8532 - lr: 1.2500e-05\n",
      "Epoch 268/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5370 - accuracy: 0.7579 - recall: 0.7776 - precision: 0.7482 - auc: 0.8398 - val_loss: 0.5213 - val_accuracy: 0.7771 - val_recall: 0.8318 - val_precision: 0.7498 - val_auc: 0.8535 - lr: 1.2500e-05\n",
      "Epoch 269/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5362 - accuracy: 0.7577 - recall: 0.7753 - precision: 0.7489 - auc: 0.8403 - val_loss: 0.5209 - val_accuracy: 0.7753 - val_recall: 0.8308 - val_precision: 0.7478 - val_auc: 0.8537 - lr: 1.2500e-05\n",
      "Epoch 270/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5347 - accuracy: 0.7589 - recall: 0.7768 - precision: 0.7500 - auc: 0.8413 - val_loss: 0.5210 - val_accuracy: 0.7769 - val_recall: 0.8305 - val_precision: 0.7501 - val_auc: 0.8537 - lr: 1.2500e-05\n",
      "Epoch 271/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5356 - accuracy: 0.7571 - recall: 0.7776 - precision: 0.7470 - auc: 0.8402 - val_loss: 0.5204 - val_accuracy: 0.7761 - val_recall: 0.8286 - val_precision: 0.7499 - val_auc: 0.8539 - lr: 1.2500e-05\n",
      "Epoch 272/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5362 - accuracy: 0.7566 - recall: 0.7753 - precision: 0.7474 - auc: 0.8399 - val_loss: 0.5206 - val_accuracy: 0.7756 - val_recall: 0.8273 - val_precision: 0.7497 - val_auc: 0.8542 - lr: 1.2500e-05\n",
      "Epoch 273/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5361 - accuracy: 0.7573 - recall: 0.7749 - precision: 0.7486 - auc: 0.8398 - val_loss: 0.5210 - val_accuracy: 0.7754 - val_recall: 0.8348 - val_precision: 0.7462 - val_auc: 0.8541 - lr: 1.2500e-05\n",
      "Epoch 274/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5343 - accuracy: 0.7584 - recall: 0.7788 - precision: 0.7483 - auc: 0.8415 - val_loss: 0.5202 - val_accuracy: 0.7762 - val_recall: 0.8332 - val_precision: 0.7479 - val_auc: 0.8541 - lr: 1.2500e-05\n",
      "Epoch 275/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5347 - accuracy: 0.7584 - recall: 0.7776 - precision: 0.7489 - auc: 0.8411 - val_loss: 0.5197 - val_accuracy: 0.7763 - val_recall: 0.8271 - val_precision: 0.7508 - val_auc: 0.8543 - lr: 1.2500e-05\n",
      "Epoch 276/500\n",
      "1203/1203 [==============================] - 7s 6ms/step - loss: 0.5331 - accuracy: 0.7575 - recall: 0.7775 - precision: 0.7477 - auc: 0.8414 - val_loss: 0.5196 - val_accuracy: 0.7767 - val_recall: 0.8316 - val_precision: 0.7493 - val_auc: 0.8546 - lr: 1.2500e-05\n",
      "Epoch 277/500\n",
      "1203/1203 [==============================] - 7s 6ms/step - loss: 0.5342 - accuracy: 0.7588 - recall: 0.7768 - precision: 0.7497 - auc: 0.8417 - val_loss: 0.5196 - val_accuracy: 0.7759 - val_recall: 0.8311 - val_precision: 0.7484 - val_auc: 0.8542 - lr: 1.2500e-05\n",
      "Epoch 278/500\n",
      "1203/1203 [==============================] - 7s 6ms/step - loss: 0.5349 - accuracy: 0.7555 - recall: 0.7724 - precision: 0.7471 - auc: 0.8404 - val_loss: 0.5201 - val_accuracy: 0.7764 - val_recall: 0.8276 - val_precision: 0.7507 - val_auc: 0.8541 - lr: 1.2500e-05\n",
      "Epoch 279/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5350 - accuracy: 0.7574 - recall: 0.7761 - precision: 0.7481 - auc: 0.8406 - val_loss: 0.5207 - val_accuracy: 0.7771 - val_recall: 0.8334 - val_precision: 0.7491 - val_auc: 0.8543 - lr: 1.2500e-05\n",
      "Epoch 280/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5355 - accuracy: 0.7575 - recall: 0.7776 - precision: 0.7476 - auc: 0.8405 - val_loss: 0.5203 - val_accuracy: 0.7757 - val_recall: 0.8342 - val_precision: 0.7468 - val_auc: 0.8544 - lr: 1.2500e-05\n",
      "Epoch 281/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5355 - accuracy: 0.7580 - recall: 0.7773 - precision: 0.7484 - auc: 0.8403 - val_loss: 0.5199 - val_accuracy: 0.7764 - val_recall: 0.8337 - val_precision: 0.7480 - val_auc: 0.8546 - lr: 1.2500e-05\n",
      "Epoch 282/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.5361 - accuracy: 0.7555 - recall: 0.7720 - precision: 0.7474 - auc: 0.8397 - val_loss: 0.5195 - val_accuracy: 0.7763 - val_recall: 0.8337 - val_precision: 0.7479 - val_auc: 0.8545 - lr: 1.0000e-05\n",
      "Epoch 283/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.5354 - accuracy: 0.7579 - recall: 0.7743 - precision: 0.7497 - auc: 0.8401 - val_loss: 0.5198 - val_accuracy: 0.7755 - val_recall: 0.8311 - val_precision: 0.7479 - val_auc: 0.8544 - lr: 1.0000e-05\n",
      "Epoch 284/500\n",
      "1203/1203 [==============================] - 5s 4ms/step - loss: 0.5330 - accuracy: 0.7607 - recall: 0.7781 - precision: 0.7520 - auc: 0.8425 - val_loss: 0.5193 - val_accuracy: 0.7758 - val_recall: 0.8331 - val_precision: 0.7475 - val_auc: 0.8545 - lr: 1.0000e-05\n",
      "Epoch 285/500\n",
      "1203/1203 [==============================] - 5s 5ms/step - loss: 0.5342 - accuracy: 0.7586 - recall: 0.7768 - precision: 0.7495 - auc: 0.8413 - val_loss: 0.5199 - val_accuracy: 0.7756 - val_recall: 0.8355 - val_precision: 0.7461 - val_auc: 0.8545 - lr: 1.0000e-05\n",
      "Epoch 286/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5325 - accuracy: 0.7592 - recall: 0.7776 - precision: 0.7500 - auc: 0.8424 - val_loss: 0.5191 - val_accuracy: 0.7747 - val_recall: 0.8326 - val_precision: 0.7463 - val_auc: 0.8549 - lr: 1.0000e-05\n",
      "Epoch 287/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5335 - accuracy: 0.7581 - recall: 0.7749 - precision: 0.7497 - auc: 0.8416 - val_loss: 0.5185 - val_accuracy: 0.7764 - val_recall: 0.8283 - val_precision: 0.7504 - val_auc: 0.8547 - lr: 1.0000e-05\n",
      "Epoch 288/500\n",
      "1203/1203 [==============================] - 6s 5ms/step - loss: 0.5319 - accuracy: 0.7594 - recall: 0.7759 - precision: 0.7510 - auc: 0.8428 - val_loss: 0.5183 - val_accuracy: 0.7760 - val_recall: 0.8268 - val_precision: 0.7506 - val_auc: 0.8548 - lr: 1.0000e-05\n",
      "2406/2406 [==============================] - 3s 1ms/step\n",
      "602/602 [==============================] - 1s 1ms/step\n",
      "Acurácia no conjunto de treinamento: 0.8772214651107788\n",
      "Acurácia no conjunto de teste: 0.777125358581543\n",
      "AUC no conjunto de treinamento: 0.9515866787402086\n",
      "AUC no conjunto de teste: 0.8535293575430207\n",
      "602/602 [==============================] - 1s 1ms/step\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Classe 0       0.81      0.72      0.76      9622\n",
      "    Classe 1       0.75      0.83      0.79      9622\n",
      "\n",
      "    accuracy                           0.78     19244\n",
      "   macro avg       0.78      0.78      0.78     19244\n",
      "weighted avg       0.78      0.78      0.78     19244\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEYCAYAAAByXKB5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAAsTAAALEwEAmpwYAABJD0lEQVR4nO3dd3iUVfbA8e9JDymEEEILJdTQWwQsSFNEVBBFhbVh767u/lzLsvbddV37ytq7CBYEy4KNIggIhN57SUJLIUASUuf+/rhvYAhJSEImk4TzeZ55MvPWc2dgzrz33vdeMcaglFJKVZSPtwNQSilVO2kCUUopVSmaQJRSSlWKJhCllFKVoglEKaVUpWgCUUopVSmaQFSNICKtRcSIiJ+3Y6lqIvKhiDzrPB8gIps8cI6dInJBFRynxn0O5S1bTYy9rtMEUseJyB9EJEFEMkVkr4jMFJHzvB2XJ4jIeOcL5Bpvx1IaY8x8Y0xHb8fhCU6iNCIyqtjyl53l470UmvIQTSB1mIj8CXgF+AfQGGgJ/BcYVcZupR2rNvyquxFIB244nYOIiG/VhHNG2ozb++/8u7ka2Oa1iJTHaAKpo0SkPvA0cI8x5mtjTJYxJt8Y850x5iFnm2NVK87rQSKS5PZ6p4g8LCKrgSzn+VfFzvOqiLzmPL9JRDaIyBER2S4id5QRn6+IvCAiqSKyHbikePwi8p5z1ZQsIs+W9cUuIq2AgcDtwEUi0qR4uUTkMed8O0XkWrf1H4rIGyIyQ0SygMEi0kxEpopIiojsEJH73bZ/UkS+EJGPnbKuE5F4t/W9RGS5s+5zIKik91hErnGuDIseuSIy11l3iYisEJHDIpIoIk8WK+/1IrJLRNJE5K/F1gWKyCsissd5vCIigZX8HJqJyLciki4iW0XkttI+A8d3wHki0sB5PRxYDexzO6aPiExw4j/gvI/1y1k2HxF5RES2Oeu/EJHIUspW0dhVBWkCqbvOxn5xTTvN44zDfqlEAFOAESISBsd+qV8NfOZsewC4FAgHbgJeFpHepRz3NmfbXkA8MKbY+g+BAqCds80w4NYy4rwBSDDGTAU2ANcWW98EiAKaY69U3hYR96qkPwB/B8KAhdgvwlXO9kOBB0TkIrftR2LfjwjgW+B1ABEJAKYDnwCRwJfAlSUFbIz53BgTaowJBZoB24HJzuosp0wR2Pf/LhG53DlHZ+AN4Hpnv4ZAjNuh/wr0B3oCPYC+wISSYuDUn8MUIMk5zxjgHyIypJRjAeQA3wBjndc3AB8X22a88xgMtAFCOf7+naps9wGXY38sNAMOAhNLiaWisauKMsboow4+sF+g+06xzYfAs26vBwFJbq93AjcX2+c34Abn+YXAtjKOPx34YynrZgN3ur0eBhjAD1vdlgsEu60fB8wp41xbgAec548Cq4qVqwAIcVv2BfA3t/fhY7d1/YDdxY7/KPCB8/xJ4Be3dZ2Bo87z84E9gLitX1j0Phd/j51lPsD3wBtllO8V4GXn+ePAFLd1IUAecIHzehswwm39RcDOSnwOLYBCIMxt/T+BD8v69wScByzCJr/9QLDz72a8s90s4G63/ToC+c45T1W2DcBQt/VN3fZtXdnY9VG5h16B1F1pQJScfttFYrHXn2G/zMH+ai+6+kBELhaR350qgwxgBPZXf0maFTv2LrfnrQB/YK+IZDjHeguILulAInIuEIv9xVkUYzcR6em22UFjTFax8zVze+0eSyugWdG5nfM/hk1sRfa5Pc8Ggpz3uhmQbJxvrBLKVpKiKx/3arJ+IjLHqUI7BNzJ8ffyhPfOKVea2/GaFTtn8bJSbNvSPodmQLox5kix9c3LKowx5jegEfZK6HtjzNESzlk8vqIfDqcqWytgmtvnsgGbKNw/m0rHripGE0jdtQj7K/7yMrbJAuq5vW5SwjbFh2v+EhgkIjHAaJwE4tSxTwVeABobYyKAGYCUcu692F+JRVq6PU90Yo8yxkQ4j3BjTJdSjnWjc56VIrIPWOy2vEgDEQkpdr49pZQzEdjhdu4IY0yYMWZEKecvXq7mIuJe7palbSwiY7EJeYwxJt9t1WfYqrEWxpj6wJscfy9PeO9EpB62qqfIHuwXrfv53ctaPN7SPoc9QGRRlaXb+uTSyuPmU+DPnFx9VVp8BdirlVOVLRG4uNhnE2SMKR7T6cSuykkTSB1ljDmErQ6YKCKXi0g9EfF3rhKedzZbiW3TiBTb6PxAOY6bAswFPsB+yW5wVgUAgUAKUCAiF2OrQ0rzBXC/iMQ4Da6PuJ1jL/AT8KKIhDsNp21FZGDxg4hIELYd5nZsnX/R4z7gD8WuwJ4SkQARGYCt9/+ylNiWAEfEdhoIdhqau4rIWWWUp8gi7Jfh/c77fQW2DeIkItIL+A9wufO+ugvD/oLOEZG+2Ku9Il8Bl4rIeU6by9Oc+H95MjBBRBqJSBT238GnpcRb1ueQiK1++6eIBIlId+CWMo7l7jVsFee8EtZNBh4UkVgRCcX2EvzcGFNQjrK9CfxdbKcJnDKe1KvwNGNX5aQJpA4zxrwI/AnbgJqC/fV2L7ZtAmxD7ypsW8dPwOflPPRnwAW4VV85VQX3Y7+QDmK/8L4t4xjvAD86518OfF1s/Q3YpLTeOd5X2Pru4i4HjmLbMPYVPYD3sdUiw53t9jnH2QNMwtb7bywpMGNMITbB9AR2AKnAu0D9krYvtm8ecAW2kTgduKaEshUZBTQAfpPjPbFmOuvuBp4WkSPYBPCF2znWAfdg3/+9TrmS3I77LJCA7f20Bvv+PkvJTvU5jMO2LezBdsh4whjzS6lvwPEY040xs4pV5RV5H/tvbx72/c3BJvzylO1V7L+rn5z35ndsm1VJKhW7Kj8p+fNVqu4QkUHAp8aYmFNsqpSqAL0CUUopVSmaQJRSSlWKVmEppZSqFL0CUUopVSm1YYC8comKijKtW7f2dhhKKVWrLFu2LNUY06gy+9aZBNK6dWsSEhK8HYZSStUqInKqkRJKpVVYSimlKkUTiFJKqUrRBKKUUqpS6kwbSEny8/NJSkoiJyfH26HUekFBQcTExODv7+/tUJRSNUSdTiBJSUmEhYXRunVrThwcVVWEMYa0tDSSkpKIjY31djhKqRqiTldh5eTk0LBhQ00ep0lEaNiwoV7JKaVOUKcTCKDJo4ro+6iUKq7OJxCllKoxNs2E1K1wYCOkbjl5fV42HHLmvDq4Cxa/BbmZ1RtjBdTpNhBvS0tLY+jQoQDs27cPX19fGjWyN3wuWbKEgICAUvd98803qVevHjfccEO1xKqUOk3GgHGBj2/J6/eshMnjIDQaco9A/lHoMho6XQpJCbD5RziUBIW50LwP7F0FrgJI+AAy94OrELqNgZAoCAyDdhfAD49C+naIuwSG/7NaiwuaQDyqYcOGrFy5EoAnn3yS0NBQ/u///u/Y+oKCAvz8Sv4I7rzzzuoIUSl1OgrzYd9qWPIurJ1qv/Cb9oBmPWHzT9BzHGyfC/Wi7Bd9cIRNHqHREHcprPgE1n0N4gMdhkOcM2vytjnQ/26IjIUfHoN2QyEgBJZ9aJMUBn6aYLdtOxRyDnml+JpAqtn48eMJCgpixYoVnHvuudxzzz3cc889pKSkUK9ePd555x3i4uJOSDiDBg2iX79+zJkzh4yMDN577z0GDBhATk4Od911FwkJCfj5+fHSSy8xePBgbxdRqboh5xAEOZNQzn8RUjZDw7aQtBR8A+wX+obvID/bvu55LQSFw/ZfIeF9iGwD8/4NES3tsQpy4NKXoUl3CG4A9SJhyN8gbas9T0SLkuPoPR58nNaGi/4J/sFwNB0WvAqNu0KfG6vl7SjJGZNAnvpuHev3HK7SY3ZuFs4Tl3Wp8H5JSUksXLgQX19fhg4dyptvvkn79u1ZvHgxd999N7Nnzz5pn4KCApYsWcKMGTN46qmn+OWXX5g4cSIiwpo1a9i4cSPDhg1j8+bNBAUFVUXxlKpbXC7718fHqW4yx7+Y3eVlw4+P2V/75z8E7S+EWU9DSCNYPcVeTRTmQ2GevcJoPQBan2evKork54CvP2z9BVqdC4GhJcfkHwRNupYdt3uMIQ3t34B6MOLf5S66p5wxCaQmueqqq/D19SUzM5OFCxdy1VVXHVuXm5tb4j5XXHEFAH369GHnzp0A/Pbbb9x3330AxMXF0apVKzZv3kz37t09WwClapvsdPhoJETH2V/xU2+B9B3Q6hxI3waj34LfXoZNMyAwHA7utNVQ856HlZMgOBL+uAoKcu0VgKvAtkkER5R8Pn/nR1yHi6qpgN5xxiSQylwpeEpISAgALpeLiIiIY+0kZQkMDATA19eXgoICT4anVPVxuWzbQMO2cKqu4kvfg6MH7Rf49l9h4MO2vWHvKvvrv34MZKfZK4vpd0LKJluNdO79MP8l2L/GPpKWwpF9ENoY1n5ljz2xLyC2rWH/OrjmE9sm8fPjsPRdGDLBVlkFhHj8LalNzpgEUhOFh4cTGxvLl19+yVVXXYUxhtWrV9OjR49y7T9gwAAmTZrEkCFD2Lx5M7t376Zjx44ejlqpKvTjY7D4DbjwGVuVc2AjtB0MqyZDQR7Uawix59skMfOh4/v5BsC2WdCsl00IAA3bHW9PKMiDTpfB7t/h0yvt+ktegl+ehIzdcO2XtmopO91edfz2Mlz5HrQ6+8T4hv8TLnjSnk+dRBOIl02aNIm77rqLZ599lvz8fMaOHVvuBHL33Xdz11130a1bN/z8/Pjwww+PXako5XWFBfD1rRDeHIY9a+9p2Dkf6reA1M32CuFwEoQ1hZ//duK+4gN+wZCfBXOcZW2H2iqnw8kweAJMutImjyETbHXSlp+g7x2w+Qe46O82gRw9CN/eb5PQWbfYq5TCPNsFFqB+c+h7G5x1a+lXQH76f6o0dWZO9Pj4eFN8QqkNGzbQqVMnL0VU9+j7qSpk9rO2FxJAuwth68+2ITo3E6LaQ3QnaB5vG6JXToaodjaZLHoduo6xVyK5R+y6oPrQ7aoTG5RzM20iat7bO+WrI0RkmTEmvjL76hWIUqpquApte8Gar2xbwm8v20Swd6W9F2LAn+2Vg8jJv/b73X78+aiJx58Hhp24zl1gqCYPL9MEopQ6falb4H9/hh2/2naLuUvAL8hWXfn6226v4U29HaWqYjoWllKq/FwuWPmZbVcoyLPL9q+HN86B5GUw8j9wb4LtHTXwLzZphERp8qhCXyYksnn/EW+HAegViFKqIha8ArOess87joD2w+y9Er6BcO/S44nijnleC7E2SM3MJcDPh/CgkidoO5pXSHDAyWNqLduVzkNfrSamQTATLulM66h6bNh7mEPZ+Yw/t/rn6tEEotSZKucQzHwEts2GwY9Cn/G2i+vWWZCXBeun24buvndAykZoOwQWTYQ2g2HPcvjlCfjqZttTasCf9SqjGJfL8NGinUxZkkiXZuF0bhZOZm4B/r4+vDl3G1FhgUy/+1yCA3x5/oeNzFy7j0ZhgTQJD+LXzSnM+OMA6gX4smnfERbvSOOGs1vz4k+bCQvyIznjKHd+ugxfH6HQZTinbUOuP7s1vj7VO+2CJhClzjRZqban095V9oa86E7w3R9h5wJY/40dDRbAP8R2k13/jU024mMH8hvwJ1j1Oaz8FKI727utz/2jd8vkRQcO5/DKrC1s3Z/Ji1f3wM9XWLE7gx2pWfz7x010j6nPT+v38/WK5GP7tIkKIfFgNpf/dwEuY9iVls2FnRuzcd9hViZm4Osj3D1pORv3Haaoo+zMNfvYnprF45d2pmVkPQB+Wr+PkEA/Hr24U7UnD9AE4nGDBw/mkUce4aKLjg9p8Morr7Bp0ybeeOONk7YfNGgQL7zwAvHx8YwYMYLPPvuMiIiIE7YpaWTf8nrllVeYMmUKLVq04PHHH6dbt24VPoaq5X55AlZ8ap9f9A97D8SMh2D5x9Cyv23HCI60jd8T+9p7KUa/bROOj48d+8kv2CaXqz+2XXLPIE98s5YjOQU8OaoLSen2SmDfoRwC/HwY8uJc8guP3xoxNC6ad2+Mxxg4klNAcIAvB7PzaBgSwPwtqbwxdxsGw1MjuzCoYzQ5+YXsyTjKf+du46tlSfRtHcm9Q9qRsOsgr83aQlyTMK4/uxX+vrb5+oLOjb31NgCaQDxu3LhxTJky5YQEMmXKFJ5//vlT7jtjxowqj+eBBx7ggQceqPLjqmqWn3N8vKXistPt1ULxcZp2LbLjOq38zFZLxd8EjeJsl9qRr9mhQUKjbeIoct3XdrTZmHjocc3x5S3OgnuXVHmxvKWg0MXm/Zl0bhZ+wvJCl+HuScvYnpJFhyZhnNWqAR8t2gVw7IoiLMiPL+48m3oBvkxZkkijsEDimoYxf3Mqdw1qi4ggAvXr2fe1cbj93AbHRTM4LvqE8wX5+9KmUSh/HGqT8l8u6kh0eBD92kSSmVPA1WfFHEseNYFHIxGR4SKySUS2isgjJax/WURWOo/NIpLhtq7Qbd23nozTk8aMGcP//vc/8vJsj5WdO3eyZ88eJk+eTHx8PF26dOGJJ54ocd/WrVuTmpoKwN///nc6dOjAeeedx6ZNm45t884773DWWWfRo0cPrrzySrKzswHYv38/o0ePpkePHvTs2ZOEhAQyMzMZOnQovXv3plu3bnzzzTfHjvPSSy/RtWtXunbtyiuvvOKhd0NVib2r4J8xsOH7k9flHIK3BsLLXeH3N+Bohr0HY9pd8MFw2PCtvSt70CO26sr9foz6zU9MHgCNO9vkUcut2H2QtcknzplhjLHtC9vTePPXbYx4bT4/rN3LP2ZsIDnjKADzNqfw47r9NAwN4PdtaTz53XqiQgP46Oa+/PnCDvzzim7M+vNAeraIoEPjMB6/rDN3DWrL4I7RPH5ZZxqFVe4u9haR9Xjhqh5EO8km0M+Xxy/rTFyT8FPsWb08dgUiIr7AROBCIAlYKiLfGmPWF21jjHnQbfv7gF5uhzhqjOlZZQHNfAT2ramywwHQpBtc/FyZm0RGRtK3b19mzpzJqFGjmDJlCldffTWPPfYYkZGRFBYWMnToUFavXl3qKLrLli1jypQprFy5koKCAnr37k2fPn0AO0rvbbfdBsCECRN47733uO+++7j//vsZMmQI06ZNo6CggOzsbIKCgpg2bRrh4eGkpqbSv39/Ro4cyfLly/nggw9YvHgxxhj69evHwIED6dWrV4nxKC9bMQlc+TDj/+zQHvUi7QCCv/4LNn5vhwdpdS788Aj8/IRt0/Dxh/MetMOT19IBAQsKXRQaQ6BfyTP+GWMQt4R46Gg+9362nPhWkbzx61Zy8l20jw7lhrNbcTingJd+3kyhy1Y31XN6PN01aTnGwLQVydw7uB2zNx6gYUgAH9/cD5cxfPr7LuKahHNe+ygGdmjk+ULXcJ6swuoLbDXGbAcQkSnAKGB9KduPA0r+KV7LFVVjFSWQ9957jy+++IK3336bgoIC9u7dy/r160tNIPPnz2f06NHUq2cbzkaOHHls3dq1a5kwYQIZGRlkZmYeqyqbPXs2n3zyCQB+fn6Eh4eTn5/PY489xrx58/Dx8SE5OZn9+/fz22+/MXr06GOjBF9xxRXMnz9fE0hNk7gUlr5j55ho3A1SNsDr8eDjZxuzt8+ByLZw8fMQf4tNKGlbode10KhTre4llZlbwLi3fye/0MX0e84lyP94Elm8PY0J09eyKz2boXHRXNytKWuSMli8I53VSYeYvyWVIH8fHrygAz9v2MdT363Hx0fo2zqSy3o046tliSzfncHoXs2ZtiKZewa3Zd7mVJ74dh0Atw2IJcDPVtbcOqCNV8pfU3kygTQHEt1eJwH9StpQRFoBsYD7TEpBIpIAFADPGWOml7Df7cDtAC1btiw7mlNcKXjSqFGjePDBB1m+fDnZ2dlERkbywgsvsHTpUho0aMD48ePJycmp1LHHjx/P9OnT6dGjBx9++CFz584tddtJkyaRkpLCsmXL8Pf3p3Xr1pU+r/Kw/Bx75VA0I97qL+Cbe4/3kBr5HwhvBvNesDPdFU1cdOP3x8eLGvyod2IvQ2ZuAalHcmkdFcLRvEJWJ2XQoXEYXyQkcl3/VoQE+rFh72Ge+HYdB7Py6N+mIXcPbsvT361n3Z5DuAz8ddpa4ls3YNmug0SFBvLJop1EhwdxZe8Yfli7l5lr9xHg50NkvQCeGdWFXzYc4ILOjbm+fyv+0K8lg/49h5wCF89d2Y1WDUO4pHtT1iUf4uy2Dbl3SDvaNgrloYtg8/4jrEk65PWG6pqspjSijwW+MsYUui1rZYxJFpE2wGwRWWOM2ea+kzHmbeBtsIMpVl+4FRMaGsrgwYO5+eabGTduHIcPHyYkJIT69euzf/9+Zs6cyaBBg0rd//zzz2f8+PE8+uijFBQU8N1333HHHXcAcOTIEZo2bUp+fj6TJk2iefPmAAwdOpS33nqL++6771gV1qFDh4iOjsbf3585c+awa5dtDBwwYADjx4/nkUcewRjDtGnTjl29KC+ZegvsmA/XfgGHkuDr26DVeTD6DTu1aruhtv1i7CS7/c7f7FVISTPsedHWA0cIDvBjwZZUDhzJYc+hHKavSOZ/9w/ggc9Xsioxg6jQAFIz8zick09UaCD/nLmR+sH+9Iipz+dLE5mxZi9pWXk8dFFHMnMLeGPuNqYuTyIs0I+svALiW0fyn3G9aBwexJMjO7Nidwadm4Ufu0nv+rNbH4unUVgg/xrTnfSsPFo1tFfc9YP9OaddFABtGx2fObBD4zA6NA6rvjerFvJkAkkG3Cf5jXGWlWQscI/7AmNMsvN3u4jMxbaPbDt519ph3LhxjB49milTphAXF0evXr2Ii4ujRYsWnHvuuWXu27t3b6655hp69OhBdHQ0Z5111rF1zzzzDP369aNRo0b069ePI0fsEAevvvoqt912G8899xwNGzbkgw8+4Nprr+Wyyy6jW7duxMfHExcXd+z448ePp2/fvgDceuutWn3lTXtX27YM30CYdDWENrJzX18/DfwC7BzbxbU+r/rjPIWdqVmMen0B0eFBpGflkZVbQFiQH9l5hYx6/TdyClxc0q0pP6zbR8fGYUycY/97D42L5vkx3WkYGsjy3QcZ+/bvtIkK4bYBbQjw8+GSbk1JzcxlYIdG5BW6TmgTCfTzpX+bhmXGdWn3Zh4t95nEY8O5i4gfsBkYik0cS4E/GGPWFdsuDvgBiDVOMCLSAMg2xuSKSBSwCBjl3gBfnA7nXrKFCxeyadMmbrrpptM+lr6fHpCyyQ57Hhxh2y92LYTExXZk2xumw/vDbYP5yNeh9/XejrZMxhh+355Op6ZhRNQL4Oo3F7Ei8eAJ90UAhAT4kpVXyBOXdeamc2PJyi0g5Ugud366jHF9W3LD2a1OaAzftO8I4cF+NK0fXN1FOiPUyOHcjTEFInIv8CPgC7xvjFknIk8DCcaYoq65Y4Ep5sRM1gl4S0Rc2K7Gz5WVPFTJJk+ezN/+9jcmTJjg7VCUO1ch5B6G7/9khwsJDLOj1eZnQ0Qr6HSpnfsiJt7e9b3yM+g2xttRl8oYw0/r9zN1WRI/rd9Pk/AgHrukE0t2pvPoxXFMX7kHwd5TsTUlk49v6cvSnQe50alaCgn0IyTQjx8eOL/E43dsotVINZVOKKXKTd/P05R5wDaGz3raTpFakANn3wNn32vntnAVQEDoifdmFP3/PNV84V70zcpk/jhlJSEBvlx3diu+XbmHfYdt54xFjwwlyN8Hl4G1yYfYeiCTm8+r/kH/VOlq5BVITVG8b7iqnLryQ8NrtvwCn10NptAORhgcYbvaxg4oe78a/m83M7eAZ/+3gR4x9Zl61zn4+frQP7YhN324lH6xkTSpf/xu+fM7NOJ8vXeiTqnTCSQoKIi0tDQaNmyoSeQ0GGNIS0sjKKiUoTPUqc173t7pfdlrEDuwxvWWqqyPF+0k5Ugub1/fBz9niI3BcdE8eVlnureI8G5wyuPqdAKJiYkhKSmJlJQUb4dS6wUFBRETE+PtMGqnxKW2Yfzi5+0837WUMQZj7EVRVl4hArz/2w7O79CIXi0bnLCtN+amUNWvTicQf39/YmP1H7LyshUf26HRe17r1TCycgvYtP8IvVs24EhOPm/+uo07BrYlPMifvAIXHy3cyfkdGhFRz5/wIP9jExodys4nLSuX937bwdTlSTQKCyQx/Sj1g/3tcCGD23m1XMp76nQCUcrr8nNg3TfQeaRtKPcSl8tw56fLmL8llen3nMvv29OYOGcbQX6+3DmoLeM/WMLCbWn8d+5WsvMK6dgkjEeGx+Hn68Pj36xl4z57f9GA9lH4iHBJt2bsTs/ihrNb0zc20mvlUt6lCUQpT1r7FeQegu7XnHrbKvLxop18vjSRs1pHMuGSTqxMzODd+TuYvyUVPx/h9dlb2HogE4CPFu2iYWggC7elcd+QdkxavJt20aGsST7EH95dDNgqqzsGtqFRaCC3nBer7YnqmDrdjVcpr0rZBG8PtsOm3/IT+JQ8iuzpKih04esjiAjbUjIZ9vI8IkMCSDmSS1yTMDbuO0Kgnw/3D21PXoGLV2dtAeCa+BZ8npCIj0DHJuHMuP88cgtcBPr58MPafRzNL+TQ0XzCg/y5so+2f9VV2o1XqZro9zfsz/drPqmy5JFbUEigny97Mo7y3MyNXNi5MROmr8Xf14cG9fzJyi0g0M+Hr+86h6vfWsSm/Ud4amQXLuvRjMiQAAoKXcRGhXDgSA43nRtL39hIXvp5Mw8P74iIHBvl9uJutXfkXlV99ApEqdP1/YN2bpj4m09c/moPO8DhuMmndXiXyyACL/+8mTd/3c4dA9vw3ao97Eyzk4fVD/bngk6N7ZAgmblc3rMZ15/dmk37jpCelcfZbcseG0qd2fQKRClv2b0YEt638224J5D07XBwp73L/BQOZuURFuTHt6v2EFHPnyFxx4cPf+mnTXyRkMSA9lF8uSyJ5hHB/Gf2VqLDAnl1bE8+W7ybO50Z8IrTIUCUp2kCUaokLhfkZNjZ/tztWwsN28JvL0Ony2D+C3Z5ygY4ss+Oort/HYQ2scvbDinzNJ8s2skz/9tAm6gQNu47go/Aq2N7cVmPZizdmc5/5mzFR4QvlyVxff9WPH5ZZ1YlZtA9JoIAPx9G9Wxe9WVXqpw0gShVkt8nwpx/wN2L7ACHGbsgIxE+uhTCY+y0sb+/aXtYdb0S1k6FFZ/Cr88fn/Sp5TkQWfoMdskZR/nbN+vo0SKCtcmHaB8dSoN6ATzw+Uqy8wr479xtxDQI5sOb+rJh72Eu6dYUESG+tXabVTWDJhClihTdZm0MLP/Yjo4762lo0Q9m/sVeVfgF2eTRZjDsmAetB8Dot2DbbJjzdztI4qUvQ3YanHN/mWNZzVyzF4DXxvYkM7eA6LAgggN8Gf/+Eh6eugaAKbf3p22j0BMmOlKqptAEohTAjIdgyTsQcxYMfBhSN0NUR3tlsW66TQyZ+2DAn+09HZFtIX0b1I8BX38Y9nfYPhc6j7LDsZchO6+AeZtTmbo8mS7Nwo/NjFdk8u39+W7VHkQ45eRISnmT9sJSyhh4oT2ENoa0rXaYdb9geGANrPwUNv0Al7wAG76Dfnee3C5STjn5hSzclsoDU1ZyOKcAgIeHx3HXoLZVWRqlKkR7YSlVHllp9su/eLVS6mbISoEhf7PdbrfPgbhL7FSy5z1oH2C76lZCfqGL52Zu5IMFO3AZ6NIsnL9e0okgf1+6Nqt/moVSyns0gagzQ0YivNbLXkn0GX/iup2/2b+tz7M9rFqcddLup+PDBTt577cdjOkTQ9dm4YyJb0FooP7XU7Wf/itWZ4Ydv9q5xX97GXpdb+8MP7wHCvNs1VRokzJ7TFVWocvw0aKd9IuN5IWrelT58ZXyJk0gqm5xuU6erOnoQecqQ+zNfau/gPYXwhvn2HUAQyZ4ZPa/2RsPkHTwKH8doVMBq7pHE4iqOxa8BkvfgbsWwZG9sPQ9aNodpt8NGIi7FDL32y65q/tA7hEY+jg06+2xiZ6+WpZIVGggF3ZufOqNlaplNIGo2i1xie01FXs+bP0FMnbDvH/DppmQuunEbVufBx1HwHvDbEP50Mdtt1wPOZSdz5yNKVzXv9Wx6V6Vqks0gajabcb/Qdo2uDcB9qy0yxa8Agj0vA7WfAljJ8G+NdBjHARHwINrwbjAL9Cjoc1cu5e8QheX92rm0fMo5S2aQFTtlZ9jx51yFcDUW+ywIoMehZAoaNEfmnS1va78g22bRxFf/2oJ75cN+4lpEEy35tpVV9VNmkBU7WCMvSu8ZX979zfA/rU2eTTpBrsW2GVxl5x4v4Z/cLWHWlDoIr/QsHBbGlf2jtEZ/FSdpQlE1Q67FtirjCbdoMtoaH0+7Flh1135Hnw00o6e2yiuWsMyxvDhwp10aBxGr5YRPDx1DTPW7KVegC/ZeYUM6tioWuNRqjppAlE1j8sFB9ZD4y6w+Qc7sGHaNvAPsW0Z+9ZAs152Do6QaIjqAFd9YLfxYPXUit0Heeir1RzMyuOZy7uyZX8mq5IymL3xAK0a1qNto1DmbjrA9f1b8eO6/eQXunQyJ1Wn6VhYquaZ/aztSdXyHNi9EIIj4Wg6XPgMNGgFuxbC4jfteFXtL7RTxnrAlCW7mbYimefHdGdbSib3TFpBVFgAQX6+bE/NotBliAwJoE1UCAm77P0kfxnekbsHtSM1M5eUI7l0ahrukdiUqio1diwsERkOvAr4Au8aY54rtv5loKgDfj0g2hgT4ay7EZjgrHvWGPORJ2NVNcT+dfZu8dDGNnn0vhEuecne11E/xt7sFzsQln1ke1Fd9PcqDyE1M5cXf9rE5CWJ+Ahc/Op8juYX0rVZfd4ffxbpWXlc9vpvXN6zOS9c1Z3cAhd9//4LAX6+3HROLABRoYFEhXq2l5dS3uaxKxAR8QU2AxcCScBSYJwxZn0p298H9DLG3CwikUACEA8YYBnQxxhzsLTz6RVILZaZYqeAbdnP3vS3/hv44yo4lAhNe5Z8h/jWWRDSyN4oWEEHjuQQERxAgJ+9N+OdedtZsjOdsWe1YE3yIV75ZQsA9wxuy8Vdm/L+gh20jKzHbQPaEOKMYZWWmUtkSMCxBvJ5m1MI8velb6xO9qRql5p6BdIX2GqM2Q4gIlOAUUCJCQQYBzzhPL8I+NkYk+7s+zMwHJjswXiVt8x8CNZNs3eKb50FPcbarrghUaXv025opU61ZEc61727mJjIYOKahHFp92b8Z/YWsvIK+XVTCggM7tiI+4a2p3fLBgC8dHXPk47TsNjVxfkdtLFcnXk8eXtscyDR7XWSs+wkItIKiAVmV2RfEbldRBJEJCElJaVKglbVaO5z8NXNdr6Nxt1gy89QcPTk0XJPw/aUTG79KIEdqVnsO5TD7Z8k0CwiCD8fYfbGA/z5i1Uczingycs6Exrkh8tleHpU12PJQylVuprSC2ss8JUxprAiOxlj3gbeBluF5YnAlIcU5MHv/4WcQ/b1iOdtu8f+ddCsZ5WcIr/QxR+nrGRN8iHSs3KpF+BHbr6LD+7uS2xUCNNWJPHg56vw9RFG9mxOjxYR7D2UQ4vIelVyfqXqOk9egSQDLdxexzjLSjKWE6unKrKvqm1SNsOKj23yCG0MES3tneMN20Lnkad16G0pmUycs5XsvALu+nQ5a5IPMbpXc5bvzuC3ran89ZJOxEbZKWQv7tqU+sH+9GoRQf1gf7rHRHBRlyZVUUKlzgiebET3wzaiD8V++S8F/mCMWVdsuzjgByDWOME4jejLgN7OZsuxjejppZ1PG9FriYI8eLkLZB2AgFA7baxxld3ecQprkw9x4EgOQ+Iac8uHS5m18QCNwgJJOZLLM6O6cF3/VizbdZDG4UEnXV0s332Q8CA/2kWHnW7JlKqVamQjujGmQETuBX7EduN93xizTkSeBhKMMd86m44Fphi3TGaMSReRZ7BJB+DpspKHqkU2fm+TR+xAaDOw0vOLF3G5DPdNXsGO1CzuHNiWOZsO0CQ8iJTMXF6+pgeje9lhT+Jbl3webetQqvL0RkJVPY7ss+NZTRkH2Wlw/0o7K+Bpmr1xPzd/mEC76FC2HshEBOY9NJjQQD8ahAScftxK1XE18gpEneGy0mDWk3DuAyA+dva//GwQXxj91mklD5fLcMP7S/D1EXalZdG0fhAz/ziARdvSOJyTr43gSlUTTSDKM2b+BdZ+BSmb7Ii44mOHWm8zyI6oWwkHs/LIzi9k7qYD/LY1FT8fIaKeP29c1wd/Xx+9F0OpaqYJRFWt/Bz4/gGbPFr0h8Tf7fIRL0Df28p9mN+3p/HQV6v4z7jeNA4PJMDXh/Ofn0NWnu3p3T2mPu/eEI+/r49WVSnlJZpAVNVa/TmsmgznPQiD/wqbZkBUR4gu/zDrOfmFPDx1NYnpRxn/wRIysvNpHx1KVl4hEy7pRICfD4M6RBMdHuTBgiilTkUTiDo9G2fAhu9g9Bv29dZfILw5DH3CjmHVeVS5DmOMYVXSIfx8hP+t2cuutGzuGtSWt37dRvOIYLYcyOTcdg25dUAbDxZGKVURmkDU6Vn9OayfDkMm2JsCt/9qbwas4Cx8z/+4iTfmbjv2+qo+MTw8PI57BrcjJ7+QBz9fyR+Htq/i4JVSp0MTiDo9B5yxMef+E3b8auclL+dAh7vTstmWmknnpuF8+vsuhsZFc1ZsJAu3pfHXSzoBEBroR2igH5/c0s9TJVBKVZImEFV5+Tl2FkCAFZ9AUH1odR60LTuBZOUW4Osj3PzR0mP3bhgDdw9uS59Wkdw5sG01BK+UOl2aQFTlpW4CU2jv7TCF0P8eGPRwmbu89es2Xp21hXoBfqRm5vLgBR3YfyQHY4zeFa5ULaMJRFXefqf6qtOltiG95x/K3PzA4Rz+9cNG4ltHsi75EG2iQrh3SDt8fSrWXqKUqhk0gaiKK8i1c3msmgy+gXDZa3DenyCiRYmb5xYUMnHONnanZeEy8M8ruhHk74uviCYPpWoxTSCq4qbdCeu+tgMitrsAgiMguOdJm+1Oy+auScvIzC1gV1o2AD1bRNC2UWj1xquU8ghNIKpijuyzc5affS9c9PdSNzuaV8gtHy1l/+EcOjYJ4+5Bbdl6IJMhcY2rMVillCdpAlEVs2qKbTDvc1OZm/20fh9bDmTy3o3xDO2kSUOpusiTMxKqusQY+1g5yY5xFdXupE3yClysTsogO6+AGWv20jg8kMEdo70QrFKqOugViCpbYT58eiXsWghth0DqZhj5+kmbbT2QyX2TV7Bh72EC/XxwGcO1/Vrho43kStVZegWiyrZqsr3DvFkv2PIj+NeDLpefsEnCznRG/3cBBw7n8OzlXRnRrSmCcEXv5t6JWSlVLfQKRJUuNxPm/Rua9YYbv4MPhtvqq8AwtqVkkp6VR0yDYO78dBlRoYF8cktfYhrU47r+rXjxqh569aFUHacJRJXMGJh+FxxKgsvfAP8guG0OiJCdV8AN7y1h76GjNK0fzNG8Qqbc3p+YBsdnAtTkoVTdV64EIiKXAF2AYxMwGGOe9lRQqgaY/yJs+BaGPQutz7PLnBF2X521heSMo7RtFML+w7l8eHNf2kWHeTFYpZQ3nDKBiMibQD1gMPAuMAZY4uG4lDclLoXZz0K3q+z9Hm427TvCe/N3cHV8DM9e3o3svAIi6umMgEqdicpzBXKOMaa7iKw2xjwlIi8CMz0dmPKSvCz434MQ1hQufeXYVcfy3QfJzi3kHzM2EBrkxyMX25kBA/w0eSh1pipPAjnq/M0WkWZAGtDUcyEpr0nZBB9dBpn74ZpPIdAOOZKYns3Yt34nr9BFkL8Pb1zXh0idh1ypM155Esj3IhIB/BtYDhhsVZaqS4yB//3ZDpR4y8/Qou+xVS/9vBkR+PeY7vRqGaHtHUopoBwJxBjzjPN0qoh8DwQZYw55NixVbTISYc9yMC7YOR9GvHBC8tieksn0lcncfn4broovebRdpdSZqdQEIiJDjDGzReSKEtZhjPnas6Epj9sxHyaPg7wj4BcMjbtB/M0AGGOYsWYf365Kxt/Xh1vPa+PlYJVSNU1ZVyADgdnAZSWsM4AmkNpuwasQGAZtB9sJoUY8Dz6+ACzfncE9ny0HYFzfljQKC/RmpEqpGqjUBGKMecL5W/awq6p2OpoB2+dC/zvhgqfgUCI0aH1s9edLd1MvwJdXx/aif5tIb0WplKrBTjkWloj8w2lEL3rdQESeLc/BRWS4iGwSka0i8kgp21wtIutFZJ2IfOa2vFBEVjqPb8tzPlUBm38AVz50vtxedbgljyM5+Xy/ei+XdW/GhZ0bExbk77UwlVI1V3l6YV1sjHms6IUx5qCIjAAmlLWTiPgCE4ELgSRgqYh8a4xZ77ZNe+BR4FznuO5jfx81xvQsf1FUuWQesD2tNs2093o06w3YNo/Plyby/oIddGwSTnZeIdf1b+XlYJVSNVl5EoiviAQaY3IBRCQYKE+FeF9gqzFmu7PfFGAUsN5tm9uAicaYgwDGmAMVCV5Vwpc32eqq3MPQcQQHMvOYvyWVQH8fHvl6Df6+wub9mQyNi6ZbTH1vR6uUqsHKk0AmAbNE5APn9U3AR+XYrzmQ6PY6CehXbJsOACKyAPAFnjTG/OCsCxKRBKAAeM4YM734CUTkduB2gJYtW5YjpDNcxm7Y9dvx120G89GinUycs43IkABio0J4/Q+9eOq79fxleJz34lRK1QrluQ/kXyKyGhjqLHrGGPNjFZ6/PTAIiAHmiUg3Y0wG0MoYkywibYDZIrLGGLOtWGxvA28DxMfHmyqKqe5a86X9GxAKeZnQZhCrl24HID0rj/uGtKNLs/p8ccfZXgxSKVVblGs0XmPMTCo+/lUy4H7nWYyzzF0SsNgYkw/sEJHN2ISy1BiT7Jx7u4jMBXoB21CVt+F7iDkLOlwEe1djQqJYk7yCbs3rExbkx5g+Md6OUClVi5TYC0tEQt2e9xeRBBE5IiJ5Tu+ow+U49lKgvYjEikgAMBYo3ptqOvbqAxGJwlZpbXd6egW6LT+XE9tOVEXlH4V9q+3Q7Oc/xMToJ+jyxI9kZOdzzVkt+Oy2/trbSilVIaV1471ORJ4WEQFeB64FEoBg4FZs76oyGWMKgHuBH4ENwBfGmHXOcUc6m/0IpInIemAO8JAxJg3oBCSIyCpn+XPuvbdUJexZAa4CiOmLy2X4ZNEusvMKAeiujeVKqUoosQrLGPOmiFyJTRwYYzaJiL8xphD4QERWYLvflskYMwOYUWzZ427PDfAn5+G+zUKgWwXLosqS6EzhEnMWi3eks+9wzrFVHZvo4IhKqYor6070qWB7OjlVUBtF5B9ACrbHlKotstNhy0/QIBZCGzHl+xXUC/Bl0SNDOXQ0n0A//TiVUhV3yjvRgeud7R4EcoCW2FkJVW1gDHx4KexaAN2vZunOdL5ZuYcbz2lN/Xr+tGxY79THUEqpEpTZC8u5m/wfxphrsclD50GvbVK3wIF1MOzv7O54M/e/vYjmEcHcN6SdtyNTStVyZV6BOG0erZwqLFUbbZtl/3a6lAc+X0F2XiHv3BBPvYBy9eBWSqlSledbZDuwwBnQMKtooTHmJY9FparGhu9g5Wdkh7Xhhx1+LN+dwd8u7UznZuHejkwpVQeUJ4Fscx4+gHbXqS0yEuHz6wD4pHAk//xiFQG+PlzRq7mXA1NK1RXlGcrkqeoIRFWxddMAeKfDW7y0LpRuzevTq2UEDUK0NlIpVTVOmUBEZA52BsITGGOGeCQidXpchbD1F1g1GVeTnry0IYKRPZvy76t6YG+7UUqpqlGeKqz/c3seBFyJHSFX1TR5WfDRSEhOAGBnnwkc3VnIxd2aAHYue6WUqirlqcJaVmzRAhFZ4qF41OnYPtcmj4ufh7ZD+GJJLv6+u+kX29DbkSml6qDyVGG5T4jtA/QBdPCkmmj37+AbQEancdwyaS1b9h+hT6sGhARql12lVNUrzzfLMmwbiGCrrnYAt3gyKFVJiYuhaU+emrmdVYkZBPn7cmn3Zt6OSilVR5WnCiu2OgJRpyk/B/asYE/HG5m2PJn7h7TjT8M6ejsqpVQddsqxsETkHhGJcHvdQETu9mhUquK2z4XCPN5PjKZ5RDB3D9ahSpRSnlWewRRvc6aYBcAYcxC4zWMRqYorzKfg5yfY49OEj1M68PDFcQT56wi7SinPKk8C8RW3/p/OAIt6N1pNkbYNJvbDL3UjT+aM47Xr+jGyh7Z7KKU8rzyN6D8An4vIW87rO6j4/OjKUxb+B3N4D48FPMbhZuczvGtTb0eklDpDlOcK5GFgNnCn81iDndpWeVv+UVj7NfubD2Py4a6MPaultyNSSp1BTplAjDEuYDGwE+gLDMHOca68bdlHkHuIF1PiadWw3rE7zpVSqjqUWoUlIh2Acc4jFfgcwBgzuHpCU6Xavx5+nwgrJ7M36hy+Sorlres76dS0SqlqVVYbyEZgPnCpMWYrgIg8WC1RqbL9+i/YNBNXh4u5afcf6B5Tnws7N/Z2VEqpM0xZVVhXAHuBOSLyjogMxd6NrrypMB+2zcZ0v5qJ0U+y8SDcNaidDpSolKp2pSYQY8x0Y8xYIA6YAzwARIvIGyIyrJriU8UlLobcw/yY34MXf97MJd2aMkyvPpRSXlCeRvQsY8xnxpjLgBhgBbZnlqpuxkDCB7h8/HlkRSQXd23Cf8b1wsdHrz6UUtWvPN14jzHGHDTGvG2MGeqpgFQZfpoAa7/iY7mcwJD6/GN0N00eSimvqVACUV60diosep1f61/Ov/Ku5N0bztLpaZVSXqUJpLZY8Bp5UV247cAV3HhOLN1idEoWpZR3eTSBiMhwEdkkIltF5JFStrlaRNaLyDoR+cxt+Y0issV53OjJOGu8vCzMvjX8UtATl/gz/pzW3o5IKaXKNRZWpTiDLk4ELgSSgKUi8q0xZr3bNu2BR4FzjTEHRSTaWR4JPAHEYyezWubse9BT8dZoycsRU8gXB5rxp2EdaFI/yNsRKaWUR69A+gJbjTHbjTF5wBRgVLFtbgMmFiUGY8wBZ/lFwM/GmHRn3c/AcA/GWnNl7CZt2dcAxJ8zjLsH6TwfSqmawZOTZTcHEt1eJwH9im3TAUBEFgC+wJPGmB9K2bd58ROIyO3A7QAtW9bBgQS3/AJfXE/D/Gz2moZcP7SXtyNSSqljPJlAynv+9sAg7D0m80SkW3l3Nsa8DbwNEB8fbzwRoNfkZZM75QZS/Zvzw9F2tOx2DhcG+3s7KqWUOsaTCSQZaOH2OsZZ5i4JWGyMyQd2iMhmbEJJxiYV933neizSGihv3XcEFmbx56PXkBHdj+/GnOftkJRS6gSebANZCrQXkVgRCQDGAt8W22Y6TqIQkShsldZ24EdgmDP/egNgmLPsjHE04VOSTBTnXziKSbf2w99Xe1wrpWoWj12BGGMKRORe7Be/L/C+MWadiDwNJBhjvuV4olgPFAIPGWPSAETkGWwSAnjaGJPuqVhrnORl1E+exweFV3B1rxY0DA30dkRKKXUSMaZuNB3Ex8ebhIQEb4dx+lwueH8Yh/dt4zLzCnMnjNSRdpVSHiMiy4wx8ZXZV+tFapiClVMgaSkvmT8QG9NUk4dSqsbSBFKTZKdzdMYEVrra8unRsxnQvpG3I1JKqVJ5uxuvKmIMmVNuJTA/gyVdXmTjmBH4acO5UqoG0wTiba5C2PoLGEPo7lk8U3g9d12iyUMpVfNpAvG2zT/ClHEApNCA7bHjiNJeV0qpWkB/5nrbgWNjS/Jx/lBGx8d6MRillCo/vQLxtgMbOOIbwauuqxh/38M0bxzl7YiUUqpcNIF4mevAepYXtuFQl+uJaaK9rpRStYdWYXlTYT4mZTPrCmIY3rWJt6NRSqkK0QTiRam71+NrCsiP7MigjtHeDkcppSpEE4gX7Zz9AQBXjBiOr4/eca6Uql00gVS31V/Ce8Ng80/EJ37ArKBhtOjYx9tRKaVUhWkjenVLeA8SF5Pz2XWkuBqRdPbToONdKaVqIb0CqU6ZKZjdv+NCCCKXN7mSi3rqfR9KqdpJr0Cq0+YfEAyPFtzOw92P8sTlzxAQEODtqJRSqlI0gVSjvUumUmiiaDboNhpc0MHb4Sil1GnRKqxqkpN1mAb7FrA8+BzuGdLe2+EopdRp0wTiSVt+hp0LMMbw/bRJBJFHu/Ov0S67Sqk6QauwPMUYmDQGgH+1/ZSOm78jOyCMzv2GezkwpZSqGppAqlpGIoQ2hiN7jy26bMtf6eC/H78eY8FX33KlVN2gVVhVKfcITOwHM/4MSUsB+MA1gi4+u/B35SA9/+DlAJVSquroz+GqtP1XyM+C5Z9QsGc1uSaQo+c/Dtv3QV4mtOjr7QiVUqrKaAKpSlt/hoAwCAjBb99KVpoO9GkTDedOg8J8veNcKVWnaBVWVXC5YOss2DQT2g6GO+axsOkNvO66kh4tIiA4AkJ1rg+lVN2iVyCn61AyfHoFpGwE30AWRIzk4I5CXs0aQ1gzP4L8fb0doVJKeYQmkMravw5+fhyO7INDSXDle+yIGsi1ry4BVhDg68PzY7p7O0qllPIYTSCVcTQDplwLh5OhMA+ueBe6jWHqj5vwEXjzuj50bV6fZhHB3o5UKaU8xqMJRESGA68CvsC7xpjniq0fD/wbSHYWvW6MeddZVwiscZbvNsaM9GSs5ZKZAjt+hZ2/QcYuuGkmRHeCoPoUFLqYtiKZAe0bMayLTk+rlKr7PJZARMQXmAhcCCQBS0XkW2PM+mKbfm6MubeEQxw1xvT0VHzlMuMvsH8tjH4TIlrCj4/Bmi/sun53Qsv+xzZ9a952kjOO8tTILl4KVimlqpcnr0D6AluNMdsBRGQKMAoonkBqppzDsOxDKMyF18+CXtfB+unQvA8EN4BBjx7b9Iulibzyy2Yu6d6UCzo39lrISilVnTzZjbc5kOj2OslZVtyVIrJaRL4SkRZuy4NEJEFEfheRyz0YZ8k2/s8mjyveha5jYOl7tr3j8jfguqm2ay6wbFc6f5m6mn6xDfnH6G7VHqZSSnmLtxvRvwMmG2NyReQO4CNgiLOulTEmWUTaALNFZI0xZpv7ziJyO3A7QMuWLasuqgMbYcErUL8ldBsD3a+C3jfAwZ3QqOMJm364cBdhQX68c0M8wQHaZVcpdebw5BVIMuB+RRHD8cZyAIwxacaYXOflu0Aft3XJzt/twFygV/ETGGPeNsbEG2PiGzWqwhv1vrgBMg/AiH8fv3u8ZT/occ2xTZIOZnPrRwnMXLOXq/q00OShlDrjeDKBLAXai0isiAQAY4Fv3TcQkaZuL0cCG5zlDUQk0HkeBZxLdbWdHD0IqZvgnPugY+lDr/9z5kbmbUmhZ4sIbj6vdbWEppRSNYnHqrCMMQUici/wI7Yb7/vGmHUi8jSQYIz5FrhfREYCBUA6MN7ZvRPwloi4sEnuuRJ6b3nG3tX2b9MepW6yKjGD/63ey/1D2vGnYR1L3U4ppeoyj7aBGGNmADOKLXvc7fmjwKMl7LcQ8E6L9N6V9m/TniWuNsbwz5kbaBgSwO0D21ZbWEopVdPoYIru5r9ohyep3wJCGpa4yZfLkvh9ezr3D21PaKC3+yAopZT3aAIpknMYZj1tn0e2KXGT71fv4S9fraZfbCTj+lZhry+llKqF9Cd0kZRN9m+7C2Ho4yVu8t5vO2gXHcrHt/QlwE9zr1LqzKbfgkVSNti/I56HpiePorv1QCYrdmdwTXwLAv20y65SSmkCKXJgI/gFQ0TrEldPXZ6Er48wqlez6o1LKaVqKE0gRVI2QKMO4HPyW1LoMny9PImBHRoRHRbkheCUUqrm0QRS5MBGaNSpxFULtqay/3AuY/rEVHNQSilVc2kCATi8B47sgcYlD8X+zco9hAf5MbRTdDUHppRSNZcmEIDNP9i/7YedtMrlMvy6OYWBHaO18VwppdxoAgHYOAMaxJ400i7A+r2HSc3MZWCHKhysUSml6gBNILmZdprajiOOj7zr5tfNKQCc3yGquiNTSqkaTRNIfjb0vhG6jD5plTGG71fvpXtMfe19pZRSxWgCCY2GS16AFmedtGp10iE27D3MVfEtSthRKaXObJpASuFyGd78dRvB/r6M6qk3DyqlVHGaQErx7P82MHPtPu4e1JbwIH9vh6OUUjWOJpASuFyGqcuTuLR7U+4b2t7b4SilVI2kCaQEWw5kcuhoPoM66o2DSilVGk0gJViyMx2Avq0jvRyJUkrVXJpASrB0RzqNwwNpERns7VCUUqrG0gRSzNrkQ/y8fj/ntI1CSrixUCmllKUJxE1egYu7Jy2nQT1/Hh0R5+1wlFKqRtMpbd1MXZ7E7vRsPrjpLL3zXCmlTkETCFBQ6OKqtxaxLvkwPWLqM0gHTlRKqVPSBAKsTj7Eit0ZDOvcmD9e0F7bPpRSqhw0gQALt6YC8NyV3YkMCfByNEopVTtoIzqwYGsanZqGa/JQSqkKOOMTSE5+Ict2H+Tctg29HYpSStUqZ3wCOZyTz/AuTRgSp8OWKKVURXg0gYjIcBHZJCJbReSREtaPF5EUEVnpPG51W3ejiGxxHjd6KsbosCBeG9eLc9rpjINKKVURHmtEFxFfYCJwIZAELBWRb40x64tt+rkx5t5i+0YCTwDxgAGWOfse9FS8SimlKsaTVyB9ga3GmO3GmDxgCjCqnPteBPxsjEl3ksbPwHAPxamUUqoSPJlAmgOJbq+TnGXFXSkiq0XkKxEpmju2XPuKyO0ikiAiCSkpKVUVt1JKqXLwdiP6d0BrY0x37FXGRxXZ2RjztjEm3hgT36iR3j2ulFLVyZMJJBlo4fY6xll2jDEmzRiT67x8F+hT3n2VUkp5lycTyFKgvYjEikgAMBb41n0DEWnq9nIksMF5/iMwTEQaiEgDYJizTCmlVA3hsV5YxpgCEbkX+8XvC7xvjFknIk8DCcaYb4H7RWQkUACkA+OdfdNF5BlsEgJ42hiT7qlYlVJKVZwYY7wdQ5WIj483CQkJ3g5DKaVqFRFZZoyJr9S+dSWBiEgKsOs0DhEFpFZRODWFlql20DLVDnW1TCHGmEr1QqozCeR0iUhCZbNwTaVlqh20TLWDlulk3u7Gq5RSqpbSBKKUUqpSNIEc97a3A/AALVPtoGWqHbRMxWgbiFJKqUrRKxCllFKVoglEKaVUpZzxCeRUk17VFiKyU0TWOBNzJTjLIkXkZ2dSrp+dYWFqLBF5X0QOiMhat2UllkGs15zPbbWI9PZe5KUrpUxPikiy20RqI9zWPeqUaZOIXOSdqMsmIi1EZI6IrBeRdSLyR2d5rf2syihTrf2sRCRIRJaIyCqnTE85y2NFZLET++fOUFOISKDzequzvvUpT2KMOWMf2CFWtgFtgABgFdDZ23FVsiw7gahiy54HHnGePwL8y9txnqIM5wO9gbWnKgMwApgJCNAfWOzt+CtQpieB/yth287Ov8FAINb5t+nr7TKUEGdToLfzPAzY7MReaz+rMspUaz8r5/0OdZ77A4ud9/8LYKyz/E3gLuf53cCbzvOx2Mn+yjzHmX4FcjqTXtUGozg+RP5HwOXeC+XUjDHzsGOiuSutDKOAj431OxBRbHDOGqGUMpVmFDDFGJNrjNkBbMX+G61RjDF7jTHLnedHsIOgNqcWf1ZllKk0Nf6zct7vTOelv/MwwBDgK2d58c+p6PP7ChgqIlLWOc70BFLeSa9qAwP8JCLLROR2Z1ljY8xe5/k+oLF3QjstpZWhtn929zrVOe+7VS3WujI51Ry9sL9u68RnVaxMUIs/KxHxFZGVwAHsnEvbgAxjTIGziXvcx8rkrD8ENCzr+Gd6AqlLzjPG9AYuBu4RkfPdVxp7XVqr+2zXhTI43gDaAj2BvcCLXo2mkkQkFJgKPGCMOey+rrZ+ViWUqVZ/VsaYQmNMT+ycSn2BuKo8/pmeQOrMxFXGmGTn7wFgGvYfy/6iqgLn7wHvRVhppZWh1n52xpj9zn9sF/AOx6s+ak2ZRMQf+0U7yRjztbO4Vn9WJZWpLnxWAMaYDGAOcDa2CrFoKg/3uI+VyVlfH0gr67hnegI55aRXtYGIhIhIWNFz7ARca7FludHZ7EbgG+9EeFpKK8O3wA1OD5/+wCG36pMarVj9/2jsZwW2TGOd3jCxQHtgSXXHdypOvfh7wAZjzEtuq2rtZ1VamWrzZyUijUQkwnkeDFyIbduZA4xxNiv+ORV9fmOA2c6VZOm83VPA2w9sD5HN2LrBv3o7nkqWoQ22R8gqYF1RObD1l7OALcAvQKS3Yz1FOSZjqwnysXWzt5RWBmwPk4nO57YGiPd2/BUo0ydOzKud/7RN3bb/q1OmTcDF3o6/lDKdh62eWg2sdB4javNnVUaZau1nBXQHVjixrwUed5a3wSa7rcCXQKCzPMh5vdVZ3+ZU59ChTJRSSlXKmV6FpZRSqpI0gSillKoUTSBKKaUqRROIUkqpStEEopRSqlI0gShVQSLiIyI/iEhLb8eilDdpN16lKkhE2gIxxphfvR2LUt6kCUSpChCRQuyNZUWmGGOe81Y8SnmTJhClKkBEMo0xod6OQ6maQNtAlKoCYmeEfF7srJBLRKSds7y1iMx2hgOfVdRuIiKNRWSaM1vcKhE5x1k+3RmSf53bsPxK1UiaQJSqmGC36U1Xisg1busOGWO6Aa8DrzjL/gN8ZIzpDkwCXnOWvwb8aozpgZ2xcJ2z/GZjTB8gHrhfRMqcj0Epb9IqLKUqoLQqLBHZCQwxxmx3hgXfZ4xpKCKp2AH48p3le40xUSKSgm2Izy12nCexo74CtAYuMnYWP6VqHL9Tb6KUKidTyvNyEZFBwAXA2caYbBGZix0hVakaSauwlKo617j9XeQ8X4idZwbgWmC+83wWcBccm3a0PnYCn4NO8ogD+ldL1EpVklZhKVUBJXTj/cEY84hThfU5dkrhXGCcMWariLQCPgCigBTgJmPMbhFpDLyNnZuhEJtMlgPTsVVXm4AI4EljzFyPF0ypStAEolQVcBJIvDEm1duxKFVdtApLKaVUpegViFJKqUrRKxCllFKVoglEKaVUpWgCUUopVSmaQJRSSlWKJhCllFKV8v/OWhr1fPqD2gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.regularizers import l2, l1_l2\n",
    "import tensorflow as tf  # Import the l2 regularizer\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# classes = np.unique(y_train)\n",
    "# weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "# class_weights = dict(zip(classes, weights))\n",
    "\n",
    "# Aumentar a complexidade da rede adicionando mais neurônios e camadas\n",
    "# Construção do modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))  # Aumentar se houver sinais de overfitting\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Otimizador e compilação do modelo\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy', 'Recall', 'Precision', 'AUC'])\n",
    "\n",
    "# Callbacks para ajuste fino e prevenção de overfitting\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)\n",
    "\n",
    "# Treinamento do modelo\n",
    "history = model.fit(\n",
    "    X_train, y_train, epochs=500, batch_size=64,\n",
    "    validation_data=(X_test, y_test),\n",
    "    # class_weight=class_weight_dict,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Avaliando a acurácia e AUC do modelo\n",
    "train_acc = model.evaluate(X_train, y_train, verbose=0)[1]\n",
    "test_acc = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "train_auc = roc_auc_score(y_train, model.predict(X_train))\n",
    "test_auc = roc_auc_score(y_test, model.predict(X_test))\n",
    "print('Acurácia no conjunto de treinamento:', train_acc)\n",
    "print('Acurácia no conjunto de teste:', test_acc)\n",
    "print('AUC no conjunto de treinamento:', train_auc)\n",
    "print('AUC no conjunto de teste:', test_auc)\n",
    "\n",
    "# Fazendo as predições no conjunto de teste\n",
    "predictions = model.predict(X_test)\n",
    "predictions = np.where(predictions > 0.5, 1, 0).flatten()  # Converter para 0s e 1s e achatar o array\n",
    "\n",
    "# Obtendo os rótulos verdadeiros\n",
    "\n",
    "# Gerando o relatório de classificação\n",
    "report = classification_report(y_test, predictions, target_names=['Classe 0', 'Classe 1'])\n",
    "\n",
    "# Imprimindo o relatório de classificação\n",
    "print(\"Relatório de Classificação:\")\n",
    "print(report)\n",
    "\n",
    "# Plotando a curva de aprendizado\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Curva de Aprendizado do Modelo')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.xlabel('Época')\n",
    "plt.legend(['Treino', 'Validação'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Salvando o modelo\n",
    "model.save('../models/model_redeht.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curva de Aprendizado: A curva de aprendizado mostra que a acurácia de validação e treinamento estão se aproximando uma da outra conforme o número de épocas aumenta, o que é um bom sinal de que o modelo não está sofrendo de overfitting significativo.\n",
    "\n",
    "Acurácia e AUC: A acurácia e a Área Sob a Curva ROC (AUC) no conjunto de teste são bastante altas, o que sugere que o modelo tem um bom desempenho geral.\n",
    "\n",
    "Recall e Precision: Os valores de recall e precisão são bastante equilibrados para as previsões no conjunto de validação, indicando que o modelo tem um desempenho bom e equilibrado em relação a ambas as classes.\n",
    "\n",
    "Relatório de Classificação: O relatório de classificação mostra resultados quase simétricos para as classes 0 e 1, com uma precisão, recall e pontuação F1 bastante semelhantes para ambas, o que sugere que o modelo está tratando ambas as classes de forma equitativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602/602 [==============================] - 1s 1ms/step\n",
      "Threshold: 0.10, Precision: 0.552, Recall: 0.998, F1 Score: 0.711, Accuracy: 0.594\n",
      "Threshold: 0.11, Precision: 0.557, Recall: 0.997, F1 Score: 0.715, Accuracy: 0.602\n",
      "Threshold: 0.12, Precision: 0.562, Recall: 0.996, F1 Score: 0.718, Accuracy: 0.609\n",
      "Threshold: 0.13, Precision: 0.566, Recall: 0.995, F1 Score: 0.722, Accuracy: 0.616\n",
      "Threshold: 0.14, Precision: 0.571, Recall: 0.993, F1 Score: 0.725, Accuracy: 0.624\n",
      "Threshold: 0.15, Precision: 0.575, Recall: 0.992, F1 Score: 0.728, Accuracy: 0.630\n",
      "Threshold: 0.16, Precision: 0.580, Recall: 0.990, F1 Score: 0.732, Accuracy: 0.637\n",
      "Threshold: 0.17, Precision: 0.585, Recall: 0.989, F1 Score: 0.735, Accuracy: 0.643\n",
      "Threshold: 0.18, Precision: 0.589, Recall: 0.988, F1 Score: 0.738, Accuracy: 0.650\n",
      "Threshold: 0.19, Precision: 0.594, Recall: 0.987, F1 Score: 0.742, Accuracy: 0.657\n",
      "Threshold: 0.20, Precision: 0.599, Recall: 0.985, F1 Score: 0.745, Accuracy: 0.663\n",
      "Threshold: 0.21, Precision: 0.603, Recall: 0.983, F1 Score: 0.748, Accuracy: 0.668\n",
      "Threshold: 0.22, Precision: 0.608, Recall: 0.981, F1 Score: 0.750, Accuracy: 0.674\n",
      "Threshold: 0.23, Precision: 0.612, Recall: 0.979, F1 Score: 0.753, Accuracy: 0.679\n",
      "Threshold: 0.24, Precision: 0.618, Recall: 0.977, F1 Score: 0.757, Accuracy: 0.686\n",
      "Threshold: 0.25, Precision: 0.622, Recall: 0.974, F1 Score: 0.759, Accuracy: 0.691\n",
      "Threshold: 0.26, Precision: 0.627, Recall: 0.971, F1 Score: 0.762, Accuracy: 0.696\n",
      "Threshold: 0.27, Precision: 0.632, Recall: 0.968, F1 Score: 0.765, Accuracy: 0.702\n",
      "Threshold: 0.28, Precision: 0.637, Recall: 0.965, F1 Score: 0.768, Accuracy: 0.708\n",
      "Threshold: 0.29, Precision: 0.641, Recall: 0.962, F1 Score: 0.770, Accuracy: 0.712\n",
      "Threshold: 0.30, Precision: 0.646, Recall: 0.958, F1 Score: 0.772, Accuracy: 0.717\n",
      "Threshold: 0.31, Precision: 0.651, Recall: 0.954, F1 Score: 0.774, Accuracy: 0.721\n",
      "Threshold: 0.32, Precision: 0.656, Recall: 0.951, F1 Score: 0.776, Accuracy: 0.726\n",
      "Threshold: 0.33, Precision: 0.659, Recall: 0.945, F1 Score: 0.777, Accuracy: 0.728\n",
      "Threshold: 0.34, Precision: 0.665, Recall: 0.941, F1 Score: 0.779, Accuracy: 0.733\n",
      "Threshold: 0.35, Precision: 0.670, Recall: 0.936, F1 Score: 0.781, Accuracy: 0.737\n",
      "Threshold: 0.36, Precision: 0.675, Recall: 0.931, F1 Score: 0.782, Accuracy: 0.741\n",
      "Threshold: 0.37, Precision: 0.679, Recall: 0.924, F1 Score: 0.783, Accuracy: 0.743\n",
      "Threshold: 0.38, Precision: 0.683, Recall: 0.918, F1 Score: 0.783, Accuracy: 0.746\n",
      "Threshold: 0.39, Precision: 0.688, Recall: 0.911, F1 Score: 0.784, Accuracy: 0.749\n",
      "Threshold: 0.40, Precision: 0.694, Recall: 0.905, F1 Score: 0.786, Accuracy: 0.753\n",
      "Threshold: 0.41, Precision: 0.700, Recall: 0.898, F1 Score: 0.787, Accuracy: 0.756\n",
      "Threshold: 0.42, Precision: 0.705, Recall: 0.890, F1 Score: 0.787, Accuracy: 0.759\n",
      "Threshold: 0.43, Precision: 0.710, Recall: 0.881, F1 Score: 0.786, Accuracy: 0.761\n",
      "Threshold: 0.44, Precision: 0.714, Recall: 0.872, F1 Score: 0.785, Accuracy: 0.761\n",
      "Threshold: 0.45, Precision: 0.720, Recall: 0.863, F1 Score: 0.785, Accuracy: 0.764\n",
      "Threshold: 0.46, Precision: 0.725, Recall: 0.854, F1 Score: 0.784, Accuracy: 0.765\n",
      "Threshold: 0.47, Precision: 0.729, Recall: 0.843, F1 Score: 0.782, Accuracy: 0.764\n",
      "Threshold: 0.48, Precision: 0.734, Recall: 0.835, F1 Score: 0.781, Accuracy: 0.766\n",
      "Threshold: 0.49, Precision: 0.738, Recall: 0.824, F1 Score: 0.778, Accuracy: 0.765\n",
      "Threshold: 0.50, Precision: 0.744, Recall: 0.814, F1 Score: 0.777, Accuracy: 0.767\n",
      "Threshold: 0.51, Precision: 0.750, Recall: 0.806, F1 Score: 0.777, Accuracy: 0.768\n",
      "Threshold: 0.52, Precision: 0.755, Recall: 0.797, F1 Score: 0.775, Accuracy: 0.769\n",
      "Threshold: 0.53, Precision: 0.760, Recall: 0.787, F1 Score: 0.773, Accuracy: 0.769\n",
      "Threshold: 0.54, Precision: 0.764, Recall: 0.777, F1 Score: 0.770, Accuracy: 0.768\n",
      "Threshold: 0.55, Precision: 0.768, Recall: 0.766, F1 Score: 0.767, Accuracy: 0.767\n",
      "Threshold: 0.56, Precision: 0.772, Recall: 0.755, F1 Score: 0.764, Accuracy: 0.766\n",
      "Threshold: 0.57, Precision: 0.777, Recall: 0.743, F1 Score: 0.759, Accuracy: 0.765\n",
      "Threshold: 0.58, Precision: 0.782, Recall: 0.730, F1 Score: 0.755, Accuracy: 0.763\n",
      "Threshold: 0.59, Precision: 0.786, Recall: 0.720, F1 Score: 0.751, Accuracy: 0.762\n",
      "Threshold: 0.60, Precision: 0.790, Recall: 0.708, F1 Score: 0.747, Accuracy: 0.760\n",
      "Threshold: 0.61, Precision: 0.795, Recall: 0.696, F1 Score: 0.742, Accuracy: 0.758\n",
      "Threshold: 0.62, Precision: 0.800, Recall: 0.684, F1 Score: 0.737, Accuracy: 0.756\n",
      "Threshold: 0.63, Precision: 0.803, Recall: 0.671, F1 Score: 0.731, Accuracy: 0.753\n",
      "Threshold: 0.64, Precision: 0.807, Recall: 0.657, F1 Score: 0.725, Accuracy: 0.750\n",
      "Threshold: 0.65, Precision: 0.811, Recall: 0.646, F1 Score: 0.719, Accuracy: 0.747\n",
      "Threshold: 0.66, Precision: 0.814, Recall: 0.631, F1 Score: 0.711, Accuracy: 0.744\n",
      "Threshold: 0.67, Precision: 0.818, Recall: 0.617, F1 Score: 0.703, Accuracy: 0.740\n",
      "Threshold: 0.68, Precision: 0.823, Recall: 0.603, F1 Score: 0.696, Accuracy: 0.737\n",
      "Threshold: 0.69, Precision: 0.825, Recall: 0.586, F1 Score: 0.685, Accuracy: 0.731\n",
      "Threshold: 0.70, Precision: 0.828, Recall: 0.570, F1 Score: 0.675, Accuracy: 0.726\n",
      "Threshold: 0.71, Precision: 0.830, Recall: 0.555, F1 Score: 0.665, Accuracy: 0.721\n",
      "Threshold: 0.72, Precision: 0.835, Recall: 0.539, F1 Score: 0.655, Accuracy: 0.716\n",
      "Threshold: 0.73, Precision: 0.837, Recall: 0.522, F1 Score: 0.643, Accuracy: 0.710\n",
      "Threshold: 0.74, Precision: 0.841, Recall: 0.505, F1 Score: 0.631, Accuracy: 0.705\n",
      "Threshold: 0.75, Precision: 0.843, Recall: 0.487, F1 Score: 0.617, Accuracy: 0.698\n",
      "Threshold: 0.76, Precision: 0.847, Recall: 0.468, F1 Score: 0.602, Accuracy: 0.691\n",
      "Threshold: 0.77, Precision: 0.850, Recall: 0.452, F1 Score: 0.590, Accuracy: 0.686\n",
      "Threshold: 0.78, Precision: 0.854, Recall: 0.434, F1 Score: 0.576, Accuracy: 0.680\n",
      "Threshold: 0.79, Precision: 0.858, Recall: 0.416, F1 Score: 0.560, Accuracy: 0.674\n",
      "Threshold: 0.80, Precision: 0.859, Recall: 0.398, F1 Score: 0.544, Accuracy: 0.667\n",
      "Threshold: 0.81, Precision: 0.863, Recall: 0.378, F1 Score: 0.526, Accuracy: 0.659\n",
      "Threshold: 0.82, Precision: 0.864, Recall: 0.356, F1 Score: 0.504, Accuracy: 0.650\n",
      "Threshold: 0.83, Precision: 0.868, Recall: 0.334, F1 Score: 0.483, Accuracy: 0.642\n",
      "Threshold: 0.84, Precision: 0.871, Recall: 0.314, F1 Score: 0.462, Accuracy: 0.634\n",
      "Threshold: 0.85, Precision: 0.870, Recall: 0.290, F1 Score: 0.435, Accuracy: 0.624\n",
      "Threshold: 0.86, Precision: 0.871, Recall: 0.267, F1 Score: 0.408, Accuracy: 0.614\n",
      "Threshold: 0.87, Precision: 0.874, Recall: 0.244, F1 Score: 0.381, Accuracy: 0.604\n",
      "Threshold: 0.88, Precision: 0.874, Recall: 0.220, F1 Score: 0.352, Accuracy: 0.594\n",
      "Threshold: 0.89, Precision: 0.875, Recall: 0.197, F1 Score: 0.322, Accuracy: 0.584\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.models import load_model\n",
    "\n",
    "# Carregar o modelo\n",
    "model = load_model('../models/model_redeht.h5')\n",
    "\n",
    "# Prever as probabilidades para o conjunto de teste\n",
    "y_probs = model.predict(X_test)\n",
    "\n",
    "# Inicialize o valor do threshold que você quer testar\n",
    "thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "\n",
    "# Para cada threshold, calcule e imprima as métricas\n",
    "for thresh in thresholds:\n",
    "    # Converta probabilidades em previsões binárias com base no threshold\n",
    "    y_pred = (y_probs >= thresh).astype(int)\n",
    "    \n",
    "    # Calcule as métricas para a classe 1\n",
    "    precision = precision_score(y_test, y_pred, pos_label=1)\n",
    "    recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "    f1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Imprima as métricas\n",
    "    print(f'Threshold: {thresh:.2f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}, Accuracy: {accuracy:.3f}')\n",
    "\n",
    "# Escolha o threshold que oferece o melhor equilíbrio entre as métricas que são importantes para o seu caso de uso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizando características importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model = load_model('../models/model_redeht_final.h5')\n",
    "\n",
    "# import shap\n",
    "# # Criando o explainer\n",
    "# explainer = shap.DeepExplainer(model, X_train)\n",
    "# # Subconjunto do conjunto de teste para análise\n",
    "# X_test_sample = X_test[:200]  # Ajuste o tamanho da amostra conforme necessário\n",
    "\n",
    "# # Calculando os valores SHAP\n",
    "# shap_values = explainer.shap_values(X_test_sample)\n",
    "# # Plotando um gráfico de resumo\n",
    "# # shap.summary_plot(shap_values, X_test_sample)\n",
    "\n",
    "# # Obter os nomes das colunas numéricas\n",
    "# numeric_features = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "# # Obter os nomes das colunas após a transformação\n",
    "# # Para as características numéricas, permanecem os mesmos\n",
    "# transformed_numeric_features = numeric_features\n",
    "\n",
    "# # Como só tem uma característica categórica:\n",
    "# categories = preprocessor.named_transformers_['cat'].categories_\n",
    "# transformed_categorical_features = [\n",
    "#     f'league_{category}' for category in categories[0][1:]  # drop='first' remove a primeira categoria\n",
    "# ]\n",
    "\n",
    "# # Combina os dois\n",
    "# all_transformed_features = transformed_numeric_features + transformed_categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotando um gráfico de resumo com nomes de features reais\n",
    "# shap.summary_plot(shap_values, features=X_test_sample, feature_names=all_transformed_features, max_display=X_test_sample.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
