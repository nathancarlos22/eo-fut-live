{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pendências\n",
    "- [x] Procurar sobre importância de atributos\n",
    "- [x] Calcular acurácia dos modelos\n",
    "- [x] Fazer previsão por liga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports e carregamentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_live_engineer_filtered.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['minute', 'homeTeam', 'awayTeam', 'shotsHome', 'shotsAway',\n",
       "       'blockedShotsHome', 'blockedShotsAway', 'league', 'corners_home',\n",
       "       'corners_away', 'shotsOffgoal_home', 'shotsOffgoal_away',\n",
       "       'shotsOngoal_home', 'shotsOngoal_away', 'yellowcards_home',\n",
       "       'yellowcards_away', 'fouls_home', 'fouls_away', 'offsides_home',\n",
       "       'offsides_away', 'tackles_home', 'tackles_away', 'result', 'match_id',\n",
       "       'possessiontime_away', 'possessiontime_home', 'shotsOnGoalEfficiency',\n",
       "       'attackPressure', 'shotAccuracy_home', 'shotAccuracy_away',\n",
       "       'possessionControl', 'passRisk', 'defensiveDiscipline',\n",
       "       'defensiveEfficacy', 'defensiveAggression'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados separados em características (X) e variável alvo (y).\n",
      "Preprocessador criado com transformações para características numéricas e categóricas.\n",
      "Preprocessador ajustado aos dados.\n",
      "Preprocessador salvo em '../models/preprocessor.pickle'.\n",
      "Dados transformados pelo preprocessador.\n",
      "Dados divididos em conjuntos de treino e teste.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import pickle\n",
    "\n",
    "# Função para separar os dados em características e variável alvo\n",
    "def prepare_data(df):\n",
    "    X = df.drop(columns=['result', 'homeTeam', 'awayTeam', 'match_id'])\n",
    "    y = df['result']\n",
    "    print(\"Dados separados em características (X) e variável alvo (y).\")\n",
    "    return X, y\n",
    "\n",
    "# Função para criar e aplicar o transformador de colunas\n",
    "def create_preprocessor(X):\n",
    "    numeric_features = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "    categorical_features = ['league']\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "            ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "        ])\n",
    "    \n",
    "    print(\"Preprocessador criado com transformações para características numéricas e categóricas.\")\n",
    "    return preprocessor.fit(X)\n",
    "\n",
    "# # Preparando os dados\n",
    "X, y = prepare_data(df)\n",
    "preprocessor = create_preprocessor(X)\n",
    "print(\"Preprocessador ajustado aos dados.\")\n",
    "\n",
    "# Salvando o preprocessador para uso futuro\n",
    "with open('../models/preprocessor.pickle', 'wb') as f:\n",
    "    pickle.dump(preprocessor, f)\n",
    "    print(\"Preprocessador salvo em '../models/preprocessor.pickle'.\")\n",
    "\n",
    "# Dividindo os dados em conjuntos de treino e teste\n",
    "X_transformed = preprocessor.transform(X)\n",
    "print(\"Dados transformados pelo preprocessador.\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.20, random_state=42, stratify=y)\n",
    "print(\"Dados divididos em conjuntos de treino e teste.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Neural no Keras: Treinamento e Avaliação "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrução do modelo, salvamento, visualização treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1837/1837 [==============================] - 9s 4ms/step - loss: 0.8168 - accuracy: 0.5123 - recall_52: 0.5133 - precision_52: 0.5113 - auc_54: 0.5172 - val_loss: 0.6953 - val_accuracy: 0.5459 - val_recall_52: 0.4528 - val_precision_52: 0.5610 - val_auc_54: 0.5655 - lr: 5.0000e-05\n",
      "Epoch 2/200\n",
      "1837/1837 [==============================] - 7s 4ms/step - loss: 0.7598 - accuracy: 0.5255 - recall_52: 0.5208 - precision_52: 0.5248 - auc_54: 0.5368 - val_loss: 0.6814 - val_accuracy: 0.5688 - val_recall_52: 0.5108 - val_precision_52: 0.5818 - val_auc_54: 0.5946 - lr: 5.0000e-05\n",
      "Epoch 3/200\n",
      "1837/1837 [==============================] - 7s 4ms/step - loss: 0.7336 - accuracy: 0.5371 - recall_52: 0.5369 - precision_52: 0.5363 - auc_54: 0.5532 - val_loss: 0.6744 - val_accuracy: 0.5773 - val_recall_52: 0.4973 - val_precision_52: 0.5963 - val_auc_54: 0.6121 - lr: 5.0000e-05\n",
      "Epoch 4/200\n",
      "1837/1837 [==============================] - 7s 4ms/step - loss: 0.7183 - accuracy: 0.5460 - recall_52: 0.5461 - precision_52: 0.5451 - auc_54: 0.5655 - val_loss: 0.6684 - val_accuracy: 0.5864 - val_recall_52: 0.5226 - val_precision_52: 0.6031 - val_auc_54: 0.6254 - lr: 5.0000e-05\n",
      "Epoch 5/200\n",
      "1837/1837 [==============================] - 8s 5ms/step - loss: 0.7035 - accuracy: 0.5539 - recall_52: 0.5493 - precision_52: 0.5535 - auc_54: 0.5793 - val_loss: 0.6654 - val_accuracy: 0.5948 - val_recall_52: 0.5422 - val_precision_52: 0.6099 - val_auc_54: 0.6320 - lr: 5.0000e-05\n",
      "Epoch 6/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6961 - accuracy: 0.5611 - recall_52: 0.5588 - precision_52: 0.5606 - auc_54: 0.5879 - val_loss: 0.6620 - val_accuracy: 0.6011 - val_recall_52: 0.5537 - val_precision_52: 0.6156 - val_auc_54: 0.6439 - lr: 5.0000e-05\n",
      "Epoch 7/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6897 - accuracy: 0.5684 - recall_52: 0.5686 - precision_52: 0.5675 - auc_54: 0.5951 - val_loss: 0.6621 - val_accuracy: 0.6002 - val_recall_52: 0.5523 - val_precision_52: 0.6147 - val_auc_54: 0.6439 - lr: 5.0000e-05\n",
      "Epoch 8/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6815 - accuracy: 0.5748 - recall_52: 0.5701 - precision_52: 0.5746 - auc_54: 0.6069 - val_loss: 0.6589 - val_accuracy: 0.6034 - val_recall_52: 0.5383 - val_precision_52: 0.6230 - val_auc_54: 0.6514 - lr: 5.0000e-05\n",
      "Epoch 9/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6765 - accuracy: 0.5834 - recall_52: 0.5778 - precision_52: 0.5834 - auc_54: 0.6153 - val_loss: 0.6545 - val_accuracy: 0.6173 - val_recall_52: 0.5812 - val_precision_52: 0.6301 - val_auc_54: 0.6632 - lr: 5.0000e-05\n",
      "Epoch 10/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6716 - accuracy: 0.5859 - recall_52: 0.5801 - precision_52: 0.5860 - auc_54: 0.6224 - val_loss: 0.6520 - val_accuracy: 0.6167 - val_recall_52: 0.5702 - val_precision_52: 0.6325 - val_auc_54: 0.6682 - lr: 5.0000e-05\n",
      "Epoch 11/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6661 - accuracy: 0.5933 - recall_52: 0.5927 - precision_52: 0.5926 - auc_54: 0.6334 - val_loss: 0.6497 - val_accuracy: 0.6232 - val_recall_52: 0.5956 - val_precision_52: 0.6340 - val_auc_54: 0.6725 - lr: 5.0000e-05\n",
      "Epoch 12/200\n",
      "1837/1837 [==============================] - 9s 5ms/step - loss: 0.6652 - accuracy: 0.5939 - recall_52: 0.5969 - precision_52: 0.5925 - auc_54: 0.6351 - val_loss: 0.6491 - val_accuracy: 0.6225 - val_recall_52: 0.5912 - val_precision_52: 0.6343 - val_auc_54: 0.6716 - lr: 5.0000e-05\n",
      "Epoch 13/200\n",
      "1837/1837 [==============================] - 9s 5ms/step - loss: 0.6602 - accuracy: 0.6023 - recall_52: 0.6014 - precision_52: 0.6017 - auc_54: 0.6447 - val_loss: 0.6455 - val_accuracy: 0.6248 - val_recall_52: 0.5687 - val_precision_52: 0.6446 - val_auc_54: 0.6804 - lr: 5.0000e-05\n",
      "Epoch 14/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6558 - accuracy: 0.6077 - recall_52: 0.6029 - precision_52: 0.6078 - auc_54: 0.6525 - val_loss: 0.6458 - val_accuracy: 0.6272 - val_recall_52: 0.5966 - val_precision_52: 0.6391 - val_auc_54: 0.6765 - lr: 5.0000e-05\n",
      "Epoch 15/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6529 - accuracy: 0.6141 - recall_52: 0.6133 - precision_52: 0.6134 - auc_54: 0.6585 - val_loss: 0.6400 - val_accuracy: 0.6391 - val_recall_52: 0.6241 - val_precision_52: 0.6468 - val_auc_54: 0.6902 - lr: 5.0000e-05\n",
      "Epoch 16/200\n",
      "1837/1837 [==============================] - 7s 4ms/step - loss: 0.6500 - accuracy: 0.6176 - recall_52: 0.6175 - precision_52: 0.6168 - auc_54: 0.6637 - val_loss: 0.6379 - val_accuracy: 0.6370 - val_recall_52: 0.5960 - val_precision_52: 0.6530 - val_auc_54: 0.6946 - lr: 5.0000e-05\n",
      "Epoch 17/200\n",
      "1837/1837 [==============================] - 7s 4ms/step - loss: 0.6452 - accuracy: 0.6215 - recall_52: 0.6189 - precision_52: 0.6213 - auc_54: 0.6715 - val_loss: 0.6334 - val_accuracy: 0.6419 - val_recall_52: 0.6126 - val_precision_52: 0.6543 - val_auc_54: 0.7008 - lr: 5.0000e-05\n",
      "Epoch 18/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6422 - accuracy: 0.6268 - recall_52: 0.6214 - precision_52: 0.6274 - auc_54: 0.6772 - val_loss: 0.6311 - val_accuracy: 0.6523 - val_recall_52: 0.6376 - val_precision_52: 0.6604 - val_auc_54: 0.7078 - lr: 5.0000e-05\n",
      "Epoch 19/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6378 - accuracy: 0.6332 - recall_52: 0.6332 - precision_52: 0.6324 - auc_54: 0.6845 - val_loss: 0.6288 - val_accuracy: 0.6543 - val_recall_52: 0.6288 - val_precision_52: 0.6660 - val_auc_54: 0.7088 - lr: 5.0000e-05\n",
      "Epoch 20/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6373 - accuracy: 0.6322 - recall_52: 0.6289 - precision_52: 0.6323 - auc_54: 0.6850 - val_loss: 0.6252 - val_accuracy: 0.6582 - val_recall_52: 0.6552 - val_precision_52: 0.6624 - val_auc_54: 0.7161 - lr: 5.0000e-05\n",
      "Epoch 21/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6328 - accuracy: 0.6370 - recall_52: 0.6357 - precision_52: 0.6365 - auc_54: 0.6917 - val_loss: 0.6236 - val_accuracy: 0.6598 - val_recall_52: 0.6413 - val_precision_52: 0.6694 - val_auc_54: 0.7167 - lr: 5.0000e-05\n",
      "Epoch 22/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6286 - accuracy: 0.6433 - recall_52: 0.6420 - precision_52: 0.6429 - auc_54: 0.6989 - val_loss: 0.6198 - val_accuracy: 0.6614 - val_recall_52: 0.6657 - val_precision_52: 0.6632 - val_auc_54: 0.7206 - lr: 5.0000e-05\n",
      "Epoch 23/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6274 - accuracy: 0.6435 - recall_52: 0.6436 - precision_52: 0.6426 - auc_54: 0.7006 - val_loss: 0.6158 - val_accuracy: 0.6702 - val_recall_52: 0.6672 - val_precision_52: 0.6744 - val_auc_54: 0.7278 - lr: 5.0000e-05\n",
      "Epoch 24/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6223 - accuracy: 0.6494 - recall_52: 0.6480 - precision_52: 0.6490 - auc_54: 0.7083 - val_loss: 0.6132 - val_accuracy: 0.6703 - val_recall_52: 0.6707 - val_precision_52: 0.6733 - val_auc_54: 0.7304 - lr: 5.0000e-05\n",
      "Epoch 25/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6203 - accuracy: 0.6503 - recall_52: 0.6504 - precision_52: 0.6494 - auc_54: 0.7106 - val_loss: 0.6111 - val_accuracy: 0.6727 - val_recall_52: 0.6757 - val_precision_52: 0.6748 - val_auc_54: 0.7323 - lr: 5.0000e-05\n",
      "Epoch 26/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6182 - accuracy: 0.6508 - recall_52: 0.6533 - precision_52: 0.6492 - auc_54: 0.7127 - val_loss: 0.6086 - val_accuracy: 0.6729 - val_recall_52: 0.6642 - val_precision_52: 0.6792 - val_auc_54: 0.7369 - lr: 5.0000e-05\n",
      "Epoch 27/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6134 - accuracy: 0.6593 - recall_52: 0.6572 - precision_52: 0.6591 - auc_54: 0.7208 - val_loss: 0.6069 - val_accuracy: 0.6717 - val_recall_52: 0.6940 - val_precision_52: 0.6674 - val_auc_54: 0.7347 - lr: 5.0000e-05\n",
      "Epoch 28/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6110 - accuracy: 0.6605 - recall_52: 0.6638 - precision_52: 0.6586 - auc_54: 0.7239 - val_loss: 0.6021 - val_accuracy: 0.6796 - val_recall_52: 0.6772 - val_precision_52: 0.6837 - val_auc_54: 0.7435 - lr: 5.0000e-05\n",
      "Epoch 29/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6097 - accuracy: 0.6619 - recall_52: 0.6628 - precision_52: 0.6608 - auc_54: 0.7250 - val_loss: 0.5990 - val_accuracy: 0.6818 - val_recall_52: 0.6840 - val_precision_52: 0.6842 - val_auc_54: 0.7476 - lr: 5.0000e-05\n",
      "Epoch 30/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6056 - accuracy: 0.6663 - recall_52: 0.6675 - precision_52: 0.6651 - auc_54: 0.7307 - val_loss: 0.5969 - val_accuracy: 0.6866 - val_recall_52: 0.6915 - val_precision_52: 0.6878 - val_auc_54: 0.7498 - lr: 5.0000e-05\n",
      "Epoch 31/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6000 - accuracy: 0.6728 - recall_52: 0.6764 - precision_52: 0.6708 - auc_54: 0.7380 - val_loss: 0.5942 - val_accuracy: 0.6898 - val_recall_52: 0.6727 - val_precision_52: 0.6997 - val_auc_54: 0.7530 - lr: 5.0000e-05\n",
      "Epoch 32/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.6012 - accuracy: 0.6703 - recall_52: 0.6696 - precision_52: 0.6697 - auc_54: 0.7362 - val_loss: 0.5909 - val_accuracy: 0.6920 - val_recall_52: 0.7057 - val_precision_52: 0.6899 - val_auc_54: 0.7551 - lr: 5.0000e-05\n",
      "Epoch 33/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5962 - accuracy: 0.6772 - recall_52: 0.6779 - precision_52: 0.6762 - auc_54: 0.7430 - val_loss: 0.5907 - val_accuracy: 0.6890 - val_recall_52: 0.6825 - val_precision_52: 0.6947 - val_auc_54: 0.7558 - lr: 5.0000e-05\n",
      "Epoch 34/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5912 - accuracy: 0.6797 - recall_52: 0.6826 - precision_52: 0.6778 - auc_54: 0.7486 - val_loss: 0.5845 - val_accuracy: 0.6965 - val_recall_52: 0.6907 - val_precision_52: 0.7018 - val_auc_54: 0.7633 - lr: 5.0000e-05\n",
      "Epoch 35/200\n",
      "1837/1837 [==============================] - 9s 5ms/step - loss: 0.5919 - accuracy: 0.6808 - recall_52: 0.6826 - precision_52: 0.6793 - auc_54: 0.7480 - val_loss: 0.5827 - val_accuracy: 0.6955 - val_recall_52: 0.6744 - val_precision_52: 0.7074 - val_auc_54: 0.7660 - lr: 5.0000e-05\n",
      "Epoch 36/200\n",
      "1837/1837 [==============================] - 9s 5ms/step - loss: 0.5861 - accuracy: 0.6863 - recall_52: 0.6859 - precision_52: 0.6857 - auc_54: 0.7540 - val_loss: 0.5799 - val_accuracy: 0.6978 - val_recall_52: 0.7013 - val_precision_52: 0.6995 - val_auc_54: 0.7682 - lr: 5.0000e-05\n",
      "Epoch 37/200\n",
      "1837/1837 [==============================] - 9s 5ms/step - loss: 0.5859 - accuracy: 0.6833 - recall_52: 0.6865 - precision_52: 0.6814 - auc_54: 0.7543 - val_loss: 0.5778 - val_accuracy: 0.6954 - val_recall_52: 0.6932 - val_precision_52: 0.6993 - val_auc_54: 0.7697 - lr: 5.0000e-05\n",
      "Epoch 38/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5815 - accuracy: 0.6866 - recall_52: 0.6893 - precision_52: 0.6849 - auc_54: 0.7590 - val_loss: 0.5737 - val_accuracy: 0.7019 - val_recall_52: 0.7118 - val_precision_52: 0.7009 - val_auc_54: 0.7738 - lr: 5.0000e-05\n",
      "Epoch 39/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5800 - accuracy: 0.6908 - recall_52: 0.6964 - precision_52: 0.6879 - auc_54: 0.7605 - val_loss: 0.5714 - val_accuracy: 0.7073 - val_recall_52: 0.7176 - val_precision_52: 0.7060 - val_auc_54: 0.7773 - lr: 5.0000e-05\n",
      "Epoch 40/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5789 - accuracy: 0.6911 - recall_52: 0.6928 - precision_52: 0.6897 - auc_54: 0.7619 - val_loss: 0.5663 - val_accuracy: 0.7106 - val_recall_52: 0.7095 - val_precision_52: 0.7140 - val_auc_54: 0.7839 - lr: 5.0000e-05\n",
      "Epoch 41/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5725 - accuracy: 0.6974 - recall_52: 0.6968 - precision_52: 0.6968 - auc_54: 0.7691 - val_loss: 0.5665 - val_accuracy: 0.7091 - val_recall_52: 0.7172 - val_precision_52: 0.7086 - val_auc_54: 0.7808 - lr: 5.0000e-05\n",
      "Epoch 42/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5719 - accuracy: 0.6980 - recall_52: 0.7018 - precision_52: 0.6958 - auc_54: 0.7700 - val_loss: 0.5625 - val_accuracy: 0.7127 - val_recall_52: 0.7195 - val_precision_52: 0.7128 - val_auc_54: 0.7865 - lr: 5.0000e-05\n",
      "Epoch 43/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5682 - accuracy: 0.7004 - recall_52: 0.6995 - precision_52: 0.6999 - auc_54: 0.7730 - val_loss: 0.5601 - val_accuracy: 0.7124 - val_recall_52: 0.7237 - val_precision_52: 0.7106 - val_auc_54: 0.7878 - lr: 5.0000e-05\n",
      "Epoch 44/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5646 - accuracy: 0.7047 - recall_52: 0.7072 - precision_52: 0.7030 - auc_54: 0.7773 - val_loss: 0.5572 - val_accuracy: 0.7171 - val_recall_52: 0.7272 - val_precision_52: 0.7156 - val_auc_54: 0.7904 - lr: 5.0000e-05\n",
      "Epoch 45/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5646 - accuracy: 0.7022 - recall_52: 0.7071 - precision_52: 0.6995 - auc_54: 0.7767 - val_loss: 0.5544 - val_accuracy: 0.7189 - val_recall_52: 0.7255 - val_precision_52: 0.7189 - val_auc_54: 0.7942 - lr: 5.0000e-05\n",
      "Epoch 46/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5628 - accuracy: 0.7057 - recall_52: 0.7056 - precision_52: 0.7050 - auc_54: 0.7790 - val_loss: 0.5533 - val_accuracy: 0.7189 - val_recall_52: 0.7283 - val_precision_52: 0.7176 - val_auc_54: 0.7931 - lr: 5.0000e-05\n",
      "Epoch 47/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5581 - accuracy: 0.7079 - recall_52: 0.7108 - precision_52: 0.7059 - auc_54: 0.7833 - val_loss: 0.5500 - val_accuracy: 0.7230 - val_recall_52: 0.7228 - val_precision_52: 0.7260 - val_auc_54: 0.7984 - lr: 5.0000e-05\n",
      "Epoch 48/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5547 - accuracy: 0.7103 - recall_52: 0.7122 - precision_52: 0.7087 - auc_54: 0.7870 - val_loss: 0.5462 - val_accuracy: 0.7250 - val_recall_52: 0.7429 - val_precision_52: 0.7199 - val_auc_54: 0.8015 - lr: 5.0000e-05\n",
      "Epoch 49/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5531 - accuracy: 0.7117 - recall_52: 0.7120 - precision_52: 0.7108 - auc_54: 0.7882 - val_loss: 0.5442 - val_accuracy: 0.7246 - val_recall_52: 0.7397 - val_precision_52: 0.7208 - val_auc_54: 0.8037 - lr: 5.0000e-05\n",
      "Epoch 50/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5506 - accuracy: 0.7140 - recall_52: 0.7161 - precision_52: 0.7124 - auc_54: 0.7909 - val_loss: 0.5405 - val_accuracy: 0.7260 - val_recall_52: 0.7229 - val_precision_52: 0.7303 - val_auc_54: 0.8071 - lr: 5.0000e-05\n",
      "Epoch 51/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5475 - accuracy: 0.7151 - recall_52: 0.7178 - precision_52: 0.7133 - auc_54: 0.7931 - val_loss: 0.5418 - val_accuracy: 0.7278 - val_recall_52: 0.7157 - val_precision_52: 0.7364 - val_auc_54: 0.8048 - lr: 5.0000e-05\n",
      "Epoch 52/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5468 - accuracy: 0.7171 - recall_52: 0.7178 - precision_52: 0.7161 - auc_54: 0.7940 - val_loss: 0.5363 - val_accuracy: 0.7283 - val_recall_52: 0.7284 - val_precision_52: 0.7310 - val_auc_54: 0.8089 - lr: 5.0000e-05\n",
      "Epoch 53/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5430 - accuracy: 0.7197 - recall_52: 0.7239 - precision_52: 0.7172 - auc_54: 0.7975 - val_loss: 0.5363 - val_accuracy: 0.7267 - val_recall_52: 0.7548 - val_precision_52: 0.7173 - val_auc_54: 0.8077 - lr: 5.0000e-05\n",
      "Epoch 54/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5425 - accuracy: 0.7197 - recall_52: 0.7195 - precision_52: 0.7190 - auc_54: 0.7980 - val_loss: 0.5289 - val_accuracy: 0.7372 - val_recall_52: 0.7388 - val_precision_52: 0.7391 - val_auc_54: 0.8167 - lr: 5.0000e-05\n",
      "Epoch 55/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5419 - accuracy: 0.7209 - recall_52: 0.7231 - precision_52: 0.7192 - auc_54: 0.7989 - val_loss: 0.5284 - val_accuracy: 0.7354 - val_recall_52: 0.7478 - val_precision_52: 0.7324 - val_auc_54: 0.8160 - lr: 5.0000e-05\n",
      "Epoch 56/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5365 - accuracy: 0.7257 - recall_52: 0.7302 - precision_52: 0.7230 - auc_54: 0.8033 - val_loss: 0.5239 - val_accuracy: 0.7400 - val_recall_52: 0.7590 - val_precision_52: 0.7339 - val_auc_54: 0.8202 - lr: 5.0000e-05\n",
      "Epoch 57/200\n",
      "1837/1837 [==============================] - 9s 5ms/step - loss: 0.5332 - accuracy: 0.7259 - recall_52: 0.7282 - precision_52: 0.7241 - auc_54: 0.8059 - val_loss: 0.5220 - val_accuracy: 0.7378 - val_recall_52: 0.7417 - val_precision_52: 0.7387 - val_auc_54: 0.8213 - lr: 5.0000e-05\n",
      "Epoch 58/200\n",
      "1837/1837 [==============================] - 10s 5ms/step - loss: 0.5342 - accuracy: 0.7263 - recall_52: 0.7298 - precision_52: 0.7241 - auc_54: 0.8057 - val_loss: 0.5205 - val_accuracy: 0.7391 - val_recall_52: 0.7330 - val_precision_52: 0.7449 - val_auc_54: 0.8233 - lr: 5.0000e-05\n",
      "Epoch 59/200\n",
      "1837/1837 [==============================] - 9s 5ms/step - loss: 0.5325 - accuracy: 0.7277 - recall_52: 0.7285 - precision_52: 0.7266 - auc_54: 0.8072 - val_loss: 0.5163 - val_accuracy: 0.7450 - val_recall_52: 0.7624 - val_precision_52: 0.7394 - val_auc_54: 0.8269 - lr: 5.0000e-05\n",
      "Epoch 60/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5293 - accuracy: 0.7294 - recall_52: 0.7319 - precision_52: 0.7275 - auc_54: 0.8095 - val_loss: 0.5159 - val_accuracy: 0.7415 - val_recall_52: 0.7433 - val_precision_52: 0.7433 - val_auc_54: 0.8276 - lr: 5.0000e-05\n",
      "Epoch 61/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5259 - accuracy: 0.7310 - recall_52: 0.7350 - precision_52: 0.7284 - auc_54: 0.8125 - val_loss: 0.5130 - val_accuracy: 0.7471 - val_recall_52: 0.7617 - val_precision_52: 0.7427 - val_auc_54: 0.8294 - lr: 5.0000e-05\n",
      "Epoch 62/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5243 - accuracy: 0.7332 - recall_52: 0.7352 - precision_52: 0.7316 - auc_54: 0.8137 - val_loss: 0.5120 - val_accuracy: 0.7452 - val_recall_52: 0.7652 - val_precision_52: 0.7384 - val_auc_54: 0.8296 - lr: 5.0000e-05\n",
      "Epoch 63/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5208 - accuracy: 0.7374 - recall_52: 0.7397 - precision_52: 0.7357 - auc_54: 0.8172 - val_loss: 0.5103 - val_accuracy: 0.7475 - val_recall_52: 0.7483 - val_precision_52: 0.7498 - val_auc_54: 0.8309 - lr: 5.0000e-05\n",
      "Epoch 64/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5198 - accuracy: 0.7386 - recall_52: 0.7387 - precision_52: 0.7379 - auc_54: 0.8184 - val_loss: 0.5080 - val_accuracy: 0.7487 - val_recall_52: 0.7737 - val_precision_52: 0.7394 - val_auc_54: 0.8323 - lr: 5.0000e-05\n",
      "Epoch 65/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5170 - accuracy: 0.7391 - recall_52: 0.7408 - precision_52: 0.7376 - auc_54: 0.8205 - val_loss: 0.5047 - val_accuracy: 0.7530 - val_recall_52: 0.7718 - val_precision_52: 0.7463 - val_auc_54: 0.8346 - lr: 5.0000e-05\n",
      "Epoch 66/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5168 - accuracy: 0.7376 - recall_52: 0.7406 - precision_52: 0.7354 - auc_54: 0.8202 - val_loss: 0.5029 - val_accuracy: 0.7534 - val_recall_52: 0.7763 - val_precision_52: 0.7448 - val_auc_54: 0.8368 - lr: 5.0000e-05\n",
      "Epoch 67/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5144 - accuracy: 0.7409 - recall_52: 0.7455 - precision_52: 0.7380 - auc_54: 0.8219 - val_loss: 0.5004 - val_accuracy: 0.7563 - val_recall_52: 0.7740 - val_precision_52: 0.7501 - val_auc_54: 0.8388 - lr: 5.0000e-05\n",
      "Epoch 68/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5116 - accuracy: 0.7421 - recall_52: 0.7443 - precision_52: 0.7404 - auc_54: 0.8242 - val_loss: 0.4982 - val_accuracy: 0.7526 - val_recall_52: 0.7670 - val_precision_52: 0.7481 - val_auc_54: 0.8403 - lr: 5.0000e-05\n",
      "Epoch 69/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5077 - accuracy: 0.7445 - recall_52: 0.7495 - precision_52: 0.7415 - auc_54: 0.8277 - val_loss: 0.4963 - val_accuracy: 0.7565 - val_recall_52: 0.7662 - val_precision_52: 0.7542 - val_auc_54: 0.8418 - lr: 5.0000e-05\n",
      "Epoch 70/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5054 - accuracy: 0.7468 - recall_52: 0.7510 - precision_52: 0.7441 - auc_54: 0.8295 - val_loss: 0.4966 - val_accuracy: 0.7547 - val_recall_52: 0.7562 - val_precision_52: 0.7566 - val_auc_54: 0.8416 - lr: 5.0000e-05\n",
      "Epoch 71/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5038 - accuracy: 0.7489 - recall_52: 0.7500 - precision_52: 0.7477 - auc_54: 0.8312 - val_loss: 0.4917 - val_accuracy: 0.7600 - val_recall_52: 0.7707 - val_precision_52: 0.7571 - val_auc_54: 0.8448 - lr: 5.0000e-05\n",
      "Epoch 72/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5045 - accuracy: 0.7460 - recall_52: 0.7472 - precision_52: 0.7448 - auc_54: 0.8300 - val_loss: 0.4901 - val_accuracy: 0.7611 - val_recall_52: 0.7752 - val_precision_52: 0.7565 - val_auc_54: 0.8458 - lr: 5.0000e-05\n",
      "Epoch 73/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5023 - accuracy: 0.7488 - recall_52: 0.7520 - precision_52: 0.7466 - auc_54: 0.8318 - val_loss: 0.4872 - val_accuracy: 0.7634 - val_recall_52: 0.7863 - val_precision_52: 0.7544 - val_auc_54: 0.8480 - lr: 5.0000e-05\n",
      "Epoch 74/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.5010 - accuracy: 0.7492 - recall_52: 0.7515 - precision_52: 0.7473 - auc_54: 0.8328 - val_loss: 0.4850 - val_accuracy: 0.7647 - val_recall_52: 0.7853 - val_precision_52: 0.7566 - val_auc_54: 0.8501 - lr: 5.0000e-05\n",
      "Epoch 75/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4968 - accuracy: 0.7507 - recall_52: 0.7552 - precision_52: 0.7478 - auc_54: 0.8359 - val_loss: 0.4867 - val_accuracy: 0.7636 - val_recall_52: 0.7712 - val_precision_52: 0.7622 - val_auc_54: 0.8487 - lr: 5.0000e-05\n",
      "Epoch 76/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4968 - accuracy: 0.7535 - recall_52: 0.7562 - precision_52: 0.7515 - auc_54: 0.8365 - val_loss: 0.4835 - val_accuracy: 0.7665 - val_recall_52: 0.7786 - val_precision_52: 0.7627 - val_auc_54: 0.8507 - lr: 5.0000e-05\n",
      "Epoch 77/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4945 - accuracy: 0.7551 - recall_52: 0.7606 - precision_52: 0.7517 - auc_54: 0.8382 - val_loss: 0.4795 - val_accuracy: 0.7675 - val_recall_52: 0.7745 - val_precision_52: 0.7664 - val_auc_54: 0.8538 - lr: 5.0000e-05\n",
      "Epoch 78/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4917 - accuracy: 0.7562 - recall_52: 0.7598 - precision_52: 0.7537 - auc_54: 0.8399 - val_loss: 0.4785 - val_accuracy: 0.7704 - val_recall_52: 0.7698 - val_precision_52: 0.7733 - val_auc_54: 0.8553 - lr: 5.0000e-05\n",
      "Epoch 79/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4866 - accuracy: 0.7591 - recall_52: 0.7618 - precision_52: 0.7571 - auc_54: 0.8439 - val_loss: 0.4768 - val_accuracy: 0.7711 - val_recall_52: 0.7889 - val_precision_52: 0.7642 - val_auc_54: 0.8557 - lr: 5.0000e-05\n",
      "Epoch 80/200\n",
      "1837/1837 [==============================] - 10s 5ms/step - loss: 0.4884 - accuracy: 0.7596 - recall_52: 0.7637 - precision_52: 0.7569 - auc_54: 0.8427 - val_loss: 0.4760 - val_accuracy: 0.7716 - val_recall_52: 0.7829 - val_precision_52: 0.7681 - val_auc_54: 0.8564 - lr: 5.0000e-05\n",
      "Epoch 81/200\n",
      "1837/1837 [==============================] - 10s 5ms/step - loss: 0.4882 - accuracy: 0.7600 - recall_52: 0.7626 - precision_52: 0.7580 - auc_54: 0.8430 - val_loss: 0.4696 - val_accuracy: 0.7741 - val_recall_52: 0.7925 - val_precision_52: 0.7667 - val_auc_54: 0.8610 - lr: 5.0000e-05\n",
      "Epoch 82/200\n",
      "1837/1837 [==============================] - 9s 5ms/step - loss: 0.4835 - accuracy: 0.7620 - recall_52: 0.7656 - precision_52: 0.7596 - auc_54: 0.8464 - val_loss: 0.4715 - val_accuracy: 0.7728 - val_recall_52: 0.7793 - val_precision_52: 0.7718 - val_auc_54: 0.8590 - lr: 5.0000e-05\n",
      "Epoch 83/200\n",
      "1837/1837 [==============================] - 8s 5ms/step - loss: 0.4858 - accuracy: 0.7600 - recall_52: 0.7624 - precision_52: 0.7581 - auc_54: 0.8444 - val_loss: 0.4689 - val_accuracy: 0.7758 - val_recall_52: 0.7862 - val_precision_52: 0.7727 - val_auc_54: 0.8610 - lr: 5.0000e-05\n",
      "Epoch 84/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4828 - accuracy: 0.7616 - recall_52: 0.7642 - precision_52: 0.7597 - auc_54: 0.8466 - val_loss: 0.4698 - val_accuracy: 0.7767 - val_recall_52: 0.7814 - val_precision_52: 0.7765 - val_auc_54: 0.8604 - lr: 5.0000e-05\n",
      "Epoch 85/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4805 - accuracy: 0.7651 - recall_52: 0.7666 - precision_52: 0.7636 - auc_54: 0.8485 - val_loss: 0.4664 - val_accuracy: 0.7783 - val_recall_52: 0.7890 - val_precision_52: 0.7749 - val_auc_54: 0.8633 - lr: 5.0000e-05\n",
      "Epoch 86/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4797 - accuracy: 0.7635 - recall_52: 0.7657 - precision_52: 0.7617 - auc_54: 0.8489 - val_loss: 0.4649 - val_accuracy: 0.7758 - val_recall_52: 0.7882 - val_precision_52: 0.7716 - val_auc_54: 0.8634 - lr: 5.0000e-05\n",
      "Epoch 87/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4787 - accuracy: 0.7651 - recall_52: 0.7679 - precision_52: 0.7630 - auc_54: 0.8499 - val_loss: 0.4639 - val_accuracy: 0.7786 - val_recall_52: 0.7990 - val_precision_52: 0.7700 - val_auc_54: 0.8646 - lr: 5.0000e-05\n",
      "Epoch 88/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4765 - accuracy: 0.7666 - recall_52: 0.7695 - precision_52: 0.7644 - auc_54: 0.8511 - val_loss: 0.4613 - val_accuracy: 0.7794 - val_recall_52: 0.7887 - val_precision_52: 0.7768 - val_auc_54: 0.8659 - lr: 5.0000e-05\n",
      "Epoch 89/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4758 - accuracy: 0.7669 - recall_52: 0.7714 - precision_52: 0.7639 - auc_54: 0.8514 - val_loss: 0.4620 - val_accuracy: 0.7809 - val_recall_52: 0.7847 - val_precision_52: 0.7812 - val_auc_54: 0.8651 - lr: 5.0000e-05\n",
      "Epoch 90/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4719 - accuracy: 0.7677 - recall_52: 0.7710 - precision_52: 0.7652 - auc_54: 0.8544 - val_loss: 0.4570 - val_accuracy: 0.7833 - val_recall_52: 0.7928 - val_precision_52: 0.7804 - val_auc_54: 0.8692 - lr: 5.0000e-05\n",
      "Epoch 91/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4723 - accuracy: 0.7696 - recall_52: 0.7717 - precision_52: 0.7679 - auc_54: 0.8542 - val_loss: 0.4554 - val_accuracy: 0.7843 - val_recall_52: 0.7886 - val_precision_52: 0.7844 - val_auc_54: 0.8705 - lr: 5.0000e-05\n",
      "Epoch 92/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4705 - accuracy: 0.7705 - recall_52: 0.7725 - precision_52: 0.7688 - auc_54: 0.8555 - val_loss: 0.4546 - val_accuracy: 0.7864 - val_recall_52: 0.8041 - val_precision_52: 0.7789 - val_auc_54: 0.8707 - lr: 5.0000e-05\n",
      "Epoch 93/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4677 - accuracy: 0.7720 - recall_52: 0.7744 - precision_52: 0.7700 - auc_54: 0.8574 - val_loss: 0.4556 - val_accuracy: 0.7851 - val_recall_52: 0.7976 - val_precision_52: 0.7805 - val_auc_54: 0.8692 - lr: 5.0000e-05\n",
      "Epoch 94/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4685 - accuracy: 0.7706 - recall_52: 0.7725 - precision_52: 0.7690 - auc_54: 0.8567 - val_loss: 0.4500 - val_accuracy: 0.7865 - val_recall_52: 0.7928 - val_precision_52: 0.7854 - val_auc_54: 0.8735 - lr: 5.0000e-05\n",
      "Epoch 95/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4674 - accuracy: 0.7723 - recall_52: 0.7748 - precision_52: 0.7703 - auc_54: 0.8577 - val_loss: 0.4513 - val_accuracy: 0.7863 - val_recall_52: 0.8133 - val_precision_52: 0.7739 - val_auc_54: 0.8725 - lr: 5.0000e-05\n",
      "Epoch 96/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4641 - accuracy: 0.7734 - recall_52: 0.7745 - precision_52: 0.7722 - auc_54: 0.8598 - val_loss: 0.4497 - val_accuracy: 0.7880 - val_recall_52: 0.8043 - val_precision_52: 0.7811 - val_auc_54: 0.8739 - lr: 5.0000e-05\n",
      "Epoch 97/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4641 - accuracy: 0.7734 - recall_52: 0.7772 - precision_52: 0.7707 - auc_54: 0.8597 - val_loss: 0.4484 - val_accuracy: 0.7856 - val_recall_52: 0.7870 - val_precision_52: 0.7872 - val_auc_54: 0.8740 - lr: 5.0000e-05\n",
      "Epoch 98/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4636 - accuracy: 0.7765 - recall_52: 0.7790 - precision_52: 0.7744 - auc_54: 0.8601 - val_loss: 0.4474 - val_accuracy: 0.7918 - val_recall_52: 0.8033 - val_precision_52: 0.7875 - val_auc_54: 0.8749 - lr: 5.0000e-05\n",
      "Epoch 99/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4583 - accuracy: 0.7775 - recall_52: 0.7795 - precision_52: 0.7758 - auc_54: 0.8635 - val_loss: 0.4444 - val_accuracy: 0.7901 - val_recall_52: 0.8108 - val_precision_52: 0.7808 - val_auc_54: 0.8770 - lr: 5.0000e-05\n",
      "Epoch 100/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4591 - accuracy: 0.7784 - recall_52: 0.7828 - precision_52: 0.7754 - auc_54: 0.8632 - val_loss: 0.4428 - val_accuracy: 0.7931 - val_recall_52: 0.8102 - val_precision_52: 0.7857 - val_auc_54: 0.8782 - lr: 5.0000e-05\n",
      "Epoch 101/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4570 - accuracy: 0.7787 - recall_52: 0.7825 - precision_52: 0.7760 - auc_54: 0.8644 - val_loss: 0.4446 - val_accuracy: 0.7929 - val_recall_52: 0.8026 - val_precision_52: 0.7896 - val_auc_54: 0.8766 - lr: 5.0000e-05\n",
      "Epoch 102/200\n",
      "1837/1837 [==============================] - 9s 5ms/step - loss: 0.4599 - accuracy: 0.7780 - recall_52: 0.7783 - precision_52: 0.7773 - auc_54: 0.8628 - val_loss: 0.4427 - val_accuracy: 0.7931 - val_recall_52: 0.8058 - val_precision_52: 0.7882 - val_auc_54: 0.8780 - lr: 5.0000e-05\n",
      "Epoch 103/200\n",
      "1835/1837 [============================>.] - ETA: 0s - loss: 0.4562 - accuracy: 0.7780 - recall_52: 0.7819 - precision_52: 0.7752 - auc_54: 0.8649\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "1837/1837 [==============================] - 9s 5ms/step - loss: 0.4561 - accuracy: 0.7780 - recall_52: 0.7819 - precision_52: 0.7752 - auc_54: 0.8649 - val_loss: 0.4430 - val_accuracy: 0.7924 - val_recall_52: 0.8020 - val_precision_52: 0.7893 - val_auc_54: 0.8775 - lr: 5.0000e-05\n",
      "Epoch 104/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4491 - accuracy: 0.7825 - recall_52: 0.7826 - precision_52: 0.7819 - auc_54: 0.8694 - val_loss: 0.4391 - val_accuracy: 0.7951 - val_recall_52: 0.8089 - val_precision_52: 0.7894 - val_auc_54: 0.8805 - lr: 1.0000e-05\n",
      "Epoch 105/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4478 - accuracy: 0.7845 - recall_52: 0.7862 - precision_52: 0.7829 - auc_54: 0.8706 - val_loss: 0.4383 - val_accuracy: 0.7947 - val_recall_52: 0.8122 - val_precision_52: 0.7869 - val_auc_54: 0.8809 - lr: 1.0000e-05\n",
      "Epoch 106/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4434 - accuracy: 0.7868 - recall_52: 0.7873 - precision_52: 0.7858 - auc_54: 0.8733 - val_loss: 0.4370 - val_accuracy: 0.7965 - val_recall_52: 0.8135 - val_precision_52: 0.7889 - val_auc_54: 0.8816 - lr: 1.0000e-05\n",
      "Epoch 107/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4417 - accuracy: 0.7888 - recall_52: 0.7917 - precision_52: 0.7866 - auc_54: 0.8746 - val_loss: 0.4365 - val_accuracy: 0.7954 - val_recall_52: 0.8075 - val_precision_52: 0.7907 - val_auc_54: 0.8816 - lr: 1.0000e-05\n",
      "Epoch 108/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4445 - accuracy: 0.7868 - recall_52: 0.7879 - precision_52: 0.7856 - auc_54: 0.8730 - val_loss: 0.4363 - val_accuracy: 0.7960 - val_recall_52: 0.8145 - val_precision_52: 0.7876 - val_auc_54: 0.8821 - lr: 1.0000e-05\n",
      "Epoch 109/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4443 - accuracy: 0.7855 - recall_52: 0.7879 - precision_52: 0.7835 - auc_54: 0.8727 - val_loss: 0.4349 - val_accuracy: 0.7956 - val_recall_52: 0.8039 - val_precision_52: 0.7930 - val_auc_54: 0.8826 - lr: 1.0000e-05\n",
      "Epoch 110/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4436 - accuracy: 0.7861 - recall_52: 0.7882 - precision_52: 0.7843 - auc_54: 0.8730 - val_loss: 0.4340 - val_accuracy: 0.7963 - val_recall_52: 0.8041 - val_precision_52: 0.7939 - val_auc_54: 0.8835 - lr: 1.0000e-05\n",
      "Epoch 111/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4407 - accuracy: 0.7879 - recall_52: 0.7908 - precision_52: 0.7857 - auc_54: 0.8749 - val_loss: 0.4335 - val_accuracy: 0.7972 - val_recall_52: 0.8122 - val_precision_52: 0.7908 - val_auc_54: 0.8836 - lr: 1.0000e-05\n",
      "Epoch 112/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4409 - accuracy: 0.7888 - recall_52: 0.7896 - precision_52: 0.7877 - auc_54: 0.8751 - val_loss: 0.4330 - val_accuracy: 0.7971 - val_recall_52: 0.8110 - val_precision_52: 0.7912 - val_auc_54: 0.8836 - lr: 1.0000e-05\n",
      "Epoch 113/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4408 - accuracy: 0.7878 - recall_52: 0.7917 - precision_52: 0.7850 - auc_54: 0.8748 - val_loss: 0.4327 - val_accuracy: 0.7975 - val_recall_52: 0.8125 - val_precision_52: 0.7911 - val_auc_54: 0.8838 - lr: 1.0000e-05\n",
      "Epoch 114/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4396 - accuracy: 0.7898 - recall_52: 0.7932 - precision_52: 0.7873 - auc_54: 0.8759 - val_loss: 0.4318 - val_accuracy: 0.7986 - val_recall_52: 0.8189 - val_precision_52: 0.7892 - val_auc_54: 0.8845 - lr: 1.0000e-05\n",
      "Epoch 115/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4369 - accuracy: 0.7910 - recall_52: 0.7956 - precision_52: 0.7878 - auc_54: 0.8775 - val_loss: 0.4318 - val_accuracy: 0.7971 - val_recall_52: 0.8093 - val_precision_52: 0.7924 - val_auc_54: 0.8844 - lr: 1.0000e-05\n",
      "Epoch 116/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4408 - accuracy: 0.7876 - recall_52: 0.7900 - precision_52: 0.7855 - auc_54: 0.8750 - val_loss: 0.4309 - val_accuracy: 0.7965 - val_recall_52: 0.8118 - val_precision_52: 0.7898 - val_auc_54: 0.8848 - lr: 1.0000e-05\n",
      "Epoch 117/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4411 - accuracy: 0.7883 - recall_52: 0.7912 - precision_52: 0.7860 - auc_54: 0.8745 - val_loss: 0.4306 - val_accuracy: 0.7971 - val_recall_52: 0.8147 - val_precision_52: 0.7893 - val_auc_54: 0.8849 - lr: 1.0000e-05\n",
      "Epoch 118/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4371 - accuracy: 0.7912 - recall_52: 0.7944 - precision_52: 0.7887 - auc_54: 0.8772 - val_loss: 0.4306 - val_accuracy: 0.7975 - val_recall_52: 0.8033 - val_precision_52: 0.7964 - val_auc_54: 0.8849 - lr: 1.0000e-05\n",
      "Epoch 119/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4382 - accuracy: 0.7895 - recall_52: 0.7911 - precision_52: 0.7880 - auc_54: 0.8763 - val_loss: 0.4299 - val_accuracy: 0.7973 - val_recall_52: 0.8098 - val_precision_52: 0.7923 - val_auc_54: 0.8853 - lr: 1.0000e-05\n",
      "Epoch 120/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4378 - accuracy: 0.7916 - recall_52: 0.7962 - precision_52: 0.7884 - auc_54: 0.8771 - val_loss: 0.4299 - val_accuracy: 0.7988 - val_recall_52: 0.8098 - val_precision_52: 0.7947 - val_auc_54: 0.8855 - lr: 1.0000e-05\n",
      "Epoch 121/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4345 - accuracy: 0.7924 - recall_52: 0.7945 - precision_52: 0.7906 - auc_54: 0.8791 - val_loss: 0.4288 - val_accuracy: 0.8012 - val_recall_52: 0.8198 - val_precision_52: 0.7925 - val_auc_54: 0.8861 - lr: 1.0000e-05\n",
      "Epoch 122/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4365 - accuracy: 0.7900 - recall_52: 0.7938 - precision_52: 0.7871 - auc_54: 0.8776 - val_loss: 0.4286 - val_accuracy: 0.8022 - val_recall_52: 0.8171 - val_precision_52: 0.7956 - val_auc_54: 0.8861 - lr: 1.0000e-05\n",
      "Epoch 123/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4354 - accuracy: 0.7921 - recall_52: 0.7972 - precision_52: 0.7886 - auc_54: 0.8784 - val_loss: 0.4281 - val_accuracy: 0.8001 - val_recall_52: 0.8135 - val_precision_52: 0.7946 - val_auc_54: 0.8865 - lr: 1.0000e-05\n",
      "Epoch 124/200\n",
      "1837/1837 [==============================] - 9s 5ms/step - loss: 0.4347 - accuracy: 0.7922 - recall_52: 0.7947 - precision_52: 0.7903 - auc_54: 0.8789 - val_loss: 0.4269 - val_accuracy: 0.8004 - val_recall_52: 0.8133 - val_precision_52: 0.7951 - val_auc_54: 0.8871 - lr: 1.0000e-05\n",
      "Epoch 125/200\n",
      "1837/1837 [==============================] - 10s 5ms/step - loss: 0.4331 - accuracy: 0.7944 - recall_52: 0.7966 - precision_52: 0.7926 - auc_54: 0.8796 - val_loss: 0.4265 - val_accuracy: 0.8010 - val_recall_52: 0.8135 - val_precision_52: 0.7958 - val_auc_54: 0.8874 - lr: 1.0000e-05\n",
      "Epoch 126/200\n",
      "1837/1837 [==============================] - 10s 5ms/step - loss: 0.4333 - accuracy: 0.7943 - recall_52: 0.7961 - precision_52: 0.7928 - auc_54: 0.8796 - val_loss: 0.4268 - val_accuracy: 0.8005 - val_recall_52: 0.8229 - val_precision_52: 0.7898 - val_auc_54: 0.8875 - lr: 1.0000e-05\n",
      "Epoch 127/200\n",
      "1837/1837 [==============================] - 8s 5ms/step - loss: 0.4376 - accuracy: 0.7910 - recall_52: 0.7931 - precision_52: 0.7893 - auc_54: 0.8770 - val_loss: 0.4265 - val_accuracy: 0.8013 - val_recall_52: 0.8116 - val_precision_52: 0.7974 - val_auc_54: 0.8873 - lr: 1.0000e-05\n",
      "Epoch 128/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4341 - accuracy: 0.7922 - recall_52: 0.7933 - precision_52: 0.7910 - auc_54: 0.8791 - val_loss: 0.4262 - val_accuracy: 0.8031 - val_recall_52: 0.8243 - val_precision_52: 0.7930 - val_auc_54: 0.8876 - lr: 1.0000e-05\n",
      "Epoch 129/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4313 - accuracy: 0.7937 - recall_52: 0.7978 - precision_52: 0.7907 - auc_54: 0.8810 - val_loss: 0.4244 - val_accuracy: 0.8031 - val_recall_52: 0.8168 - val_precision_52: 0.7971 - val_auc_54: 0.8881 - lr: 1.0000e-05\n",
      "Epoch 130/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4333 - accuracy: 0.7936 - recall_52: 0.7969 - precision_52: 0.7911 - auc_54: 0.8795 - val_loss: 0.4246 - val_accuracy: 0.8029 - val_recall_52: 0.8148 - val_precision_52: 0.7980 - val_auc_54: 0.8882 - lr: 1.0000e-05\n",
      "Epoch 131/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4329 - accuracy: 0.7933 - recall_52: 0.7967 - precision_52: 0.7907 - auc_54: 0.8798 - val_loss: 0.4259 - val_accuracy: 0.8014 - val_recall_52: 0.8144 - val_precision_52: 0.7959 - val_auc_54: 0.8876 - lr: 1.0000e-05\n",
      "Epoch 132/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4304 - accuracy: 0.7941 - recall_52: 0.7963 - precision_52: 0.7922 - auc_54: 0.8813 - val_loss: 0.4245 - val_accuracy: 0.8029 - val_recall_52: 0.8205 - val_precision_52: 0.7948 - val_auc_54: 0.8884 - lr: 1.0000e-05\n",
      "Epoch 133/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4348 - accuracy: 0.7922 - recall_52: 0.7945 - precision_52: 0.7902 - auc_54: 0.8785 - val_loss: 0.4245 - val_accuracy: 0.8022 - val_recall_52: 0.8144 - val_precision_52: 0.7972 - val_auc_54: 0.8884 - lr: 1.0000e-05\n",
      "Epoch 134/200\n",
      "1837/1837 [==============================] - 8s 4ms/step - loss: 0.4332 - accuracy: 0.7921 - recall_52: 0.7943 - precision_52: 0.7902 - auc_54: 0.8794 - val_loss: 0.4248 - val_accuracy: 0.8037 - val_recall_52: 0.8208 - val_precision_52: 0.7959 - val_auc_54: 0.8883 - lr: 1.0000e-05\n",
      "574/574 [==============================] - 1s 1ms/step\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Classe 0       0.81      0.79      0.80      9181\n",
      "    Classe 1       0.79      0.81      0.80      9181\n",
      "\n",
      "    accuracy                           0.80     18362\n",
      "   macro avg       0.80      0.80      0.80     18362\n",
      "weighted avg       0.80      0.80      0.80     18362\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, LayerNormalization, Activation, BatchNormalization, LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.regularizers import l2, l1_l2\n",
    "import tensorflow as tf  # Import the l2 regularizer\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.00005)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy', tf.metrics.Recall(), tf.metrics.Precision(), tf.metrics.AUC()])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001, verbose=1) # Reduz a taxa de aprendizado \n",
    "# quando a métrica para de melhorar. monitor='val_loss' indica que a perda de validação é a métrica a ser observada, \n",
    "# factor=0.2 significa que a nova taxa de aprendizado será 20% da anterior, \n",
    "# patience=3 é o número de épocas sem melhoria após as quais a taxa de aprendizado será reduzida,\n",
    "# e min_lr=0.00001 é o valor mínimo que a taxa de aprendizado pode atingir.\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "# restore_best_weights=True indica que o modelo irá reter os pesos da época com o melhor valor da métrica monitorada após a interrupção.\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stop, reduce_lr], verbose=1)\n",
    "\n",
    "\n",
    "# Fazendo as predições no conjunto de teste\n",
    "predictions = model.predict(X_test)\n",
    "predictions = np.where(predictions > 0.5, 1, 0).flatten()  # Converter para 0s e 1s e achatar o array\n",
    "\n",
    "# Gerando o relatório de classificação\n",
    "report = classification_report(y_test, predictions, target_names=['Classe 0', 'Classe 1'])\n",
    "\n",
    "# Imprimindo o relatório de classificação\n",
    "print(\"Relatório de Classificação:\")\n",
    "print(report)\n",
    "\n",
    "# Plotando a curva de aprendizado\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Curva de Aprendizado do Modelo')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.xlabel('Época')\n",
    "plt.legend(['Treino', 'Validação'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Salvando o modelo\n",
    "model.save('../models/model_redeht.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curva de Aprendizado: A curva de aprendizado mostra que a acurácia de validação e treinamento estão se aproximando uma da outra conforme o número de épocas aumenta, o que é um bom sinal de que o modelo não está sofrendo de overfitting significativo.\n",
    "\n",
    "Acurácia e AUC: A acurácia e a Área Sob a Curva ROC (AUC) no conjunto de teste são bastante altas, o que sugere que o modelo tem um bom desempenho geral.\n",
    "\n",
    "Recall e Precision: Os valores de recall e precisão são bastante equilibrados para as previsões no conjunto de validação, indicando que o modelo tem um desempenho bom e equilibrado em relação a ambas as classes.\n",
    "\n",
    "Relatório de Classificação: O relatório de classificação mostra resultados quase simétricos para as classes 0 e 1, com uma precisão, recall e pontuação F1 bastante semelhantes para ambas, o que sugere que o modelo está tratando ambas as classes de forma equitativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574/574 [==============================] - 1s 1ms/step\n",
      "Threshold: 0.10, Precision: 0.592, Recall: 0.995, F1 Score: 0.742, Accuracy: 0.655\n",
      "Threshold: 0.11, Precision: 0.599, Recall: 0.994, F1 Score: 0.747, Accuracy: 0.664\n",
      "Threshold: 0.12, Precision: 0.606, Recall: 0.992, F1 Score: 0.752, Accuracy: 0.673\n",
      "Threshold: 0.13, Precision: 0.612, Recall: 0.992, F1 Score: 0.757, Accuracy: 0.681\n",
      "Threshold: 0.14, Precision: 0.618, Recall: 0.990, F1 Score: 0.761, Accuracy: 0.689\n",
      "Threshold: 0.15, Precision: 0.624, Recall: 0.990, F1 Score: 0.765, Accuracy: 0.696\n",
      "Threshold: 0.16, Precision: 0.630, Recall: 0.988, F1 Score: 0.769, Accuracy: 0.704\n",
      "Threshold: 0.17, Precision: 0.635, Recall: 0.986, F1 Score: 0.773, Accuracy: 0.710\n",
      "Threshold: 0.18, Precision: 0.641, Recall: 0.985, F1 Score: 0.776, Accuracy: 0.716\n",
      "Threshold: 0.19, Precision: 0.646, Recall: 0.984, F1 Score: 0.780, Accuracy: 0.722\n",
      "Threshold: 0.20, Precision: 0.651, Recall: 0.982, F1 Score: 0.783, Accuracy: 0.728\n",
      "Threshold: 0.21, Precision: 0.657, Recall: 0.980, F1 Score: 0.786, Accuracy: 0.734\n",
      "Threshold: 0.22, Precision: 0.662, Recall: 0.979, F1 Score: 0.790, Accuracy: 0.739\n",
      "Threshold: 0.23, Precision: 0.666, Recall: 0.976, F1 Score: 0.792, Accuracy: 0.743\n",
      "Threshold: 0.24, Precision: 0.672, Recall: 0.973, F1 Score: 0.795, Accuracy: 0.749\n",
      "Threshold: 0.25, Precision: 0.677, Recall: 0.970, F1 Score: 0.797, Accuracy: 0.754\n",
      "Threshold: 0.26, Precision: 0.681, Recall: 0.967, F1 Score: 0.799, Accuracy: 0.757\n",
      "Threshold: 0.27, Precision: 0.686, Recall: 0.964, F1 Score: 0.802, Accuracy: 0.761\n",
      "Threshold: 0.28, Precision: 0.691, Recall: 0.961, F1 Score: 0.804, Accuracy: 0.765\n",
      "Threshold: 0.29, Precision: 0.695, Recall: 0.957, F1 Score: 0.806, Accuracy: 0.769\n",
      "Threshold: 0.30, Precision: 0.700, Recall: 0.953, F1 Score: 0.807, Accuracy: 0.773\n",
      "Threshold: 0.31, Precision: 0.705, Recall: 0.950, F1 Score: 0.809, Accuracy: 0.776\n",
      "Threshold: 0.32, Precision: 0.709, Recall: 0.945, F1 Score: 0.810, Accuracy: 0.778\n",
      "Threshold: 0.33, Precision: 0.714, Recall: 0.941, F1 Score: 0.812, Accuracy: 0.782\n",
      "Threshold: 0.34, Precision: 0.718, Recall: 0.936, F1 Score: 0.813, Accuracy: 0.784\n",
      "Threshold: 0.35, Precision: 0.723, Recall: 0.932, F1 Score: 0.814, Accuracy: 0.788\n",
      "Threshold: 0.36, Precision: 0.728, Recall: 0.927, F1 Score: 0.815, Accuracy: 0.790\n",
      "Threshold: 0.37, Precision: 0.732, Recall: 0.922, F1 Score: 0.816, Accuracy: 0.792\n",
      "Threshold: 0.38, Precision: 0.738, Recall: 0.917, F1 Score: 0.818, Accuracy: 0.796\n",
      "Threshold: 0.39, Precision: 0.744, Recall: 0.912, F1 Score: 0.819, Accuracy: 0.799\n",
      "Threshold: 0.40, Precision: 0.749, Recall: 0.905, F1 Score: 0.819, Accuracy: 0.801\n",
      "Threshold: 0.41, Precision: 0.754, Recall: 0.899, F1 Score: 0.820, Accuracy: 0.803\n",
      "Threshold: 0.42, Precision: 0.759, Recall: 0.893, F1 Score: 0.821, Accuracy: 0.805\n",
      "Threshold: 0.43, Precision: 0.764, Recall: 0.886, F1 Score: 0.821, Accuracy: 0.807\n",
      "Threshold: 0.44, Precision: 0.768, Recall: 0.879, F1 Score: 0.820, Accuracy: 0.807\n",
      "Threshold: 0.45, Precision: 0.773, Recall: 0.870, F1 Score: 0.819, Accuracy: 0.807\n",
      "Threshold: 0.46, Precision: 0.778, Recall: 0.864, F1 Score: 0.819, Accuracy: 0.809\n",
      "Threshold: 0.47, Precision: 0.783, Recall: 0.858, F1 Score: 0.819, Accuracy: 0.810\n",
      "Threshold: 0.48, Precision: 0.790, Recall: 0.850, F1 Score: 0.819, Accuracy: 0.812\n",
      "Threshold: 0.49, Precision: 0.794, Recall: 0.843, F1 Score: 0.818, Accuracy: 0.812\n",
      "Threshold: 0.50, Precision: 0.798, Recall: 0.836, F1 Score: 0.817, Accuracy: 0.812\n",
      "Threshold: 0.51, Precision: 0.803, Recall: 0.828, F1 Score: 0.815, Accuracy: 0.813\n",
      "Threshold: 0.52, Precision: 0.808, Recall: 0.819, F1 Score: 0.813, Accuracy: 0.812\n",
      "Threshold: 0.53, Precision: 0.813, Recall: 0.811, F1 Score: 0.812, Accuracy: 0.812\n",
      "Threshold: 0.54, Precision: 0.818, Recall: 0.804, F1 Score: 0.811, Accuracy: 0.813\n",
      "Threshold: 0.55, Precision: 0.823, Recall: 0.796, F1 Score: 0.809, Accuracy: 0.812\n",
      "Threshold: 0.56, Precision: 0.827, Recall: 0.787, F1 Score: 0.806, Accuracy: 0.811\n",
      "Threshold: 0.57, Precision: 0.832, Recall: 0.779, F1 Score: 0.804, Accuracy: 0.810\n",
      "Threshold: 0.58, Precision: 0.836, Recall: 0.770, F1 Score: 0.801, Accuracy: 0.809\n",
      "Threshold: 0.59, Precision: 0.842, Recall: 0.761, F1 Score: 0.799, Accuracy: 0.809\n",
      "Threshold: 0.60, Precision: 0.846, Recall: 0.753, F1 Score: 0.797, Accuracy: 0.808\n",
      "Threshold: 0.61, Precision: 0.851, Recall: 0.742, F1 Score: 0.793, Accuracy: 0.806\n",
      "Threshold: 0.62, Precision: 0.856, Recall: 0.732, F1 Score: 0.789, Accuracy: 0.804\n",
      "Threshold: 0.63, Precision: 0.860, Recall: 0.721, F1 Score: 0.785, Accuracy: 0.802\n",
      "Threshold: 0.64, Precision: 0.864, Recall: 0.709, F1 Score: 0.779, Accuracy: 0.799\n",
      "Threshold: 0.65, Precision: 0.867, Recall: 0.700, F1 Score: 0.774, Accuracy: 0.796\n",
      "Threshold: 0.66, Precision: 0.870, Recall: 0.689, F1 Score: 0.769, Accuracy: 0.793\n",
      "Threshold: 0.67, Precision: 0.876, Recall: 0.677, F1 Score: 0.764, Accuracy: 0.791\n",
      "Threshold: 0.68, Precision: 0.881, Recall: 0.667, F1 Score: 0.759, Accuracy: 0.789\n",
      "Threshold: 0.69, Precision: 0.884, Recall: 0.657, F1 Score: 0.754, Accuracy: 0.786\n",
      "Threshold: 0.70, Precision: 0.887, Recall: 0.645, F1 Score: 0.747, Accuracy: 0.781\n",
      "Threshold: 0.71, Precision: 0.891, Recall: 0.634, F1 Score: 0.741, Accuracy: 0.778\n",
      "Threshold: 0.72, Precision: 0.896, Recall: 0.621, F1 Score: 0.734, Accuracy: 0.775\n",
      "Threshold: 0.73, Precision: 0.899, Recall: 0.610, F1 Score: 0.727, Accuracy: 0.771\n",
      "Threshold: 0.74, Precision: 0.903, Recall: 0.597, F1 Score: 0.719, Accuracy: 0.767\n",
      "Threshold: 0.75, Precision: 0.907, Recall: 0.585, F1 Score: 0.711, Accuracy: 0.762\n",
      "Threshold: 0.76, Precision: 0.911, Recall: 0.571, F1 Score: 0.702, Accuracy: 0.758\n",
      "Threshold: 0.77, Precision: 0.915, Recall: 0.558, F1 Score: 0.693, Accuracy: 0.753\n",
      "Threshold: 0.78, Precision: 0.918, Recall: 0.543, F1 Score: 0.682, Accuracy: 0.747\n",
      "Threshold: 0.79, Precision: 0.920, Recall: 0.529, F1 Score: 0.672, Accuracy: 0.742\n",
      "Threshold: 0.80, Precision: 0.923, Recall: 0.513, F1 Score: 0.660, Accuracy: 0.735\n",
      "Threshold: 0.81, Precision: 0.927, Recall: 0.497, F1 Score: 0.647, Accuracy: 0.729\n",
      "Threshold: 0.82, Precision: 0.932, Recall: 0.480, F1 Score: 0.634, Accuracy: 0.722\n",
      "Threshold: 0.83, Precision: 0.933, Recall: 0.464, F1 Score: 0.620, Accuracy: 0.715\n",
      "Threshold: 0.84, Precision: 0.938, Recall: 0.447, F1 Score: 0.605, Accuracy: 0.709\n",
      "Threshold: 0.85, Precision: 0.942, Recall: 0.427, F1 Score: 0.587, Accuracy: 0.700\n",
      "Threshold: 0.86, Precision: 0.944, Recall: 0.406, F1 Score: 0.568, Accuracy: 0.691\n",
      "Threshold: 0.87, Precision: 0.946, Recall: 0.384, F1 Score: 0.546, Accuracy: 0.681\n",
      "Threshold: 0.88, Precision: 0.950, Recall: 0.361, F1 Score: 0.523, Accuracy: 0.671\n",
      "Threshold: 0.89, Precision: 0.954, Recall: 0.335, F1 Score: 0.496, Accuracy: 0.660\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.models import load_model\n",
    "\n",
    "# Carregar o modelo\n",
    "model = load_model('../models/model_redeht.h5')\n",
    "\n",
    "# Prever as probabilidades para o conjunto de teste\n",
    "y_probs = model.predict(X_test)\n",
    "\n",
    "# Inicialize o valor do threshold que você quer testar\n",
    "thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "\n",
    "# Para cada threshold, calcule e imprima as métricas\n",
    "for thresh in thresholds:\n",
    "    # Converta probabilidades em previsões binárias com base no threshold\n",
    "    y_pred = (y_probs >= thresh).astype(int)\n",
    "    \n",
    "    # Calcule as métricas para a classe 1\n",
    "    precision = precision_score(y_test, y_pred, pos_label=1)\n",
    "    recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "    f1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Imprima as métricas\n",
    "    print(f'Threshold: {thresh:.2f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1:.3f}, Accuracy: {accuracy:.3f}')\n",
    "\n",
    "# Escolha o threshold que oferece o melhor equilíbrio entre as métricas que são importantes para o seu caso de uso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizando características importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model = load_model('../models/model_redeht_final.h5')\n",
    "\n",
    "# import shap\n",
    "# # Criando o explainer\n",
    "# explainer = shap.DeepExplainer(model, X_train)\n",
    "# # Subconjunto do conjunto de teste para análise\n",
    "# X_test_sample = X_test[:200]  # Ajuste o tamanho da amostra conforme necessário\n",
    "\n",
    "# # Calculando os valores SHAP\n",
    "# shap_values = explainer.shap_values(X_test_sample)\n",
    "# # Plotando um gráfico de resumo\n",
    "# # shap.summary_plot(shap_values, X_test_sample)\n",
    "\n",
    "# # Obter os nomes das colunas numéricas\n",
    "# numeric_features = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "# # Obter os nomes das colunas após a transformação\n",
    "# # Para as características numéricas, permanecem os mesmos\n",
    "# transformed_numeric_features = numeric_features\n",
    "\n",
    "# # Como só tem uma característica categórica:\n",
    "# categories = preprocessor.named_transformers_['cat'].categories_\n",
    "# transformed_categorical_features = [\n",
    "#     f'league_{category}' for category in categories[0][1:]  # drop='first' remove a primeira categoria\n",
    "# ]\n",
    "\n",
    "# # Combina os dois\n",
    "# all_transformed_features = transformed_numeric_features + transformed_categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotando um gráfico de resumo com nomes de features reais\n",
    "# shap.summary_plot(shap_values, features=X_test_sample, feature_names=all_transformed_features, max_display=X_test_sample.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
